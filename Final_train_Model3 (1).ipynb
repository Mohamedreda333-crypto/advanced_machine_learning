{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQtyWERImVoE"
      },
      "source": [
        "<div style=\"text-align: center;\">\n",
        "  <h1><b>üå± Plant Type Classifier ‚Äî Model 3 (Tomato,Corn, Apple, Potato,.......)üß†</b></h1>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPkQtGNGmVoG"
      },
      "source": [
        "### üñºÔ∏è Model 3 Images\n",
        "\n",
        "<div style=\"display: flex; justify-content: center; align-items: center; gap: 20px;\">\n",
        "\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"https://storage.googleapis.com/kagglesdsdata/datasets/145158/338958/plantvillage/Tomato___Septoria_leaf_spot/0025c401-7785-49c5-8bef-780a8a0d3652___Matt.S_CG%202694.JPG?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20250610%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20250610T032057Z&X-Goog-Expires=345600&X-Goog-SignedHeaders=host&X-Goog-Signature=112a45ee35202dc7f54946a9d2c5573644066e43111e9d84d2dfd293d6763419da9f60f9d2ca3236ba03bfa8033bdb04c7e810375cd8abb78ba251555f309369d1d1d9f8e8be816a520e405eefa228d7651cc1fd5f8918fd666658e3ee2bed31e460d8993d479aea5e851e7953816b90b17675c2561134010288bdcdd013ed3f4cfcec8a8d36fd4342b7a6df899f091ce8f6df63957004383ea0130d574d108bd3d5fc54bf49bf82a180ff979c5442b9d0ee4e8414b31f5fab43cb50b839030ab9073e4b249c526173e41dcf05aaa19d7dc2d3000fc8daf61af1aa1d0b3e06330ea3dd255fad4fa897af46d70cd730c7fb8d048fc91e9462099ea912649aebe2\" width=\"200\"/>\n",
        "    <p>‚úÖ Tomato üçÖ </p>\n",
        "  </div>\n",
        "\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"https://storage.googleapis.com/kagglesdsdata/datasets/277323/658267/color/Corn_%28maize%29___Northern_Leaf_Blight/0079c731-80f5-4fea-b6a2-4ff23a7ce139___RS_NLB%204121.JPG?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20250610%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20250610T032313Z&X-Goog-Expires=345600&X-Goog-SignedHeaders=host&X-Goog-Signature=669665fcd419e0e29d84e0552a3cc4bee79ef65f77d53d77480ac5c6718e1c41afc585f4fa1dd0231ee73b8fbf81b820c5e4d4a417cb245c973a9b79ee302807702fce82261f5924494a34d3d8bffad6dc0b3eed10af53fda879f5286f280e252606063d009b51a24c1cd66f0569a5bcdda431257814c6cbd847a87c5b395ac02bc9fc7d5a9cea9e98e8e4432acb901c77493b43b19532054ea0a18ed978e45a5d2fe89a5e627ffef70fba6c5cce3a1dd0425601e094414e67ef5af21bca3e4fd34f3b42ae168d24f6df2112fde8363e7461ec24edbc777f1c8ad431f2a4b59188da72a94454775d639011c87642a26f346be99d1561f7c2bee39e4f3c8ccd07\n",
        "    \" width=\"200\"/>\n",
        "    <p>‚úÖ Corn üåΩ</p>\n",
        "  </div>\n",
        "\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"https://storage.googleapis.com/kagglesdsdata/datasets/277323/658267/color/Apple___Apple_scab/0a6812de-7416-4ffe-aba9-307599a02c84___FREC_Scab%202973.JPG?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20250610%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20250610T032412Z&X-Goog-Expires=345600&X-Goog-SignedHeaders=host&X-Goog-Signature=5f77f9b0e74a77fe5f90eee0f2466ccd60a0b6367ffb2030f1efdf2815c93eda6d775243e132b0a31fc34a9199a1acc529bcd34dde278ca465a38f8537c31c3cea563c02c28e8b2e0f73a0b6905b6ba754139a17f4a8dfb38b341d5eac2b43e9da6daa04988f594c2f5a9579fc71b6780a642915d69ba03ea77263cbf457c8136da3dc7533dc2662977b50d60f69e91a45124c13f5ba9f49d4f17810a925286e8b254cdcb982802dfc323c9da32fa5b08a29ba5a17241ea71a37d880b0f993d55d5d8c9dda1104ee3e41d4bf4e251aff46ee7f1a3cc71776c01b12c57fabb391c36a4f9ed19d384637b68e05eed586c3c1df95b7420c7b245ba2db8f318b6c46\" width=\"200\"/>\n",
        "    <p>‚úÖ Apple üçé</p>\n",
        "  </div>\n",
        "\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"https://storage.googleapis.com/kagglesdsdata/datasets/277323/658267/color/Grape___healthy/0163a6aa-fbf8-47c5-965f-59b6efe8bfe5___Mt.N.V_HL%206103.JPG?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20250610%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20250610T032520Z&X-Goog-Expires=345600&X-Goog-SignedHeaders=host&X-Goog-Signature=52dcc1be0b00e7ac2691a4a05bdeb3cd3a8614763d44bd04f95c1b36633c83ebaaac96aa789136d993c46e6d354e1348c54624ca731edc1feb8b2a6b1b5d0d6f2895701c1eec7705857830d55c970b88ca504885a48d39fd478d6d7130477d24f36a2af48474640339cfb9e2ad508e3774584407a4248d24b816b1a0c9e93b66d1faac4468cb2d4dac50299eeaab23919ad94d3e40b2912846882536bf79b9f0374f8c4d9dd02c080c36f0811edada996a269cd6f23d1464c16cded45563a90a2d9ab57d6284158886476fa22711aa596d928d37557d57e912e088f0fddb1f512524ff8360c0f705445c93364b830dfb42327d6db547fe20e4bf6d0cd4696776\" width=\"200\"/>\n",
        "    <p>‚úÖ Grape üçá </p>\n",
        "  </div>\n",
        "\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuKecnxAmVoG"
      },
      "source": [
        "üå∏ ‚úø ‚úø ‚úø ‚úø ‚úø üå∏üå∏ ‚úø ‚úø ‚úø ‚úø ‚úø üå∏üå∏ ‚úø ‚úø ‚úø ‚úø ‚úø üå∏üå∏ ‚úø ‚úø ‚úø ‚úø ‚úø üå∏üå∏ ‚úø ‚úø ‚úø ‚úø ‚úø üå∏üå∏ ‚úø ‚úø ‚úø ‚úø ‚úø üå∏üå∏ ‚úø ‚úø ‚úø ‚úø ‚úø üå∏üå∏ ‚úø ‚úø ‚úø ‚úø ‚úø üå∏üå∏ ‚úø ‚úø ‚úø ‚úø ‚úø üå∏üå∏ ‚úø ‚úø ‚úø ‚úø ‚úø üå∏üå∏ ‚úø ‚úø ‚úø ‚úø ‚úø üå∏\n",
        "\n",
        "üåø ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üåøüåø ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üåøüåø ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üåøüåø ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üåø\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a899pCZdmVoG"
      },
      "source": [
        "# **Our team**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G50vNTNMmVoH"
      },
      "source": [
        "### ‚Äé‚Äß‚ÇäÀö‚úø[Mohamed Reda Ramadan Khamis]‚úøÀö : **üÜî 221902936**\n",
        "### ‚Äé‚Äß‚ÇäÀö‚úø[Abdelrahman Ghareeb Mohamed Ghareeb]‚úøÀö: **üÜî 221902910**\n",
        "### ‚Äé‚Äß‚ÇäÀö‚úø[Shady Khaled Metwally Abdelghany]‚úøÀö: **üÜî 221903202**\n",
        "### ‚Äé‚Äß‚ÇäÀö‚úø[Ahmed Gamal El Din Mohamed]‚úøÀö: **üÜî 221902990**\n",
        "### ‚Äé‚Äß‚ÇäÀö‚úø[Mohamed Tarek Sayedelahl ]‚úøÀö: **üÜî 221902939**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZ9FAz-WmVoH"
      },
      "source": [
        "üå∏ ‚úø ‚úø ‚úø ‚úø ‚úø üå∏üå∏ ‚úø ‚úø ‚úø ‚úø ‚úø üå∏üå∏ ‚úø ‚úø ‚úø ‚úø ‚úø üå∏üå∏ ‚úø ‚úø ‚úø ‚úø ‚úø üå∏üå∏ ‚úø ‚úø ‚úø ‚úø ‚úø üå∏üå∏ ‚úø ‚úø ‚úø ‚úø ‚úø üå∏üå∏ ‚úø ‚úø ‚úø ‚úø ‚úø üå∏üå∏ ‚úø ‚úø ‚úø ‚úø ‚úø üå∏üå∏ ‚úø ‚úø ‚úø ‚úø ‚úø üå∏üå∏ ‚úø ‚úø ‚úø ‚úø ‚úø üå∏üå∏ ‚úø ‚úø ‚úø ‚úø ‚úø üå∏\n",
        "\n",
        "üåø ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üåøüåø ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üåøüåø ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üåøüåø ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üåø"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9Vlw4dMmVoH"
      },
      "source": [
        "### ***Install Required Package ‚Äî `opendatasets`***\n",
        "‚¨áÔ∏è Installing the package:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20k4ZGgVAuB4",
        "outputId": "ef2b5d1d-b2c5-4777-8b8b-69265e8553a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from opendatasets) (4.67.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (from opendatasets) (1.7.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from opendatasets) (8.1.8)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2025.4.26)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (3.4.1)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (5.29.4)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (0.5.1)\n",
            "Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n"
          ]
        }
      ],
      "source": [
        "!pip install opendatasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsOC_yHamVoI"
      },
      "source": [
        "### ***üå±Download PlantDoc Dataset from Kaggle***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0xiSITFcfBH",
        "outputId": "7a07c645-b82c-49bf-c8f4-bd9803c0dc62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: medokhamis\n",
            "Your Kaggle Key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "Dataset URL: https://www.kaggle.com/datasets/nirmalsankalana/plantdoc-dataset\n"
          ]
        }
      ],
      "source": [
        "import opendatasets as od\n",
        "od.download (\"https://www.kaggle.com/datasets/nirmalsankalana/plantdoc-dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dkxl9sR9mVoI"
      },
      "source": [
        "### ***üå±Download PlantVillage Dataset from Kaggle***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CahPzIIHchsG",
        "outputId": "070e05d2-872f-4c6c-ac39-e535f7c6b562"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: medokhamis\n",
            "Your Kaggle Key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "Dataset URL: https://www.kaggle.com/datasets/emmarex/plantdisease\n"
          ]
        }
      ],
      "source": [
        "import opendatasets as od\n",
        "od.download (\"https://www.kaggle.com/datasets/emmarex/plantdisease\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrcaCpESmVoI"
      },
      "source": [
        "### ***üöÄ Mount Google Drive in Colab***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_ttjYmXcjji",
        "outputId": "dc7c678b-f438-48f9-9cd4-829f9d048b58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dO_cAb6AmVoJ"
      },
      "source": [
        "### ***üìÇ Key Project Directory Paths***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "go1ETEkadZHu"
      },
      "outputs": [],
      "source": [
        "# Core Libraries\n",
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "import json\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "\n",
        "# Data Handling\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import concurrent.futures\n",
        "\n",
        "# Machine Learning & Deep Learning\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, callbacks, optimizers, applications, regularizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Google Colab Specific (if needed)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXPNk6XRdtQ6"
      },
      "outputs": [],
      "source": [
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('plant_type_classification.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geByYHB5mVoJ"
      },
      "source": [
        "üåø ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üåøüåø ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üåøüåø ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üåøüåø ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üåø\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lIHXupMmVoJ"
      },
      "source": [
        "### ***üåø Configuration Class for Model 3 ‚Äî Plant Type Classification***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAoaeYeJdyXZ"
      },
      "outputs": [],
      "source": [
        "class PlantTypeConfig:\n",
        "    \"\"\"Configuration for plant type classification model\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize all configuration parameters including dataset paths,\n",
        "        model directories, hyperparameters, and other settings.\n",
        "        \"\"\"\n",
        "        # Base directory (Colab environment)\n",
        "        self.BASE_DIR = \"/content/drive/MyDrive/Graduation Project\"\n",
        "\n",
        "        # Dataset source paths\n",
        "        self.PLANTVILLAGE_DIR = \"/content/plantdisease/PlantVillage\"\n",
        "        self.PLANTDOC_TRAIN_DIR = \"/content/plantdoc-dataset/train\"\n",
        "        self.PLANTDOC_TEST_DIR = \"/content/plantdoc-dataset/test\"\n",
        "\n",
        "        # Processed data paths\n",
        "        self.DATA_DIR = os.path.join(self.BASE_DIR, \"plant_type_data\")\n",
        "        self.PLANT_CLASSES = [\n",
        "            'Apple', 'Bell_pepper', 'Cherry', 'Corn', 'Grape',\n",
        "            'Peach','Potato', 'Raspberry',\n",
        "            'Soyabean', 'Strawberry', 'Tomato'\n",
        "        ]\n",
        "\n",
        "        # Create class directories\n",
        "        self.class_dirs = {\n",
        "            cls: os.path.join(self.DATA_DIR, cls) for cls in self.PLANT_CLASSES\n",
        "        }\n",
        "\n",
        "        # Model paths\n",
        "        self.MODEL_NAME = \"model3_plant_type\"\n",
        "        self.MODEL_DIR = os.path.join(self.BASE_DIR, f\"saved_models/{self.MODEL_NAME}\")\n",
        "        self.LOG_DIR = os.path.join(self.BASE_DIR, f\"training_logs/{self.MODEL_NAME}\")\n",
        "\n",
        "        # Model hyperparameters\n",
        "        self.IMG_SIZE = (384, 384)\n",
        "        self.BATCH_SIZE = 32\n",
        "        self.EPOCHS = 60\n",
        "        self.SEED = 42\n",
        "\n",
        "        # Anti-overfitting\n",
        "        self.DROPOUT_RATE = 0.6\n",
        "        self.L2_REG = 0.001\n",
        "        self.LABEL_SMOOTHING = 0.1\n",
        "\n",
        "        # Create directories\n",
        "        self.setup_dirs()\n",
        "\n",
        "    def setup_dirs(self):\n",
        "        \"\"\"Create all necessary directories\"\"\"\n",
        "        os.makedirs(self.DATA_DIR, exist_ok=True)\n",
        "        for cls_dir in self.class_dirs.values():\n",
        "            os.makedirs(cls_dir, exist_ok=True)\n",
        "        os.makedirs(self.MODEL_DIR, exist_ok=True)\n",
        "        os.makedirs(self.LOG_DIR, exist_ok=True)\n",
        "        logger.info(\"All directories created successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_BTv_CRmVoJ"
      },
      "source": [
        "üåø ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üåøüåø ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üåøüåø ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üåøüåø ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üåø\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4K3eIuCUmVoJ"
      },
      "source": [
        "### ***üß™ DataProcessor ‚Äî Base Class for Image Verification and Augmentation***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGAC_w1TTwq4"
      },
      "outputs": [],
      "source": [
        "class DataProcessor:\n",
        "    \"\"\"\n",
        "    Base class for handling image dataset preprocessing tasks such as:\n",
        "    - Directory management\n",
        "    - Image validation\n",
        "    - Data augmentation\n",
        "\n",
        "    Parameters:\n",
        "        config (Config): Configuration object containing paths and settings.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        \"\"\"\n",
        "        Initialize the DataProcessor with a config object.\n",
        "\n",
        "        Args:\n",
        "            config (Config): Configuration containing dataset paths and parameters.\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "\n",
        "    def clear_directory(self, dir_path: str):\n",
        "        \"\"\"\n",
        "        Delete all files in the given directory path (if it exists).\n",
        "\n",
        "        Args:\n",
        "            dir_path (str): Path to the directory to be cleared.\n",
        "\n",
        "        Notes:\n",
        "            - Only files are deleted (not subfolders).\n",
        "            - Logs error if file deletion fails.\n",
        "        \"\"\"\n",
        "        \"\"\"Empty a directory if it exists\"\"\"\n",
        "        if os.path.exists(dir_path):\n",
        "            for filename in os.listdir(dir_path):\n",
        "                file_path = os.path.join(dir_path, filename)\n",
        "                try:\n",
        "                    if os.path.isfile(file_path):\n",
        "                        os.unlink(file_path)\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Failed to delete {file_path}: {e}\")\n",
        "\n",
        "    def verify_image(self, img_path: str) -> bool:\n",
        "        \"\"\"\n",
        "        Verify whether a given image file is valid and can be opened.\n",
        "\n",
        "        Args:\n",
        "            img_path (str): Path to the image file.\n",
        "\n",
        "        Returns:\n",
        "            bool: True if the image is valid, False otherwise.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            img = Image.open(img_path)\n",
        "            img.verify()\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Invalid image {img_path}: {e}\")\n",
        "            return False\n",
        "\n",
        "    def augment_class(self, class_dir: str, augment_by: int):\n",
        "        \"\"\"\n",
        "        Augment images in a given class directory using basic image transformations.\n",
        "\n",
        "        Args:\n",
        "            class_dir (str): Path to the class-specific image directory.\n",
        "            augment_by (int): Number of augmented images to generate.\n",
        "        \"\"\"\n",
        "        datagen = ImageDataGenerator(\n",
        "            rotation_range=20,\n",
        "            width_shift_range=0.1,\n",
        "            height_shift_range=0.1,\n",
        "            shear_range=0.1,\n",
        "            zoom_range=0.2,\n",
        "            horizontal_flip=True,\n",
        "            fill_mode='reflect'\n",
        "        )\n",
        "\n",
        "        files = [f for f in os.listdir(class_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "        for i in tqdm(range(augment_by), desc=\"Augmenting class\"):\n",
        "            img_file = files[i % len(files)]\n",
        "            img_path = os.path.join(class_dir, img_file)\n",
        "\n",
        "            try:\n",
        "                img = Image.open(img_path)\n",
        "                img_array = np.array(img)\n",
        "                if img_array.ndim != 3 or img_array.shape[2] != 3:\n",
        "                    continue\n",
        "                img_array = np.expand_dims(img_array, axis=0)\n",
        "                augmented = datagen.random_transform(img_array[0])\n",
        "                save_path = os.path.join(class_dir, f\"aug_{i}_{img_file}\")\n",
        "                Image.fromarray(augmented.astype(np.uint8)).save(save_path)\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Failed to augment {img_path}: {e}\")\n",
        "                continue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASeuLGAemVoK"
      },
      "source": [
        "üåø ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üåøüåø ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üåøüåø ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üåøüåø ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üåø\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpoGYKA9mVoK"
      },
      "source": [
        "### ***üåø PlantTypeProcessor ‚Äî Class for Preparing and Balancing Plant Type Datasets***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DsOi8v-tnjS"
      },
      "outputs": [],
      "source": [
        "class PlantTypeProcessor(DataProcessor):\n",
        "    \"\"\"\n",
        "    A specialized data processor for plant type classification.\n",
        "\n",
        "    Features:\n",
        "    - Maps multiple dataset folder names to standardized plant class labels.\n",
        "    - Supports image verification and clean copying.\n",
        "    - Handles class balancing via safe data augmentation.\n",
        "    - Can resume from previously processed state.\n",
        "\n",
        "    Args:\n",
        "        config (PlantTypeConfig): Configuration object holding dataset paths and class info.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: PlantTypeConfig):\n",
        "        \"\"\"\n",
        "        Initialize the PlantTypeProcessor with a configuration object.\n",
        "\n",
        "        Args:\n",
        "            config (PlantTypeConfig): Config object with dataset directories and mappings.\n",
        "        \"\"\"\n",
        "        super().__init__(config)\n",
        "        self.config = config\n",
        "        self.plant_mapping = self._create_plant_mapping()\n",
        "        self.augmented_count = 0\n",
        "        self.skipped_count = 0\n",
        "        self.processed_images = set()\n",
        "\n",
        "    def _create_plant_mapping(self):\n",
        "        \"\"\"\n",
        "        Creates a dictionary mapping original dataset folder names (PlantVillage, PlantDoc)\n",
        "        to a unified list of standardized plant class names.\n",
        "\n",
        "        Returns:\n",
        "            dict: Mapping of original folder names to standardized plant types.\n",
        "        \"\"\"\n",
        "        return {\n",
        "            # PlantVillage mappings\n",
        "            'Pepper__bell___healthy': 'Bell_pepper',\n",
        "            'Pepper__bell___Bacterial_spot': 'Bell_pepper',\n",
        "            'Potato___healthy': 'Potato',\n",
        "            'Potato___Early_blight': 'Potato',\n",
        "            'Potato___Late_blight': 'Potato',\n",
        "            'Tomato_healthy': 'Tomato',\n",
        "            'Tomato_Bacterial_spot': 'Tomato',\n",
        "            'Tomato_Early_blight': 'Tomato',\n",
        "            'Tomato_Late_blight': 'Tomato',\n",
        "            'Tomato_Leaf_Mold': 'Tomato',\n",
        "            'Tomato_Septoria_leaf_spot': 'Tomato',\n",
        "            'Tomato__Tomato_mosaic_virus': 'Tomato',\n",
        "\n",
        "            # PlantDoc mappings\n",
        "            'Apple_leaf': 'Apple',\n",
        "            'Apple_rust_leaf': 'Apple',\n",
        "            'Apple_Scab_Leaf': 'Apple',\n",
        "            'Bell_pepper_leaf': 'Bell_pepper',\n",
        "            'Bell_pepper_leaf_spot': 'Bell_pepper',\n",
        "            'Cherry_leaf': 'Cherry',\n",
        "            'Corn_Gray_leaf_spot': 'Corn',\n",
        "            'Corn_leaf_blight': 'Corn',\n",
        "            'Corn_rust_leaf': 'Corn',\n",
        "            'grape_leaf': 'Grape',\n",
        "            'grape_leaf_black_rot': 'Grape',\n",
        "            'Peach_leaf': 'Peach',\n",
        "            'Raspberry_leaf': 'Raspberry',\n",
        "            'Soyabean_leaf': 'Soyabean',\n",
        "            'Strawberry_leaf': 'Strawberry',\n",
        "            'Tomato_leaf': 'Tomato',\n",
        "            'Tomato_leaf_bacterial_spot': 'Tomato',\n",
        "            'Tomato_leaf_late_blight': 'Tomato',\n",
        "            'Tomato_leaf_mosaic_virus': 'Tomato',\n",
        "            'Potato_leaf_early_blight': 'Potato',\n",
        "            'Potato_leaf_late_blight': 'Potato'\n",
        "        }\n",
        "\n",
        "    def process_dataset(self, resume=False):\n",
        "        \"\"\"\n",
        "        Main method to process the dataset:\n",
        "        - Clears old class folders unless in resume mode.\n",
        "        - Copies verified images from multiple sources to target folders.\n",
        "        - Balances dataset using augmentation.\n",
        "\n",
        "        Args:\n",
        "            resume (bool): If True, skips already processed images and avoids clearing folders.\n",
        "        \"\"\"\n",
        "        logger.info(\"Starting plant type dataset preparation\")\n",
        "\n",
        "        if not resume:\n",
        "            for cls_dir in self.config.class_dirs.values():\n",
        "                self.clear_directory(cls_dir)\n",
        "\n",
        "        counts = {cls: 0 for cls in self.config.PLANT_CLASSES}\n",
        "        source_dirs = {\n",
        "            'plantvillage': self.config.PLANTVILLAGE_DIR,\n",
        "            'plantdoc_train': self.config.PLANTDOC_TRAIN_DIR,\n",
        "            'plantdoc_test': self.config.PLANTDOC_TEST_DIR\n",
        "        }\n",
        "\n",
        "        for source_name, base_dir in source_dirs.items():\n",
        "            if not os.path.exists(base_dir):\n",
        "                logger.warning(f\"Source directory not found: {base_dir}\")\n",
        "                continue\n",
        "\n",
        "            for folder in os.listdir(base_dir):\n",
        "                if folder not in self.plant_mapping:\n",
        "                    continue\n",
        "\n",
        "                plant_class = self.plant_mapping[folder]\n",
        "                src_folder = os.path.join(base_dir, folder)\n",
        "\n",
        "                if not os.path.exists(src_folder):\n",
        "                    logger.warning(f\"Source folder not found: {src_folder}\")\n",
        "                    continue\n",
        "\n",
        "                for file in os.listdir(src_folder):\n",
        "                    if not file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                        continue\n",
        "\n",
        "                    src_path = os.path.join(src_folder, file)\n",
        "                    file_id = f\"{source_name}_{folder}_{file}\"\n",
        "\n",
        "                    if resume and file_id in self.processed_images:\n",
        "                        continue\n",
        "\n",
        "                    if self.verify_image(src_path):\n",
        "                        dest_path = os.path.join(\n",
        "                            self.config.class_dirs[plant_class],\n",
        "                            file_id.replace(\" \", \"_\")\n",
        "                        )\n",
        "                        shutil.copy2(src_path, dest_path)\n",
        "                        counts[plant_class] += 1\n",
        "                        self.processed_images.add(file_id)\n",
        "\n",
        "        self.balance_classes(counts, resume=resume)\n",
        "        logger.info(\"Dataset preparation complete\")\n",
        "\n",
        "    def balance_classes(self, counts: dict, resume=False):\n",
        "        \"\"\"\n",
        "        Balance the number of images per plant class using data augmentation.\n",
        "\n",
        "        Args:\n",
        "            counts (dict): Initial image count per class.\n",
        "            resume (bool): Whether to skip already augmented images.\n",
        "\n",
        "        Notes:\n",
        "            - Automatically skips classes with no original images.\n",
        "            - Prevents zero-division and skips over-augmented classes.\n",
        "        \"\"\"\n",
        "        current_counts = self.get_current_counts()\n",
        "\n",
        "        # Remove zero counts\n",
        "        valid_counts = {k:v for k,v in current_counts.items() if v > 0}\n",
        "        if not valid_counts:\n",
        "            raise ValueError(\"No valid images found in any class\")\n",
        "\n",
        "        max_count = max(valid_counts.values())\n",
        "\n",
        "        for plant_class, count in current_counts.items():\n",
        "            if count == 0:\n",
        "                logger.warning(f\"Skipping {plant_class} - no images\")\n",
        "                continue\n",
        "\n",
        "            if count >= max_count:\n",
        "                continue\n",
        "\n",
        "            self._safe_augment(\n",
        "                class_dir=self.config.class_dirs[plant_class],\n",
        "                current_count=count,\n",
        "                target_count=max_count,\n",
        "                resume=resume\n",
        "            )\n",
        "\n",
        "    def _safe_augment(self, class_dir: str, current_count: int, target_count: int, resume=False):\n",
        "        \"\"\"\n",
        "        Augments a plant class directory to reach the target number of images safely.\n",
        "\n",
        "        Args:\n",
        "            class_dir (str): Path to the class folder.\n",
        "            current_count (int): Number of existing (non-augmented) images.\n",
        "            target_count (int): Desired number of total images.\n",
        "            resume (bool): Skip already augmented files if True.\n",
        "\n",
        "        Returns:\n",
        "            bool: True if augmentation succeeded, False otherwise.\n",
        "\n",
        "        Notes:\n",
        "            - Uses Keras's ImageDataGenerator for augmentation.\n",
        "            - Skips grayscale or corrupted images.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            original_files = [\n",
        "                f for f in os.listdir(class_dir)\n",
        "                if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
        "                and not f.startswith('aug_')\n",
        "            ]\n",
        "\n",
        "            if not original_files:\n",
        "                return False\n",
        "\n",
        "            needed = max(0, target_count - current_count)\n",
        "            if needed == 0:\n",
        "                return True\n",
        "\n",
        "            datagen = ImageDataGenerator(\n",
        "                rotation_range=20,\n",
        "                width_shift_range=0.1,\n",
        "                height_shift_range=0.1,\n",
        "                zoom_range=0.2,\n",
        "                horizontal_flip=True,\n",
        "                fill_mode='reflect'\n",
        "            )\n",
        "\n",
        "            pbar = tqdm(total=needed, desc=f\"Augmenting {os.path.basename(class_dir)}\")\n",
        "            generated = 0\n",
        "\n",
        "            while generated < needed:\n",
        "                for img_file in original_files:\n",
        "                    if generated >= needed:\n",
        "                        break\n",
        "\n",
        "                    img_path = os.path.join(class_dir, img_file)\n",
        "                    try:\n",
        "                        img = Image.open(img_path)\n",
        "                        img_array = np.array(img)\n",
        "                        if img_array.ndim != 3:\n",
        "                            continue\n",
        "\n",
        "                        new_name = f\"aug_{generated}_{img_file}\"\n",
        "                        save_path = os.path.join(class_dir, new_name)\n",
        "\n",
        "                        img_array = np.expand_dims(img_array, axis=0)\n",
        "                        augmented = datagen.random_transform(img_array[0])\n",
        "                        Image.fromarray(augmented.astype(np.uint8)).save(save_path)\n",
        "\n",
        "                        generated += 1\n",
        "                        pbar.update(1)\n",
        "\n",
        "                    except Exception as e:\n",
        "                        logger.debug(f\"Skipping {img_path}: {str(e)}\")\n",
        "                        continue\n",
        "\n",
        "            pbar.close()\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Augmentation failed: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def get_current_counts(self, exclude_augmented=True):\n",
        "        \"\"\"\n",
        "        Returns the current image count for each class.\n",
        "\n",
        "        Args:\n",
        "            exclude_augmented (bool): If True, ignores files prefixed with 'aug_'.\n",
        "\n",
        "        Returns:\n",
        "            dict: Mapping of class names to image counts.\n",
        "        \"\"\"\n",
        "        counts = {}\n",
        "        for plant_class, class_dir in self.config.class_dirs.items():\n",
        "            if os.path.exists(class_dir):\n",
        "                counts[plant_class] = len([\n",
        "                    f for f in os.listdir(class_dir)\n",
        "                    if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
        "                    and (not exclude_augmented or not f.startswith('aug_'))\n",
        "                ])\n",
        "            else:\n",
        "                counts[plant_class] = 0\n",
        "        return counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ud1yAAkBmVoL"
      },
      "source": [
        "üåø ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üåøüåø ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üåøüåø ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üåøüåø ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üåø\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4o1sylKmVoL"
      },
      "source": [
        "### ***üå± Plant Type Trainer Class ‚Äì Efficient & Robust Classification Pipeline***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5ELnjS2eZg3"
      },
      "outputs": [],
      "source": [
        "class PlantTypeTrainer:\n",
        "    \"\"\"Enhanced Plant Type Classification Trainer with Robust Error Handling\"\"\"\n",
        "    \"\"\"Trainer class for plant type classification using EfficientNetV2M.\"\"\"\n",
        "\n",
        "    def __init__(self, config: PlantTypeConfig):\n",
        "        \"\"\"\n",
        "        Initialize the PlantTypeTrainer.\n",
        "\n",
        "        Args:\n",
        "            config (PlantTypeConfig): Configuration object containing model settings and paths.\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "        self.model = None\n",
        "        self.history = None\n",
        "        self.class_weights = None\n",
        "\n",
        "    def calculate_class_weights(self):\n",
        "        \"\"\"\n",
        "        Calculate class weights to handle class imbalance in the training dataset.\n",
        "\n",
        "        This method counts the number of images per class and computes weights inversely\n",
        "        proportional to the frequency of each class. It ensures no division by zero occurs.\n",
        "\n",
        "        Returns:\n",
        "            bool: True if weights were calculated successfully, False otherwise.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            class_counts = {}\n",
        "            for plant_class in self.config.PLANT_CLASSES:\n",
        "                class_dir = self.config.class_dirs[plant_class]\n",
        "                if os.path.exists(class_dir):\n",
        "                    count = len([f for f in os.listdir(class_dir)\n",
        "                               if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "                    # Ensure no zero counts\n",
        "                    class_counts[plant_class] = max(count, 1)\n",
        "                else:\n",
        "                    logger.warning(f\"Directory not found: {class_dir}\")\n",
        "                    # Default minimum count\n",
        "                    class_counts[plant_class] = 1\n",
        "\n",
        "            total = sum(class_counts.values())\n",
        "            self.class_weights = {\n",
        "                i: total / (len(class_counts) * count)\n",
        "                for i, (cls, count) in enumerate(class_counts.items())\n",
        "            }\n",
        "            logger.info(f\"Class weights calculated: {self.class_weights}\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            logger.error(f\"‚ùå Class weight calculation failed: {str(e)}\")\n",
        "            self.class_weights = None\n",
        "            return False\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"\n",
        "        Build the EfficientNetV2M-based plant type classification model.\n",
        "\n",
        "        This includes preprocessing, feature extraction with dropout and regularization,\n",
        "        and a softmax output layer. Compiles the model with Adam optimizer and\n",
        "        common classification metrics.\n",
        "\n",
        "        Returns:\n",
        "            bool: True if the model was built successfully, False otherwise.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            base_model = applications.EfficientNetV2M(\n",
        "                include_top=False,\n",
        "                weights='imagenet',\n",
        "                input_shape=(*self.config.IMG_SIZE, 3),\n",
        "                pooling='avg'\n",
        "            )\n",
        "            base_model.trainable = False\n",
        "\n",
        "            inputs = tf.keras.Input(shape=(*self.config.IMG_SIZE, 3))\n",
        "            x = applications.efficientnet_v2.preprocess_input(inputs)\n",
        "            x = base_model(x)\n",
        "\n",
        "            # Enhanced feature extraction with regularization\n",
        "            x = layers.Dropout(self.config.DROPOUT_RATE)(x)\n",
        "            x = layers.Dense(1024, activation='swish',\n",
        "                           kernel_regularizer=regularizers.l2(self.config.L2_REG))(x)\n",
        "            x = layers.BatchNormalization()(x)\n",
        "            x = layers.Dropout(self.config.DROPOUT_RATE/2)(x)\n",
        "\n",
        "            outputs = layers.Dense(\n",
        "                len(self.config.PLANT_CLASSES),\n",
        "                activation='softmax',\n",
        "                kernel_regularizer=regularizers.l2(self.config.L2_REG)\n",
        "            )(x)\n",
        "\n",
        "            self.model = models.Model(inputs, outputs)\n",
        "\n",
        "            # Optimized compilation\n",
        "            self.model.compile(\n",
        "                optimizer=optimizers.Adam(learning_rate=1e-4),\n",
        "                loss=tf.keras.losses.CategoricalCrossentropy(\n",
        "                    label_smoothing=self.config.LABEL_SMOOTHING\n",
        "                ),\n",
        "                metrics=[\n",
        "                    'accuracy',\n",
        "                    tf.keras.metrics.Precision(name='precision'),\n",
        "                    tf.keras.metrics.Recall(name='recall'),\n",
        "                    tf.keras.metrics.AUC(name='auc')\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            logger.info(\"‚úÖ Plant type model built successfully\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"‚ùå Model building failed: {str(e)}\")\n",
        "            self.model = None\n",
        "            return False\n",
        "\n",
        "    def get_callbacks(self):\n",
        "        \"\"\"\n",
        "        Define and return a list of Keras callbacks for training.\n",
        "\n",
        "        Includes:\n",
        "        - EarlyStopping based on AUC\n",
        "        - ModelCheckpoint for saving the best model\n",
        "        - ReduceLROnPlateau for learning rate adjustment\n",
        "        - TensorBoard logging\n",
        "        - CSVLogger for training logs\n",
        "\n",
        "        Returns:\n",
        "            list: A list of configured Keras callback instances.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            os.makedirs(self.config.MODEL_DIR, exist_ok=True)\n",
        "            os.makedirs(self.config.LOG_DIR, exist_ok=True)\n",
        "\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "            return [\n",
        "                callbacks.EarlyStopping(\n",
        "                    monitor='val_auc',\n",
        "                    mode='max',\n",
        "                    patience=15,\n",
        "                    restore_best_weights=True,\n",
        "                    verbose=1\n",
        "                ),\n",
        "                callbacks.ModelCheckpoint(\n",
        "                    filepath=os.path.join(self.config.MODEL_DIR, f'best_model_{timestamp}.keras'),\n",
        "                    monitor='val_auc',\n",
        "                    save_best_only=True,\n",
        "                    mode='max'\n",
        "                ),\n",
        "                callbacks.ReduceLROnPlateau(\n",
        "                    monitor='val_loss',\n",
        "                    factor=0.5,\n",
        "                    patience=5,\n",
        "                    min_lr=1e-6,\n",
        "                    verbose=1\n",
        "                ),\n",
        "                callbacks.TensorBoard(\n",
        "                    log_dir=os.path.join(self.config.LOG_DIR, timestamp),\n",
        "                    histogram_freq=1\n",
        "                ),\n",
        "                callbacks.CSVLogger(\n",
        "                    os.path.join(self.config.LOG_DIR, f'training_{timestamp}.csv')\n",
        "                )\n",
        "            ]\n",
        "        except Exception as e:\n",
        "            logger.error(f\"‚ùå Callback setup failed: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def create_data_pipeline(self, subset: str) -> tf.data.Dataset:\n",
        "        \"\"\"\n",
        "        Create and return a TensorFlow data pipeline for training or validation.\n",
        "\n",
        "        Applies image loading, optional data augmentation (only for training),\n",
        "        and prefetching for performance.\n",
        "\n",
        "        Args:\n",
        "            subset (str): One of 'training' or 'validation'.\n",
        "\n",
        "        Returns:\n",
        "            tf.data.Dataset: The configured dataset pipeline.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if not os.path.exists(self.config.DATA_DIR):\n",
        "                raise FileNotFoundError(f\"Data directory not found: {self.config.DATA_DIR}\")\n",
        "\n",
        "            ds = tf.keras.utils.image_dataset_from_directory(\n",
        "                self.config.DATA_DIR,\n",
        "                validation_split=0.2,\n",
        "                subset=subset,\n",
        "                seed=self.config.SEED,\n",
        "                image_size=self.config.IMG_SIZE,\n",
        "                batch_size=self.config.BATCH_SIZE,\n",
        "                label_mode='categorical'\n",
        "            )\n",
        "\n",
        "            if subset == 'training':\n",
        "                aug = tf.keras.Sequential([\n",
        "                    layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "                    layers.RandomRotation(0.2),\n",
        "                    layers.RandomZoom(0.2),\n",
        "                    layers.RandomContrast(0.1),\n",
        "                    layers.RandomTranslation(0.1, 0.1)\n",
        "                ])\n",
        "                ds = ds.map(lambda x, y: (aug(x, training=True), y),\n",
        "                          num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "            return ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"‚ùå Data pipeline creation failed: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Execute the training process in three progressive phases:\n",
        "\n",
        "        Phase 1: Train with frozen base model.\n",
        "        Phase 2: Partially unfreeze and fine-tune the last 20 layers.\n",
        "        Phase 3: Fully unfreeze base model for final fine-tuning.\n",
        "\n",
        "        Includes robust error handling at each phase and utilizes calculated class weights\n",
        "        and callbacks.\n",
        "\n",
        "        Returns:\n",
        "            History: A Keras History object containing training metrics.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Initial validation\n",
        "            if not self.build_model():\n",
        "                raise RuntimeError(\"Model building failed\")\n",
        "\n",
        "            if not self.calculate_class_weights():\n",
        "                logger.warning(\"Proceeding without class weights\")\n",
        "                self.class_weights = None\n",
        "\n",
        "            train_ds = self.create_data_pipeline('training')\n",
        "            val_ds = self.create_data_pipeline('validation')\n",
        "            callbacks = self.get_callbacks()\n",
        "\n",
        "            if not callbacks:\n",
        "                raise RuntimeError(\"Callback setup failed\")\n",
        "\n",
        "            # Phase 1: Frozen base training\n",
        "            logger.info(\"üöÄ Phase 1: Frozen base training\")\n",
        "            self.history = self.model.fit(\n",
        "                train_ds,\n",
        "                validation_data=val_ds,\n",
        "                epochs=int(self.config.EPOCHS * 0.7),\n",
        "                callbacks=callbacks,\n",
        "                class_weight=self.class_weights,\n",
        "                verbose=1\n",
        "            )\n",
        "\n",
        "            # Phase 2: Partial unfreezing (with error handling)\n",
        "            logger.info(\"üîß Phase 2: Partial unfreezing\")\n",
        "            try:\n",
        "                # Corrected layer name with hyphen\n",
        "                base_model = self.model.get_layer('efficientnetv2-m')\n",
        "                for layer in base_model.layers[-20:]:\n",
        "                    layer.trainable = True\n",
        "            except Exception as e:\n",
        "                logger.error(f\"‚ùå Partial unfreezing failed: {str(e)}\")\n",
        "                logger.info(\"Proceeding without unfreezing\")\n",
        "                pass\n",
        "\n",
        "            self.model.compile(\n",
        "                optimizer=optimizers.Adam(1e-5),\n",
        "                loss=tf.keras.losses.CategoricalCrossentropy(\n",
        "                    label_smoothing=self.config.LABEL_SMOOTHING\n",
        "                ),\n",
        "                metrics=[\n",
        "                    'accuracy',\n",
        "                    tf.keras.metrics.Precision(name='precision'),\n",
        "                    tf.keras.metrics.Recall(name='recall'),\n",
        "                    tf.keras.metrics.AUC(name='auc')\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            self.model.fit(\n",
        "                train_ds,\n",
        "                validation_data=val_ds,\n",
        "                epochs=int(self.config.EPOCHS * 0.2),\n",
        "                callbacks=callbacks,\n",
        "                class_weight=self.class_weights,\n",
        "                verbose=1\n",
        "            )\n",
        "\n",
        "            # Phase 3: Full fine-tuning (with error handling)\n",
        "            logger.info(\"üîßüîß Phase 3: Full fine-tuning\")\n",
        "            try:\n",
        "                base_model = self.model.get_layer('efficientnetv2-m')\n",
        "                base_model.trainable = True\n",
        "            except Exception as e:\n",
        "                logger.error(f\"‚ùå Full unfreezing failed: {str(e)}\")\n",
        "                logger.info(\"Proceeding without full fine-tuning\")\n",
        "                pass\n",
        "\n",
        "            self.model.compile(\n",
        "                optimizer=optimizers.Adam(1e-6),\n",
        "                loss=tf.keras.losses.CategoricalCrossentropy(\n",
        "                    label_smoothing=self.config.LABEL_SMOOTHING\n",
        "                ),\n",
        "                metrics=[\n",
        "                    'accuracy',\n",
        "                    tf.keras.metrics.Precision(name='precision'),\n",
        "                    tf.keras.metrics.Recall(name='recall'),\n",
        "                    tf.keras.metrics.AUC(name='auc')\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            self.model.fit(\n",
        "                train_ds,\n",
        "                validation_data=val_ds,\n",
        "                epochs=int(self.config.EPOCHS * 0.1),\n",
        "                callbacks=callbacks,\n",
        "                class_weight=self.class_weights,\n",
        "                verbose=1\n",
        "            )\n",
        "\n",
        "            logger.info(\"‚úÖ Training completed successfully\")\n",
        "            return self.history\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"‚ùå Training failed: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def save_model(self):\n",
        "        \"\"\"\n",
        "        Save the trained model in both Keras (.keras) and TensorFlow SavedModel formats.\n",
        "\n",
        "        Ensures model directory creation and verifies the saved paths exist.\n",
        "\n",
        "        Returns:\n",
        "            str or None: The primary Keras model path if successful, None otherwise.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            os.makedirs(self.config.MODEL_DIR, exist_ok=True)\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "            # Save as .keras format (recommended)\n",
        "            keras_path = os.path.join(self.config.MODEL_DIR, f'plant_type_model_{timestamp}.keras')\n",
        "            self.model.save(keras_path, save_format='keras')\n",
        "\n",
        "            # Additional save as SavedModel format (for compatibility)\n",
        "            savedmodel_path = os.path.join(self.config.MODEL_DIR, f'plant_type_model_{timestamp}')\n",
        "            self.model.save(savedmodel_path, save_format='tf')\n",
        "\n",
        "            # Verify saves\n",
        "            if not (os.path.exists(keras_path) and os.path.exists(savedmodel_path)):\n",
        "                raise RuntimeError(\"Model files were not created properly\")\n",
        "\n",
        "            logger.info(f\"‚úÖ Model saved successfully to:\\n- {keras_path}\\n- {savedmodel_path}\")\n",
        "            return keras_path  # Return primary save path\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"‚ùå Model saving failed: {str(e)}\")\n",
        "            return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNKK0zYNmVoM"
      },
      "source": [
        "üåø ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üåøüåø ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üåøüåø ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üåøüåø ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üåø\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HLu4LM6mVoM"
      },
      "source": [
        "### ***üèãÔ∏è‚Äç‚ôÇÔ∏è Training Pipeline: Plant Type Classifier (Model 3)***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMJMVyMtea4t"
      },
      "outputs": [],
      "source": [
        "def main_train_3(resume=True):\n",
        "    \"\"\"\n",
        "    Main pipeline function for training the Plant Type Classifier (Model 3).\n",
        "\n",
        "    This function initializes the configuration, processor, and trainer, then executes the full\n",
        "    training pipeline including model training, model saving, and saving the training history.\n",
        "\n",
        "    Args:\n",
        "        resume (bool): Placeholder for future implementation (e.g., resuming from checkpoints).\n",
        "                       Currently unused.\n",
        "\n",
        "    Returns:\n",
        "        str or None: Path to the saved model if training succeeds, otherwise None.\n",
        "    \"\"\"\n",
        "    config = PlantTypeConfig()\n",
        "    processor = PlantTypeProcessor(config)\n",
        "    trainer = PlantTypeTrainer(config)\n",
        "\n",
        "    try:\n",
        "        # Training\n",
        "        history = trainer.train()\n",
        "        if history is None:\n",
        "            raise RuntimeError(\"Training failed\")\n",
        "\n",
        "        # Save model\n",
        "        model_path = trainer.save_model()\n",
        "        if not model_path:\n",
        "            raise RuntimeError(\"Model saving failed\")\n",
        "\n",
        "        print(f\"\\nüåü Training complete! Model saved to:\\n{model_path}\")\n",
        "\n",
        "        # Save training history\n",
        "        history_path = os.path.join(config.LOG_DIR, f'training_history_{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}.json')\n",
        "        with open(history_path, 'w') as f:\n",
        "            json.dump(history.history, f)\n",
        "        print(f\"Training history saved to: {history_path}\")\n",
        "\n",
        "        return model_path\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Pipeline failed: {str(e)}\")\n",
        "        print(f\"Error: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Cgz0rDHmVoM"
      },
      "source": [
        "üåø ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üåøüåø ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üåøüåø ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üåøüåø ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üåø\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qY1yYGWwmVoM"
      },
      "source": [
        "### ***Train the Plant Type Classifier (Model 3)***\n",
        "‚¨áÔ∏èWe initiate the training process for **Model 3**, responsible for identifying the type of plant in a valid and infected image. The model is trained using a custom `PlantTypeTrainer` pipeline, which includes data preprocessing, augmentation, model compilation, and training. Once training is complete, the model is saved in `.keras` format, and its path is returned for future use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMslR_rAmVoN"
      },
      "outputs": [],
      "source": [
        "# 1. Train and get .keras model path\n",
        "model_path = main_train_3(resume=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yk3eXht2mVoN"
      },
      "source": [
        "üå∏ ‚úø ‚úø ‚úø ‚úø ‚úø üå∏üå∏ ‚úø ‚úø ‚úø ‚úø ‚úø üå∏üå∏ ‚úø ‚úø ‚úø ‚úø ‚úø üå∏üå∏ ‚úø ‚úø ‚úø ‚úø ‚úø üå∏üå∏ ‚úø ‚úø ‚úø ‚úø ‚úø üå∏üå∏ ‚úø ‚úø ‚úø ‚úø ‚úø üå∏üå∏ ‚úø ‚úø ‚úø ‚úø ‚úø üå∏üå∏ ‚úø ‚úø ‚úø ‚úø ‚úø üå∏üå∏ ‚úø ‚úø ‚úø ‚úø ‚úø üå∏üå∏ ‚úø ‚úø ‚úø ‚úø ‚úø üå∏üå∏ ‚úø ‚úø ‚úø ‚úø ‚úø üå∏\n",
        "\n",
        "üåø ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üåøüåø ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üåøüåø ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üåøüåø ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üåø\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWBjnCVPmVoN"
      },
      "source": [
        "<div style=\"text-align: center;\">\n",
        "  <h1><b>üèÅ Training Session Completed Successfully</b></h1>\n",
        "</div>\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}