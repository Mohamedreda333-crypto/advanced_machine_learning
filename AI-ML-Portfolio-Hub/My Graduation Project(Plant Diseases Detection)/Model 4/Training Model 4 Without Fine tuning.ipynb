{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"text-align: center;\">\n",
        "  <h1><b>🌱 Plant Disease Classifier — Model 4 (bacterial_spot',\n",
        "            'powdery_mildew', 'gray_leaf_spot', 'common_rust',.......)🧠</b></h1>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 🖼️ Model 4 Diseases Images\n",
        "\n",
        "<div style=\"display: flex; justify-content: center; align-items: center; gap: 20px;\">\n",
        "\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"https://storage.googleapis.com/kagglesdsdata/datasets/277323/658267/color/Tomato___Bacterial_spot/00639d29-2d1a-4fcf-9bd3-a2b3109c74c4___UF.GRC_BS_Lab%20Leaf%201054.JPG?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20250610%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20250610T041845Z&X-Goog-Expires=345600&X-Goog-SignedHeaders=host&X-Goog-Signature=9d5a274b2d8a1bdb38f4cca593051b599c88d9f9085e84f8211a16aee4e3c6fa5b86a84f8dc747e697e9be6e2bd0a746a53fd7f52b33c67b58b26377d8e1afd0953206fed910dab1949f1850aa8bfd2a0d11b7ac527258e09d0b8f0396d88013864e1d31079b62d908ead25336ae09d16c6dcb9a30113ae5b58dc4926d7057c6ae847d4763e54b73356247fbc2cb88f1bc87b1d5fe53a2efcd8b60c11494df98d43f7a14a52be0795ed059fd001326e3468e94c3643162316a850fa261e8db2bbebb5a10d96a514dab738c3323595013330e625d996fbee5db8f1c901c3af5738d8138a45bb1032ab432ac4a31f2538f90fb855337808679a3e31b3c4194f99b\" width=\"200\"/>\n",
        "    <p>✅ Tomato Bacterial Spot 🍅 </p>\n",
        "  </div>\n",
        "\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"https://storage.googleapis.com/kagglesdsdata/datasets/277323/658267/color/Corn_%28maize%29___Common_rust_/RS_Rust%201565.JPG?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20250610%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20250610T041954Z&X-Goog-Expires=345600&X-Goog-SignedHeaders=host&X-Goog-Signature=8d843cf6db9a32d1ee30b91a4c5d06c3cd07ce3fdbf8d589bcecd0b5b13f6dfe266e61ecc9cf74cc172be9ab5f7fbb2ec89361db36f46305281bbd347df6f299fbbfa1a33c36522430d9f818fe92887eef2abcef64bf3db85cf3c873cdcaa548328ca65f48c51cb25733bd80d70bb6c415af918717b1ce375075f306ef8ebd139ddcec1df31f881de73f9de8daa3a3e654d7b07f3acc28a7bbc004f0416f7734e20b513510350ab19d7c2e48c7c744e3d19ad291bfe2216322939be3711a8ed1e54edfeb3ac3e5a281ae6c0c7afe815535972e88f9664180263292fddedac1cf662a804bbb70944a6d503c194579368b679f894841ce9bc8fd775a5fb7c3621d\n",
        "    \" width=\"200\"/>\n",
        "    <p>✅ Corn Northern Leaf Blight 🌽</p>\n",
        "  </div>\n",
        "\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"https://storage.googleapis.com/kagglesdsdata/datasets/277323/658267/color/Apple___Cedar_apple_rust/112cfc79-00ab-4920-a235-977c134bdb5d___FREC_C.Rust%204365.JPG?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20250610%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20250610T042207Z&X-Goog-Expires=345600&X-Goog-SignedHeaders=host&X-Goog-Signature=771746e3537bdb06267693455e084ba91be86a3618cdfd81740f57d258519afd37d141a55705ebc3a2e2e8775ae7114391695ebacc75f235f0e20a43642e3f933c17106873be5879477fa7af9849daf578a56e849c5f4db3560a0d56142d0100ddc47bf3b073bc2f77cac199b63e67e219e72aa5b2ed2e89d94a4b0c112d6d4f0129a0f2c7971e1539e15a580fc3f0a1938b5158f05e6991f0791a91dc466930211e1cb3d60df002e3d883627326de2d34003a9dd79d151068272a2738b5a0cecbe9865f0ca70a1491df36d501c729c7f1c2bbd4af489c6fd5eaaa25b2dc86954f187ea76a5109857fe471dba680e72fd3b754953e2f2a93b5414cd168a39dd1\" width=\"200\"/>\n",
        "    <p>✅ Apple Cadar Rust 🍎</p>\n",
        "  </div>\n",
        "\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"https://storage.googleapis.com/kagglesdsdata/datasets/277323/658267/color/Grape___Esca_%28Black_Measles%29/011f307f-e06b-4604-9419-d940f7b00290___FAM_B.Msls%201096.JPG?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20250610%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20250610T042122Z&X-Goog-Expires=345600&X-Goog-SignedHeaders=host&X-Goog-Signature=981c40a4a9a8d6824dad6c4d709c1fc1b3e331ed8c4032ddc93a4ce504cc21b9e9c29ef469d541eb7b85a52aaef0bbd654085583deb5d62a1e0a7a8f166c81296663c690f68812bcc38ba39cb591ef66ac7b9cfb50d244d1a78b8296be27dc69826ad0ad1972aae5fbd3f06efb9237a042a2b4a5fcc1067a86a91d9f76f2b0da681edcad462802f1791e345524bb4f0dcea2bbf9e41420d6a9301f2ba0fe58d3bbed1bb56c4a05722da9dbedc408f43d5c05268dbddfd15a1f691893741fca6561a1e5da55111afbcb0e91cb7c0e731fefe38cb81d2d7ecee6ec09c1cac2190a975fb4c7541f2ce0303a1eec4a453cb2a79f08c2207982fdef2d0d24101a1909\" width=\"200\"/>\n",
        "    <p>✅ Grape Esca 🍇 </p>\n",
        "  </div>\n",
        "\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "🌸 ✿ ✿ ✿ ✿ ✿ 🌸🌸 ✿ ✿ ✿ ✿ ✿ 🌸🌸 ✿ ✿ ✿ ✿ ✿ 🌸🌸 ✿ ✿ ✿ ✿ ✿ 🌸🌸 ✿ ✿ ✿ ✿ ✿ 🌸🌸 ✿ ✿ ✿ ✿ ✿ 🌸🌸 ✿ ✿ ✿ ✿ ✿ 🌸🌸 ✿ ✿ ✿ ✿ ✿ 🌸🌸 ✿ ✿ ✿ ✿ ✿ 🌸🌸 ✿ ✿ ✿ ✿ ✿ 🌸🌸 ✿ ✿ ✿ ✿ ✿ 🌸\n",
        "\n",
        "🌿 ───────────────────────────── 🌿🌿 ───────────────────────────── 🌿🌿 ───────────────────────────── 🌿🌿 ───────────────────────────── 🌿\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5zGc-5YVkZy",
        "outputId": "c8fc920e-caf8-4878-d32e-7ba7ce73d2ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opendatasets in /usr/local/lib/python3.11/dist-packages (0.1.22)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from opendatasets) (4.67.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (from opendatasets) (1.7.4.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from opendatasets) (8.2.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2025.4.26)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (5.29.4)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (0.5.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install opendatasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ***🌱Download PlantDoc Dataset from Kaggle***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzqVFvx0Vm-C",
        "outputId": "e3dde94a-8984-4541-8409-f3e13ef641c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipping, found downloaded files in \"./plantdoc-dataset\" (use force=True to force download)\n"
          ]
        }
      ],
      "source": [
        "import opendatasets as od\n",
        "od.download (\"https://www.kaggle.com/datasets/nirmalsankalana/plantdoc-dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ***🌱Download PlantVillage Dataset from Kaggle***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJlpQJL7Vo3s",
        "outputId": "42ef1836-dcb3-481e-903f-1d7201f09b15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipping, found downloaded files in \"./plantdisease\" (use force=True to force download)\n"
          ]
        }
      ],
      "source": [
        "import opendatasets as od\n",
        "od.download (\"https://www.kaggle.com/datasets/emmarex/plantdisease\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ***🚀 Mount Google Drive in Colab***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeYrDUKKVrhB",
        "outputId": "6b4ead1f-4f72-4322-cf37-f5cb4be91f9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ***📂 Key Project Directory Paths***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "m2lYd3VLVvFG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, callbacks, optimizers, applications, regularizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.utils import class_weight\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import psutil\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('plant_disease_classification.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Enable memory optimizations\n",
        "policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
        "tf.keras.mixed_precision.set_global_policy(policy)\n",
        "tf.config.optimizer.set_jit(True)\n",
        "\n",
        "# Configure GPU memory growth\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    except RuntimeError as e:\n",
        "        logger.warning(f\"Could not set GPU memory growth: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ***🧠 DiseaseConfig Class – High Accuracy + Memory Efficient Setup***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbMLknHWWA-7"
      },
      "outputs": [],
      "source": [
        "class DiseaseConfig:\n",
        "    \"\"\"Optimized configuration for high accuracy model with memory efficiency\"\"\"\n",
        "    def __init__(self):\n",
        "        self.BASE_DIR = \"/content/drive/MyDrive/Graduation Project\"\n",
        "        self.DATA_DIR = os.path.join(self.BASE_DIR, \"disease_data\")\n",
        "\n",
        "        # Ensure these match your actual directory names exactly\n",
        "        self.ALL_POSSIBLE_CLASSES = {\n",
        "            'scab', 'black_rot', 'cedar_apple_rust', 'bacterial_spot',\n",
        "            'powdery_mildew', 'gray_leaf_spot', 'common_rust',\n",
        "            'northern_leaf_blight', 'esca', 'leaf_blight', 'early_blight',\n",
        "            'late_blight', 'leaf_scorch', 'leaf_mold', 'septoria_leaf_spot',\n",
        "            'target_spot', 'mosaic_virus', 'yellow_leaf_curl_virus'\n",
        "        }\n",
        "\n",
        "        # Optimized model parameters\n",
        "        self.IMG_SIZE = (384, 384)  \n",
        "        self.BATCH_SIZE = 32\n",
        "        self.EPOCHS = 100\n",
        "        self.SEED = 42\n",
        "        self.MIN_SAMPLES_PER_CLASS = 1500\n",
        "\n",
        "        # Enhanced Regularization\n",
        "        self.DROPOUT_RATE = 0.5\n",
        "        self.L2_REG = 0.0001\n",
        "        self.LABEL_SMOOTHING = 0.1  \n",
        "        self.STOCHASTIC_DEPTH_RATE = 0.2  \n",
        "\n",
        "        # Augmentation\n",
        "        self.AUGMENTATION_FACTOR = 10\n",
        "\n",
        "        # Paths\n",
        "        self.MODEL_NAME = \"optimized_model_v4\"\n",
        "        self.MODEL_DIR = os.path.join(self.BASE_DIR, f\"saved_models/{self.MODEL_NAME}\")\n",
        "        self.LOG_DIR = os.path.join(self.BASE_DIR, f\"training_logs/{self.MODEL_NAME}\")\n",
        "        self.FINAL_MODEL_PATH = os.path.join(self.MODEL_DIR, \"final_model.keras\")\n",
        "        self.SAVED_MODEL_PATH = os.path.join(self.MODEL_DIR, \"final_saved_model\")\n",
        "\n",
        "        self.ACTIVE_CLASSES = set()\n",
        "        self.class_dirs = {}\n",
        "        self.setup_dirs()\n",
        "        # Call the new verification method during initialization\n",
        "        self.verify_directory_structure()\n",
        "        self.update_active_classes()\n",
        "\n",
        "    def setup_dirs(self):\n",
        "        \"\"\"\n",
        "        Creates essential directories for data, model, and logs if they do not exist.\n",
        "\n",
        "        Arguments:\n",
        "            None\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        os.makedirs(self.DATA_DIR, exist_ok=True)\n",
        "        os.makedirs(self.MODEL_DIR, exist_ok=True)\n",
        "        os.makedirs(self.LOG_DIR, exist_ok=True)\n",
        "\n",
        "    def update_active_classes(self):\n",
        "        \"\"\"\n",
        "        Updates the ACTIVE_CLASSES attribute to include only those class folders\n",
        "        that both exist in the dataset and are listed in ALL_POSSIBLE_CLASSES.\n",
        "\n",
        "        Arguments:\n",
        "            None\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        if os.path.exists(self.DATA_DIR):\n",
        "            available_classes = set()\n",
        "            for item in os.listdir(self.DATA_DIR):\n",
        "                full_path = os.path.join(self.DATA_DIR, item)\n",
        "                # Check if it's a directory and if it's in ALL_POSSIBLE_CLASSES\n",
        "                if os.path.isdir(full_path) and item in self.ALL_POSSIBLE_CLASSES:\n",
        "                    # Check if the directory contains any image files\n",
        "                    if any(fname.lower().endswith(('.jpg', '.jpeg', '.png')) for fname in os.listdir(full_path)):\n",
        "                        available_classes.add(item)\n",
        "                        self.class_dirs[item] = full_path\n",
        "            # Update ACTIVE_CLASSES to only include valid directories with images that are in ALL_POSSIBLE_CLASSES\n",
        "            self.ACTIVE_CLASSES = available_classes if available_classes else self.ALL_POSSIBLE_CLASSES.copy()\n",
        "\n",
        "\n",
        "    # Add this method to verify directory structure\n",
        "    def verify_directory_structure(self):\n",
        "        \"\"\"\n",
        "        Verifies that the dataset directory exists and checks for the presence\n",
        "        of valid class subdirectories. Logs warnings for missing or unexpected folders.\n",
        "\n",
        "        Arguments:\n",
        "            None\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "\n",
        "        Raises:\n",
        "            FileNotFoundError: If DATA_DIR does not exist.\n",
        "            OSError: If an error occurs while listing contents of DATA_DIR.\n",
        "        \"\"\"\n",
        "        if not os.path.exists(self.DATA_DIR):\n",
        "            raise FileNotFoundError(f\"Data directory {self.DATA_DIR} not found. Please ensure your dataset is downloaded and extracted to this location.\")\n",
        "\n",
        "        try:\n",
        "            dir_items = os.listdir(self.DATA_DIR)\n",
        "        except OSError as e:\n",
        "            raise OSError(f\"Error listing contents of data directory {self.DATA_DIR}: {e}\")\n",
        "\n",
        "        # Find all subdirectories in the data directory\n",
        "        valid_classes_in_dir = {d for d in dir_items if os.path.isdir(os.path.join(self.DATA_DIR, d))}\n",
        "\n",
        "        if not valid_classes_in_dir:\n",
        "            logger.warning(f\"No subdirectories found in {self.DATA_DIR}. Please ensure your dataset is organized into subfolders, where each subfolder name is a class name.\")\n",
        "            # Depending on your pipeline, you might want to raise an error here\n",
        "            # or handle this gracefully. For now, we log a warning.\n",
        "            return # Exit if no subdirectories are found\n",
        "\n",
        "        # Check for directories that are in the data directory but not in ALL_POSSIBLE_CLASSES\n",
        "        extra_dirs = valid_classes_in_dir - self.ALL_POSSIBLE_CLASSES\n",
        "        if extra_dirs:\n",
        "            logger.warning(f\"Unexpected directories found in {self.DATA_DIR}: {extra_dirs}. These directories will be ignored.\")\n",
        "\n",
        "        # Check for classes defined in ALL_POSSIBLE_CLASSES but not found as directories\n",
        "        missing_dirs = self.ALL_POSSIBLE_CLASSES - valid_classes_in_dir\n",
        "        if missing_dirs:\n",
        "            logger.warning(f\"Expected directories missing from {self.DATA_DIR}: {missing_dirs}. These classes will not be included in training.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ***🔁 StochasticDepth Layer – Regularization via Random Skipping***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVxJ1OmScPES"
      },
      "outputs": [],
      "source": [
        "class StochasticDepth(layers.Layer):\n",
        "    class StochasticDepth(layers.Layer):\n",
        "        \"\"\"\n",
        "        Stochastic Depth layer for regularization.\n",
        "\n",
        "        This layer randomly drops entire residual paths during training as a form of\n",
        "        regularization (a technique proposed in \"Deep Networks with Stochastic Depth\").\n",
        "\n",
        "        Args:\n",
        "            drop_rate (float): Probability of dropping a residual path (between 0 and 1).\n",
        "            **kwargs: Additional keyword arguments passed to the base Layer class.\n",
        "\n",
        "        Usage:\n",
        "            Used within a residual connection block to probabilistically bypass the block\n",
        "            during training, improving model generalization.\n",
        "        \"\"\"\n",
        "    def __init__(self, drop_rate, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.drop_rate = drop_rate\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        if not training:\n",
        "            return inputs\n",
        "\n",
        "        keep_prob = 1 - self.drop_rate\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        random_tensor = keep_prob\n",
        "        random_tensor += tf.random.uniform([batch_size, 1, 1, 1], dtype=inputs.dtype)\n",
        "        binary_tensor = tf.floor(random_tensor)\n",
        "        output = tf.math.divide(inputs, keep_prob) * binary_tensor\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ***🧪 AugmentationEngine – Advanced Data Augmentation with Memory Efficiency***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kg3_hhAZfbFV"
      },
      "outputs": [],
      "source": [
        "class AugmentationEngine:\n",
        "    \"\"\"\n",
        "    Enhanced augmentation engine for image preprocessing and data augmentation.\n",
        "\n",
        "    This engine combines Keras' ImageDataGenerator with custom augmentation \n",
        "    techniques such as color distortion, blurring, sharpening, and noise addition, \n",
        "    while maintaining memory efficiency.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        \"\"\"\n",
        "        Initialize the AugmentationEngine with configuration.\n",
        "\n",
        "        Arguments:\n",
        "            config (object): Configuration object with attributes like IMG_SIZE.\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "        self.setup_augmentation_pipeline()\n",
        "\n",
        "    def setup_augmentation_pipeline(self):\n",
        "        \"\"\"\n",
        "        Configure the base augmentation pipeline using Keras' ImageDataGenerator.\n",
        "        \n",
        "        Arguments:\n",
        "            None\n",
        "\n",
        "        Output:\n",
        "            None\n",
        "        \"\"\"\n",
        "        self.datagen = ImageDataGenerator(\n",
        "            rotation_range=45,\n",
        "            width_shift_range=0.2,\n",
        "            height_shift_range=0.2,\n",
        "            shear_range=0.2,\n",
        "            zoom_range=[0.7, 1.3],\n",
        "            horizontal_flip=True,\n",
        "            vertical_flip=True,\n",
        "            brightness_range=[0.5, 1.5],\n",
        "            channel_shift_range=50.0,\n",
        "            fill_mode='reflect',\n",
        "            preprocessing_function=lambda x: self.random_color_distortion(x)\n",
        "        )\n",
        "\n",
        "    def random_color_distortion(self, image, s=1.0):\n",
        "        \"\"\"\n",
        "        Apply random color distortion: brightness, contrast, saturation, and hue.\n",
        "\n",
        "        Arguments:\n",
        "            image (np.ndarray or tf.Tensor): Input image to distort.\n",
        "            s (float): Strength multiplier for distortions. Default is 1.0.\n",
        "\n",
        "        Output:\n",
        "            np.ndarray: Color-distorted image, clipped to [0, 255].\n",
        "        \"\"\"\n",
        "        image = tf.image.random_brightness(image, max_delta=0.8 * s)\n",
        "        image = tf.image.random_contrast(image, lower=1 - 0.8 * s, upper=1 + 0.8 * s)\n",
        "        image = tf.image.random_saturation(image, lower=1 - 0.8 * s, upper=1 + 0.8 * s)\n",
        "        image = tf.image.random_hue(image, max_delta=0.2 * s)\n",
        "        image = tf.clip_by_value(image, 0, 255)\n",
        "        return image.numpy() if tf.is_tensor(image) else image\n",
        "\n",
        "    def apply_random_filter(self, image):\n",
        "        \"\"\"\n",
        "        Apply a random filter (blur, sharpen, or noise) to the image.\n",
        "\n",
        "        Arguments:\n",
        "            image (np.ndarray): Input RGB image as a NumPy array.\n",
        "\n",
        "        Output:\n",
        "            np.ndarray: Image with a randomly applied filter.\n",
        "        \"\"\"\n",
        "        choice = np.random.choice(['blur', 'sharpen', 'noise', 'none'], p=[0.3, 0.3, 0.2, 0.2])\n",
        "\n",
        "        if choice == 'blur':\n",
        "            from PIL import ImageFilter\n",
        "            pil_img = Image.fromarray(image)\n",
        "            pil_img = pil_img.filter(ImageFilter.GaussianBlur(radius=np.random.uniform(0.5, 1.5)))\n",
        "            return np.array(pil_img)\n",
        "        elif choice == 'sharpen':\n",
        "            from PIL import ImageFilter\n",
        "            pil_img = Image.fromarray(image)\n",
        "            pil_img = pil_img.filter(ImageFilter.UnsharpMask(\n",
        "                radius=np.random.uniform(1, 3),\n",
        "                percent=np.random.uniform(100, 300),\n",
        "                threshold=np.random.uniform(1, 5)\n",
        "            ))\n",
        "            return np.array(pil_img)\n",
        "        elif choice == 'noise':\n",
        "            noise = np.random.normal(0, 10, image.shape).astype('uint8')\n",
        "            return np.clip(image + noise, 0, 255)\n",
        "        else:\n",
        "            return image\n",
        "\n",
        "    def preprocess_image(self, img_path):\n",
        "        \"\"\"\n",
        "        Load and preprocess a single image with memory-efficient handling.\n",
        "\n",
        "        Arguments:\n",
        "            img_path (str): Path to the input image.\n",
        "\n",
        "        Output:\n",
        "            np.ndarray or None: Preprocessed image array with shape (1, H, W, 3), \n",
        "            or None if loading fails.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            with Image.open(img_path) as img:\n",
        "                img = img.resize(self.config.IMG_SIZE)\n",
        "\n",
        "                # Convert to RGB if not already\n",
        "                if img.mode != 'RGB':\n",
        "                    img = img.convert('RGB')\n",
        "\n",
        "                img_array = np.array(img)\n",
        "\n",
        "                # Handle grayscale or alpha channel images\n",
        "                if img_array.ndim == 2:\n",
        "                    img_array = np.stack((img_array,) * 3, axis=-1)\n",
        "                elif img_array.shape[2] == 4:\n",
        "                    img_array = img_array[:, :, :3]\n",
        "\n",
        "                return img_array.reshape((1,) + img_array.shape)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading image {img_path}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def generate_augmented_images(self, img_array, count):\n",
        "        \"\"\"\n",
        "        Generate a specified number of augmented versions of the input image.\n",
        "\n",
        "        Arguments:\n",
        "            img_array (np.ndarray): Image array with shape (1, H, W, 3).\n",
        "            count (int): Number of augmented images to generate.\n",
        "\n",
        "        Output:\n",
        "            List[np.ndarray]: List of augmented images.\n",
        "        \"\"\"\n",
        "        augmented_images = []\n",
        "        aug_iter = self.datagen.flow(img_array, batch_size=1)\n",
        "\n",
        "        for _ in range(min(count, 8)):  # Cap for memory efficiency\n",
        "            augmented = next(aug_iter)[0].astype('uint8')\n",
        "\n",
        "            # Optionally apply additional filter\n",
        "            if np.random.rand() > 0.7:\n",
        "                augmented = self.apply_random_filter(augmented)\n",
        "\n",
        "            augmented_images.append(augmented)\n",
        "            if len(augmented_images) >= count:\n",
        "                break\n",
        "\n",
        "        return augmented_images\n",
        "\n",
        "    def save_augmented_images(self, augmented_images, class_dir):\n",
        "        \"\"\"\n",
        "        Save augmented images to a specified class directory.\n",
        "\n",
        "        Arguments:\n",
        "            augmented_images (List[np.ndarray]): List of images to save.\n",
        "            class_dir (str): Directory path to save images into.\n",
        "\n",
        "        Output:\n",
        "            int: Number of images successfully saved.\n",
        "        \"\"\"\n",
        "        saved_count = 0\n",
        "        for i, img in enumerate(augmented_images):\n",
        "            try:\n",
        "                timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S%f\")\n",
        "                aug_path = os.path.join(class_dir, f\"aug_{timestamp}_{i}.jpg\")\n",
        "                Image.fromarray(img).save(aug_path, quality=95, optimize=True)\n",
        "                saved_count += 1\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error saving augmented image: {str(e)}\")\n",
        "\n",
        "        return saved_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ***🧬 DiseaseProcessor – Class Balancing & Memory-Aware Augmentation Controller***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BFuvP7mWKBl"
      },
      "outputs": [],
      "source": [
        "class DiseaseProcessor:\n",
        "    \"\"\"\n",
        "    A class responsible for balancing image datasets across disease classes using data augmentation,\n",
        "    optimized for memory efficiency and logging system resource usage.\n",
        "\n",
        "    Attributes:\n",
        "        config (DiseaseConfig): Configuration object containing paths, thresholds, and class info.\n",
        "        augmentor (AugmentationEngine): Instance for applying augmentation strategies.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: DiseaseConfig):\n",
        "        \"\"\"\n",
        "        Initialize the DiseaseProcessor.\n",
        "\n",
        "        Args:\n",
        "            config (DiseaseConfig): Configuration object with class directories and augmentation settings.\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "        self.augmentor = AugmentationEngine(config)\n",
        "\n",
        "    def balance_classes(self):\n",
        "        \"\"\"\n",
        "        Balance each class by augmenting images to reach the target count.\n",
        "\n",
        "        Returns:\n",
        "            bool: True if all classes are processed successfully.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If no images are found in any of the active class directories.\n",
        "        \"\"\"\n",
        "        logger.info(f\"Current RAM usage: {psutil.virtual_memory().percent}%\")\n",
        "        class_counts = self._count_class_samples()\n",
        "\n",
        "        if not class_counts:\n",
        "            raise ValueError(\"No images found in any class directory\")\n",
        "\n",
        "        target_count = max(max(class_counts.values()), self.config.MIN_SAMPLES_PER_CLASS)\n",
        "\n",
        "        for class_name in tqdm(self.config.ACTIVE_CLASSES, desc=\"Augmenting classes\"):\n",
        "            class_dir = self.config.class_dirs[class_name]\n",
        "            final_count = self._augment_class(class_dir, target_count, class_counts[class_name])\n",
        "            logger.info(f\"Class {class_name}: {final_count} images (RAM: {psutil.virtual_memory().percent}%)\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _count_class_samples(self):\n",
        "        \"\"\"\n",
        "        Count the number of image files in each active class directory.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary mapping class names to their corresponding image counts.\n",
        "        \"\"\"\n",
        "        class_counts = {}\n",
        "        for class_name in self.config.ACTIVE_CLASSES:\n",
        "            class_dir = self.config.class_dirs[class_name]\n",
        "            count = len([f for f in os.listdir(class_dir)\n",
        "                         if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "            class_counts[class_name] = count\n",
        "        return class_counts\n",
        "\n",
        "    def _augment_class(self, class_dir, target_count, current_count):\n",
        "        \"\"\"\n",
        "        Apply augmentation to a class to reach the desired number of samples.\n",
        "\n",
        "        Args:\n",
        "            class_dir (str): Path to the directory of the specific class.\n",
        "            target_count (int): Target number of images desired per class.\n",
        "            current_count (int): Current number of images available in the class.\n",
        "\n",
        "        Returns:\n",
        "            int: Final number of images after augmentation.\n",
        "        \"\"\"\n",
        "        if current_count >= target_count:\n",
        "            return current_count\n",
        "\n",
        "        current_files = [f for f in os.listdir(class_dir)\n",
        "                         if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        needed = target_count - current_count\n",
        "        aug_per_image = max(1, min(8, needed // max(1, current_count)))  # Cap augmentation to reduce memory load\n",
        "        saved_count = 0\n",
        "\n",
        "        for img_file in current_files:\n",
        "            if needed <= 0:\n",
        "                break\n",
        "\n",
        "            img_path = os.path.join(class_dir, img_file)\n",
        "            img_array = self.augmentor.preprocess_image(img_path)\n",
        "            if img_array is None:\n",
        "                continue\n",
        "\n",
        "            # Generate a small batch of augmented images\n",
        "            batch_size = min(aug_per_image, needed, 4)\n",
        "            augmented_images = self.augmentor.generate_augmented_images(img_array, batch_size)\n",
        "\n",
        "            # Save images and update counters\n",
        "            saved = self.augmentor.save_augmented_images(augmented_images, class_dir)\n",
        "            saved_count += saved\n",
        "            needed -= saved\n",
        "\n",
        "            # Log memory usage periodically\n",
        "            if saved_count % 50 == 0:\n",
        "                logger.info(f\"Progress: {saved_count} added, {needed} needed (RAM: {psutil.virtual_memory().percent}%)\")\n",
        "\n",
        "        return current_count + saved_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ***🚀 High Accuracy Model: EfficientNetV2L + Custom Head***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pfrjc-yvWwnk"
      },
      "outputs": [],
      "source": [
        "class HighAccuracyModel:\n",
        "    \"\"\"\n",
        "    Memory-optimized high accuracy model for multi-class image classification.\n",
        "    This model uses EfficientNetV2L as the backbone and supports dataset creation,\n",
        "    training with class weights, and fine-tuning.\n",
        "\n",
        "    Attributes:\n",
        "        config (DiseaseConfig): Configuration parameters.\n",
        "        model (tf.keras.Model): Keras model instance.\n",
        "        history (History): Training history returned by `model.fit()`.\n",
        "        class_names (List[str]): Names of detected classes in the dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: DiseaseConfig):\n",
        "        \"\"\"\n",
        "        Initialize the model class with configuration.\n",
        "\n",
        "        Args:\n",
        "            config (DiseaseConfig): Configuration with parameters for model, dataset, and training.\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "        self.model = None\n",
        "        self.history = None\n",
        "        self.class_names = None \n",
        "\n",
        "    def build(self):\n",
        "        \"\"\"\n",
        "        Build the EfficientNetV2L-based model with a custom head for classification.\n",
        "\n",
        "        Returns:\n",
        "            tf.keras.Model: A compiled Keras model ready for training.\n",
        "        \"\"\"\n",
        "        # Clear previous sessions\n",
        "        if hasattr(self, 'model'):\n",
        "            del self.model\n",
        "            keras.backend.clear_session()\n",
        "            tf.compat.v1.reset_default_graph()\n",
        "\n",
        "        # Load pre-trained EfficientNetV2L base\n",
        "        base_model = applications.EfficientNetV2L(\n",
        "            include_top=False,\n",
        "            weights='imagenet',\n",
        "            input_shape=(*self.config.IMG_SIZE, 3),\n",
        "            pooling='avg'\n",
        "        )\n",
        "        base_model.trainable = False\n",
        "\n",
        "        # Input layer and preprocessing\n",
        "        inputs = keras.Input(shape=(*self.config.IMG_SIZE, 3), dtype=tf.float32)\n",
        "        x = tf.keras.applications.efficientnet_v2.preprocess_input(inputs)\n",
        "        x = base_model(x)\n",
        "\n",
        "        if isinstance(x, tuple):\n",
        "            logger.warning(f\"Base model returned a tuple. Using the first element.\")\n",
        "            x = x[0]\n",
        "\n",
        "        # Global pooling if necessary\n",
        "        if len(x.shape) > 2:  \n",
        "            x = layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "        # Custom dense head\n",
        "        x = layers.Dense(1536, kernel_regularizer=regularizers.l2(self.config.L2_REG))(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Activation('swish')(x)\n",
        "        x = layers.Dropout(self.config.DROPOUT_RATE)(x)\n",
        "\n",
        "        x = layers.Dense(768, kernel_regularizer=regularizers.l2(self.config.L2_REG))(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Activation('swish')(x)\n",
        "        x = layers.Dropout(self.config.DROPOUT_RATE / 2)(x)\n",
        "\n",
        "        x = layers.Dense(384, kernel_regularizer=regularizers.l2(self.config.L2_REG))(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Activation('swish')(x)\n",
        "\n",
        "        outputs = layers.Dense(len(self.config.ACTIVE_CLASSES), activation='softmax', dtype=tf.float32)(x)\n",
        "\n",
        "        self.model = keras.Model(inputs, outputs)\n",
        "\n",
        "        try:\n",
        "            self.model.summary()\n",
        "        except Exception as build_error:\n",
        "            logger.error(f\"Model build failed: {build_error}\")\n",
        "            for i, layer in enumerate(self.model.layers):\n",
        "                try:\n",
        "                    logger.info(f\"Layer {i}: Input={layer.input_shape}, Output={layer.output_shape}\")\n",
        "                except Exception:\n",
        "                    pass\n",
        "            raise build_error\n",
        "\n",
        "        # Compile model\n",
        "        optimizer = optimizers.AdamW(\n",
        "            learning_rate=0.0001,\n",
        "            weight_decay=0.0001,\n",
        "            beta_1=0.9,\n",
        "            beta_2=0.999,\n",
        "            global_clipnorm=1.0\n",
        "        )\n",
        "\n",
        "        self.model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss=keras.losses.CategoricalCrossentropy(label_smoothing=self.config.LABEL_SMOOTHING),\n",
        "            metrics=[\n",
        "                'accuracy',\n",
        "                keras.metrics.Precision(name='precision'),\n",
        "                keras.metrics.Recall(name='recall'),\n",
        "                keras.metrics.AUC(name='auc'),\n",
        "                keras.metrics.TopKCategoricalAccuracy(k=3, name='top3_accuracy'),\n",
        "                keras.metrics.TopKCategoricalAccuracy(k=5, name='top5_accuracy')\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        return self.model\n",
        "\n",
        "    def create_datasets(self):\n",
        "        \"\"\"\n",
        "        Load, prepare, and return training, validation, and test datasets.\n",
        "        Applies memory-optimized loading and data augmentation.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[tf.data.Dataset, tf.data.Dataset, tf.data.Dataset, List[str]]:\n",
        "                train_ds, val_ds, test_ds, class_names.\n",
        "        \"\"\"\n",
        "        options = tf.data.Options()\n",
        "        options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\n",
        "        options.experimental_optimization.parallel_batch = True\n",
        "        options.experimental_optimization.map_parallelization = True\n",
        "        options.experimental_optimization.inject_prefetch = False\n",
        "\n",
        "        logger.info(f\"Creating datasets (RAM: {psutil.virtual_memory().percent}%)\")\n",
        "\n",
        "        train_ds_initial = keras.utils.image_dataset_from_directory(\n",
        "            self.config.DATA_DIR,\n",
        "            validation_split=0.15,\n",
        "            subset='training',\n",
        "            seed=self.config.SEED,\n",
        "            image_size=self.config.IMG_SIZE,\n",
        "            batch_size=self.config.BATCH_SIZE,\n",
        "            label_mode='categorical'\n",
        "        )\n",
        "\n",
        "        self.class_names = train_ds_initial.class_names\n",
        "        train_ds = train_ds_initial.with_options(options)\n",
        "\n",
        "        val_ds = keras.utils.image_dataset_from_directory(\n",
        "            self.config.DATA_DIR,\n",
        "            validation_split=0.15,\n",
        "            subset='validation',\n",
        "            seed=self.config.SEED,\n",
        "            image_size=self.config.IMG_SIZE,\n",
        "            batch_size=self.config.BATCH_SIZE,\n",
        "            label_mode='categorical'\n",
        "        ).with_options(options)\n",
        "\n",
        "        test_ds = keras.utils.image_dataset_from_directory(\n",
        "            self.config.DATA_DIR,\n",
        "            validation_split=0.1,\n",
        "            subset='validation',\n",
        "            seed=self.config.SEED + 1,\n",
        "            image_size=self.config.IMG_SIZE,\n",
        "            batch_size=self.config.BATCH_SIZE,\n",
        "            label_mode='categorical',\n",
        "            shuffle=False\n",
        "        ).with_options(options)\n",
        "\n",
        "        # Data augmentation\n",
        "        augmentation_layers = keras.Sequential([\n",
        "            layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "            layers.RandomRotation(0.1),\n",
        "            layers.RandomZoom(0.1),\n",
        "            layers.RandomContrast(0.1),\n",
        "            layers.RandomTranslation(0.05, 0.05),\n",
        "            layers.RandomBrightness(0.1),\n",
        "        ])\n",
        "\n",
        "        train_ds = train_ds.map(\n",
        "            lambda x, y: (augmentation_layers(x, training=True), y),\n",
        "            num_parallel_calls=tf.data.AUTOTUNE\n",
        "        )\n",
        "\n",
        "        train_ds = train_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "        val_ds = val_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "        test_ds = test_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "        return train_ds, val_ds, test_ds, self.class_names\n",
        "\n",
        "    def get_callbacks(self):\n",
        "        \"\"\"\n",
        "        Setup callbacks for training with backup, early stopping, checkpointing, etc.\n",
        "\n",
        "        Returns:\n",
        "            List[tf.keras.callbacks.Callback]: List of callbacks.\n",
        "        \"\"\"\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "        log_dir = os.path.join(self.config.LOG_DIR, timestamp)\n",
        "        os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "        return [\n",
        "            callbacks.EarlyStopping(\n",
        "                monitor='val_auc',\n",
        "                patience=15,\n",
        "                mode='max',\n",
        "                restore_best_weights=True,\n",
        "                verbose=1\n",
        "            ),\n",
        "            callbacks.ModelCheckpoint(\n",
        "                os.path.join(self.config.MODEL_DIR, f'best_model_{timestamp}.keras'),\n",
        "                monitor='val_auc',\n",
        "                save_best_only=True,\n",
        "                mode='max',\n",
        "                save_weights_only=False\n",
        "            ),\n",
        "            callbacks.ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.5,\n",
        "                patience=5,\n",
        "                min_lr=1e-6,\n",
        "                verbose=1\n",
        "            ),\n",
        "            callbacks.TerminateOnNaN(),\n",
        "            callbacks.BackupAndRestore(os.path.join(log_dir, 'backup')),\n",
        "            callbacks.CSVLogger(os.path.join(log_dir, 'training_log.csv'))\n",
        "        ]\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Execute full training process with two phases:\n",
        "        - Phase 1: Train custom top layers.\n",
        "        - Phase 2: Fine-tune the EfficientNetV2L base model.\n",
        "\n",
        "        Returns:\n",
        "            History: Training history object.\n",
        "        \"\"\"\n",
        "        keras.backend.clear_session()\n",
        "        tf.compat.v1.reset_default_graph()\n",
        "\n",
        "        self.build()\n",
        "        train_ds, val_ds, test_ds, class_names = self.create_datasets()\n",
        "\n",
        "        logger.info(f\"Calculating class weights (RAM: {psutil.virtual_memory().percent}%)\")\n",
        "        class_counts = {}\n",
        "\n",
        "        if not self.class_names:\n",
        "            logger.warning(\"No class names detected.\")\n",
        "            return None\n",
        "\n",
        "        for class_name in self.class_names:\n",
        "            class_dir = os.path.join(self.config.DATA_DIR, class_name)\n",
        "            if os.path.exists(class_dir):\n",
        "                class_counts[class_name] = len([\n",
        "                    f for f in os.listdir(class_dir)\n",
        "                    if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
        "                ])\n",
        "            else:\n",
        "                logger.warning(f\"Missing class dir: {class_dir}\")\n",
        "                class_counts[class_name] = 0\n",
        "\n",
        "        filtered_class_counts = {k: v for k, v in class_counts.items() if v > 0}\n",
        "        if not filtered_class_counts:\n",
        "            raise ValueError(\"No training samples found.\")\n",
        "\n",
        "        total_samples = sum(filtered_class_counts.values())\n",
        "        num_classes = len(filtered_class_counts)\n",
        "\n",
        "        if num_classes == 0:\n",
        "            raise ValueError(\"No active classes found.\")\n",
        "\n",
        "        class_indices = {name: i for i, name in enumerate(self.class_names)}\n",
        "        class_weights = {\n",
        "            class_indices[name]: (total_samples / (num_classes * count)) ** 0.5\n",
        "            for name, count in filtered_class_counts.items()\n",
        "            if name in class_indices\n",
        "        }\n",
        "\n",
        "        final_class_weights = {\n",
        "            i: class_weights.get(i, 0.1)\n",
        "            for i in range(len(self.class_names))\n",
        "        }\n",
        "\n",
        "        logger.info(f\"Starting Phase 1 training...\")\n",
        "        self.history = self.model.fit(\n",
        "            train_ds,\n",
        "            validation_data=val_ds,\n",
        "            epochs=self.config.EPOCHS,\n",
        "            callbacks=self.get_callbacks(),\n",
        "            class_weight=final_class_weights,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # Fine-tuning\n",
        "        logger.info(\"Starting fine-tuning phase...\")\n",
        "        base_model_layer = next(\n",
        "            (layer for layer in self.model.layers if isinstance(layer, keras.Model) and 'efficientnetv2' in layer.name.lower()),\n",
        "            None\n",
        "        )\n",
        "\n",
        "        if base_model_layer is None:\n",
        "            raise RuntimeError(\"EfficientNetV2L base model not found for fine-tuning.\")\n",
        "\n",
        "        base_model_layer.trainable = True\n",
        "        for layer in base_model_layer.layers[:150]:\n",
        "            layer.trainable = False\n",
        "\n",
        "        self.model.compile(\n",
        "            optimizer=optimizers.Adam(learning_rate=self.config.FINE_TUNE_LR),\n",
        "            loss=keras.losses.CategoricalCrossentropy(label_smoothing=self.config.LABEL_SMOOTHING),\n",
        "            metrics=self.model.metrics\n",
        "        )\n",
        "\n",
        "        self.history = self.model.fit(\n",
        "            train_ds,\n",
        "            validation_data=val_ds,\n",
        "            epochs=self.config.FINE_TUNE_EPOCHS,\n",
        "            callbacks=self.get_callbacks(),\n",
        "            class_weight=final_class_weights,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        return self.history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ***🔧 Main Training Pipeline Function***\n",
        "This cell contains the main logic for the training pipeline: preprocessing, balancing classes, and training the model with logging and error handling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3B1zPDmjW6aa"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to execute the end-to-end disease classification pipeline.\n",
        "\n",
        "    This pipeline includes:\n",
        "        - Memory monitoring\n",
        "        - Directory structure verification\n",
        "        - Data preprocessing and class balancing\n",
        "        - Model training and evaluation\n",
        "        - Metric reporting and visualization\n",
        "\n",
        "    Arguments:\n",
        "        None\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "\n",
        "    Raises:\n",
        "        Exception: If any step in the pipeline fails, it will be logged and raised.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Initial memory check and logging system status\n",
        "        logger.info(f\"Starting pipeline (RAM: {psutil.virtual_memory().percent}%)\")\n",
        "\n",
        "        # Load configuration settings for the pipeline (paths, parameters, etc.)\n",
        "        config = DiseaseConfig()\n",
        "        config.verify_directory_structure()  # Ensure required folders and paths exist\n",
        "\n",
        "        # Initialize the data processor with the configuration\n",
        "        processor = DiseaseProcessor(config)\n",
        "        \n",
        "        # Balance dataset classes (e.g., via oversampling or undersampling)\n",
        "        processor.balance_classes()\n",
        "        \n",
        "        # Update the list of active classes after balancing\n",
        "        config.update_active_classes()\n",
        "\n",
        "        # Initialize and train the high-accuracy deep learning model\n",
        "        model = HighAccuracyModel(config)\n",
        "        history = model.train()  # Returns a training history object (Keras History)\n",
        "\n",
        "        # Evaluate the trained model on the test set\n",
        "        metrics, report = model.evaluate()  # metrics: dict, report: classification report\n",
        "\n",
        "        # Print final results\n",
        "        print(\"\\n=== Final Results ===\")\n",
        "        print(f\"Test Accuracy: {metrics['accuracy']:.4f}\")\n",
        "        print(f\"Test AUC: {metrics['auc']:.4f}\")\n",
        "        print(f\"Top-3 Accuracy: {metrics['top3_accuracy']:.4f}\")\n",
        "        print(f\"Top-5 Accuracy: {metrics['top5_accuracy']:.4f}\")\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(pd.DataFrame(report).transpose())\n",
        "\n",
        "        # Plot training history (accuracy, loss, AUC)\n",
        "        plt.figure(figsize=(18, 6))\n",
        "\n",
        "        # Plot training and validation accuracy per epoch\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "        plt.title('Model Accuracy', fontsize=12)\n",
        "        plt.ylabel('Accuracy', fontsize=12)\n",
        "        plt.xlabel('Epoch', fontsize=12)\n",
        "        plt.legend()\n",
        "\n",
        "        # Plot training and validation loss per epoch\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.plot(history.history['loss'], label='Train Loss')\n",
        "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "        plt.title('Model Loss', fontsize=12)\n",
        "        plt.ylabel('Loss', fontsize=12)\n",
        "        plt.xlabel('Epoch', fontsize=12)\n",
        "        plt.legend()\n",
        "\n",
        "        # Plot training and validation AUC per epoch\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.plot(history.history['auc'], label='Train AUC')\n",
        "        plt.plot(history.history['val_auc'], label='Validation AUC')\n",
        "        plt.title('Model AUC', fontsize=12)\n",
        "        plt.ylabel('AUC', fontsize=12)\n",
        "        plt.xlabel('Epoch', fontsize=12)\n",
        "        plt.legend()\n",
        "\n",
        "        # Save the plot to the log directory\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(config.LOG_DIR, 'training_history.png'), dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "        # Log successful completion of the pipeline\n",
        "        logger.info(\"Pipeline completed successfully\")\n",
        "\n",
        "    except Exception as e:\n",
        "        # Log and raise any exceptions that occur during execution\n",
        "        logger.error(f\"Pipeline failed: {str(e)}\")\n",
        "        raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fKUMhXYmXkPd",
        "outputId": "0c96182a-8694-409a-fe33-f486525fa31e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:__main__:Expected directories missing from /content/drive/MyDrive/Graduation Project/disease_data: {'leaf_scorch', 'powdery_mildew', 'northern_leaf_blight', 'esca', 'target_spot', 'yellow_leaf_curl_virus'}. These classes will not be included in training.\n",
            "WARNING:__main__:Expected directories missing from /content/drive/MyDrive/Graduation Project/disease_data: {'leaf_scorch', 'powdery_mildew', 'northern_leaf_blight', 'esca', 'target_spot', 'yellow_leaf_curl_virus'}. These classes will not be included in training.\n",
            "Augmenting classes: 100%|██████████| 12/12 [00:00<00:00, 11066.77it/s]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ efficientnetv2-l (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │   <span style=\"color: #00af00; text-decoration-color: #00af00\">117,746,848</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,967,616</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,144</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,180,416</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,072</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,296</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,620</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m, \u001b[38;5;34m384\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ efficientnetv2-l (\u001b[38;5;33mFunctional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │   \u001b[38;5;34m117,746,848\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1536\u001b[0m)           │     \u001b[38;5;34m1,967,616\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1536\u001b[0m)           │         \u001b[38;5;34m6,144\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation (\u001b[38;5;33mActivation\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1536\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1536\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)            │     \u001b[38;5;34m1,180,416\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)            │         \u001b[38;5;34m3,072\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_1 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m)            │       \u001b[38;5;34m295,296\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m)            │         \u001b[38;5;34m1,536\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_2 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m)             │         \u001b[38;5;34m4,620\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">121,205,548</span> (462.36 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m121,205,548\u001b[0m (462.36 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,453,324</span> (13.17 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,453,324\u001b[0m (13.17 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">117,752,224</span> (449.19 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m117,752,224\u001b[0m (449.19 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 39804 files belonging to 12 classes.\n",
            "Using 33834 files for training.\n",
            "Found 39804 files belonging to 12 classes.\n",
            "Using 5970 files for validation.\n",
            "Found 39804 files belonging to 12 classes.\n",
            "Using 3980 files for validation.\n",
            "Epoch 1/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m506s\u001b[0m 359ms/step - accuracy: 0.5194 - auc: 0.9051 - loss: 1.9090 - precision: 0.7012 - recall: 0.2968 - top3_accuracy: 0.8263 - top5_accuracy: 0.9127 - val_accuracy: 0.7819 - val_auc: 0.9843 - val_loss: 1.3368 - val_precision: 0.8755 - val_recall: 0.6670 - val_top3_accuracy: 0.9755 - val_top5_accuracy: 0.9918 - learning_rate: 1.0000e-04\n",
            "Epoch 2/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 288ms/step - accuracy: 0.7192 - auc: 0.9710 - loss: 1.4781 - precision: 0.8302 - recall: 0.5716 - top3_accuracy: 0.9523 - top5_accuracy: 0.9827 - val_accuracy: 0.8137 - val_auc: 0.9879 - val_loss: 1.2704 - val_precision: 0.8839 - val_recall: 0.7189 - val_top3_accuracy: 0.9811 - val_top5_accuracy: 0.9941 - learning_rate: 1.0000e-04\n",
            "Epoch 3/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 283ms/step - accuracy: 0.7567 - auc: 0.9783 - loss: 1.3919 - precision: 0.8519 - recall: 0.6251 - top3_accuracy: 0.9632 - top5_accuracy: 0.9884 - val_accuracy: 0.8365 - val_auc: 0.9904 - val_loss: 1.2195 - val_precision: 0.8989 - val_recall: 0.7519 - val_top3_accuracy: 0.9843 - val_top5_accuracy: 0.9951 - learning_rate: 1.0000e-04\n",
            "Epoch 4/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 283ms/step - accuracy: 0.7805 - auc: 0.9824 - loss: 1.3341 - precision: 0.8702 - recall: 0.6587 - top3_accuracy: 0.9705 - top5_accuracy: 0.9912 - val_accuracy: 0.8494 - val_auc: 0.9915 - val_loss: 1.1913 - val_precision: 0.9086 - val_recall: 0.7722 - val_top3_accuracy: 0.9836 - val_top5_accuracy: 0.9963 - learning_rate: 1.0000e-04\n",
            "Epoch 5/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 285ms/step - accuracy: 0.7889 - auc: 0.9832 - loss: 1.3106 - precision: 0.8744 - recall: 0.6749 - top3_accuracy: 0.9702 - top5_accuracy: 0.9910 - val_accuracy: 0.8514 - val_auc: 0.9920 - val_loss: 1.1727 - val_precision: 0.9081 - val_recall: 0.7782 - val_top3_accuracy: 0.9868 - val_top5_accuracy: 0.9963 - learning_rate: 1.0000e-04\n",
            "Epoch 6/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 274ms/step - accuracy: 0.8063 - auc: 0.9859 - loss: 1.2717 - precision: 0.8861 - recall: 0.6984 - top3_accuracy: 0.9756 - top5_accuracy: 0.9922 - val_accuracy: 0.8670 - val_auc: 0.9933 - val_loss: 1.1410 - val_precision: 0.9185 - val_recall: 0.8007 - val_top3_accuracy: 0.9879 - val_top5_accuracy: 0.9973 - learning_rate: 1.0000e-04\n",
            "Epoch 7/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 288ms/step - accuracy: 0.8165 - auc: 0.9869 - loss: 1.2460 - precision: 0.8937 - recall: 0.7144 - top3_accuracy: 0.9761 - top5_accuracy: 0.9923 - val_accuracy: 0.8621 - val_auc: 0.9933 - val_loss: 1.1328 - val_precision: 0.9119 - val_recall: 0.7993 - val_top3_accuracy: 0.9883 - val_top5_accuracy: 0.9963 - learning_rate: 1.0000e-04\n",
            "Epoch 8/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 288ms/step - accuracy: 0.8187 - auc: 0.9873 - loss: 1.2292 - precision: 0.8930 - recall: 0.7163 - top3_accuracy: 0.9782 - top5_accuracy: 0.9931 - val_accuracy: 0.8682 - val_auc: 0.9937 - val_loss: 1.1166 - val_precision: 0.9193 - val_recall: 0.8037 - val_top3_accuracy: 0.9881 - val_top5_accuracy: 0.9968 - learning_rate: 1.0000e-04\n",
            "Epoch 9/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m294s\u001b[0m 274ms/step - accuracy: 0.8258 - auc: 0.9883 - loss: 1.2091 - precision: 0.8979 - recall: 0.7289 - top3_accuracy: 0.9786 - top5_accuracy: 0.9937 - val_accuracy: 0.8811 - val_auc: 0.9946 - val_loss: 1.0869 - val_precision: 0.9303 - val_recall: 0.8300 - val_top3_accuracy: 0.9894 - val_top5_accuracy: 0.9970 - learning_rate: 1.0000e-04\n",
            "Epoch 10/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m295s\u001b[0m 277ms/step - accuracy: 0.8310 - auc: 0.9892 - loss: 1.1885 - precision: 0.9018 - recall: 0.7383 - top3_accuracy: 0.9804 - top5_accuracy: 0.9947 - val_accuracy: 0.8792 - val_auc: 0.9944 - val_loss: 1.0855 - val_precision: 0.9228 - val_recall: 0.8265 - val_top3_accuracy: 0.9883 - val_top5_accuracy: 0.9972 - learning_rate: 1.0000e-04\n",
            "Epoch 11/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 284ms/step - accuracy: 0.8373 - auc: 0.9899 - loss: 1.1717 - precision: 0.9052 - recall: 0.7492 - top3_accuracy: 0.9803 - top5_accuracy: 0.9939 - val_accuracy: 0.8843 - val_auc: 0.9947 - val_loss: 1.0754 - val_precision: 0.9231 - val_recall: 0.8362 - val_top3_accuracy: 0.9898 - val_top5_accuracy: 0.9975 - learning_rate: 1.0000e-04\n",
            "Epoch 12/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 282ms/step - accuracy: 0.8457 - auc: 0.9904 - loss: 1.1520 - precision: 0.9098 - recall: 0.7559 - top3_accuracy: 0.9835 - top5_accuracy: 0.9948 - val_accuracy: 0.8884 - val_auc: 0.9948 - val_loss: 1.0589 - val_precision: 0.9292 - val_recall: 0.8335 - val_top3_accuracy: 0.9894 - val_top5_accuracy: 0.9972 - learning_rate: 1.0000e-04\n",
            "Epoch 13/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 284ms/step - accuracy: 0.8448 - auc: 0.9907 - loss: 1.1414 - precision: 0.9122 - recall: 0.7623 - top3_accuracy: 0.9817 - top5_accuracy: 0.9942 - val_accuracy: 0.8881 - val_auc: 0.9950 - val_loss: 1.0510 - val_precision: 0.9287 - val_recall: 0.8357 - val_top3_accuracy: 0.9891 - val_top5_accuracy: 0.9977 - learning_rate: 1.0000e-04\n",
            "Epoch 14/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m298s\u001b[0m 277ms/step - accuracy: 0.8550 - auc: 0.9914 - loss: 1.1223 - precision: 0.9147 - recall: 0.7704 - top3_accuracy: 0.9817 - top5_accuracy: 0.9948 - val_accuracy: 0.8893 - val_auc: 0.9954 - val_loss: 1.0383 - val_precision: 0.9249 - val_recall: 0.8442 - val_top3_accuracy: 0.9918 - val_top5_accuracy: 0.9978 - learning_rate: 1.0000e-04\n",
            "Epoch 15/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m309s\u001b[0m 289ms/step - accuracy: 0.8521 - auc: 0.9913 - loss: 1.1160 - precision: 0.9125 - recall: 0.7721 - top3_accuracy: 0.9850 - top5_accuracy: 0.9955 - val_accuracy: 0.8943 - val_auc: 0.9958 - val_loss: 1.0203 - val_precision: 0.9325 - val_recall: 0.8494 - val_top3_accuracy: 0.9926 - val_top5_accuracy: 0.9980 - learning_rate: 1.0000e-04\n",
            "Epoch 16/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m289s\u001b[0m 271ms/step - accuracy: 0.8575 - auc: 0.9921 - loss: 1.0985 - precision: 0.9163 - recall: 0.7789 - top3_accuracy: 0.9850 - top5_accuracy: 0.9963 - val_accuracy: 0.8915 - val_auc: 0.9956 - val_loss: 1.0209 - val_precision: 0.9283 - val_recall: 0.8503 - val_top3_accuracy: 0.9898 - val_top5_accuracy: 0.9980 - learning_rate: 1.0000e-04\n",
            "Epoch 17/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m295s\u001b[0m 275ms/step - accuracy: 0.8627 - auc: 0.9922 - loss: 1.0875 - precision: 0.9164 - recall: 0.7839 - top3_accuracy: 0.9852 - top5_accuracy: 0.9963 - val_accuracy: 0.8938 - val_auc: 0.9958 - val_loss: 1.0091 - val_precision: 0.9299 - val_recall: 0.8489 - val_top3_accuracy: 0.9911 - val_top5_accuracy: 0.9980 - learning_rate: 1.0000e-04\n",
            "Epoch 18/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 279ms/step - accuracy: 0.8623 - auc: 0.9925 - loss: 1.0783 - precision: 0.9188 - recall: 0.7873 - top3_accuracy: 0.9849 - top5_accuracy: 0.9966 - val_accuracy: 0.8963 - val_auc: 0.9959 - val_loss: 1.0028 - val_precision: 0.9286 - val_recall: 0.8566 - val_top3_accuracy: 0.9908 - val_top5_accuracy: 0.9977 - learning_rate: 1.0000e-04\n",
            "Epoch 19/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m299s\u001b[0m 279ms/step - accuracy: 0.8676 - auc: 0.9924 - loss: 1.0709 - precision: 0.9230 - recall: 0.7926 - top3_accuracy: 0.9840 - top5_accuracy: 0.9968 - val_accuracy: 0.8982 - val_auc: 0.9961 - val_loss: 0.9938 - val_precision: 0.9315 - val_recall: 0.8590 - val_top3_accuracy: 0.9911 - val_top5_accuracy: 0.9982 - learning_rate: 1.0000e-04\n",
            "Epoch 20/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 281ms/step - accuracy: 0.8702 - auc: 0.9928 - loss: 1.0592 - precision: 0.9239 - recall: 0.7959 - top3_accuracy: 0.9851 - top5_accuracy: 0.9962 - val_accuracy: 0.8993 - val_auc: 0.9961 - val_loss: 0.9869 - val_precision: 0.9371 - val_recall: 0.8588 - val_top3_accuracy: 0.9901 - val_top5_accuracy: 0.9985 - learning_rate: 1.0000e-04\n",
            "Epoch 21/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m298s\u001b[0m 279ms/step - accuracy: 0.8710 - auc: 0.9933 - loss: 1.0459 - precision: 0.9236 - recall: 0.8014 - top3_accuracy: 0.9867 - top5_accuracy: 0.9965 - val_accuracy: 0.9015 - val_auc: 0.9962 - val_loss: 0.9792 - val_precision: 0.9344 - val_recall: 0.8593 - val_top3_accuracy: 0.9903 - val_top5_accuracy: 0.9987 - learning_rate: 1.0000e-04\n",
            "Epoch 22/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 285ms/step - accuracy: 0.8747 - auc: 0.9932 - loss: 1.0420 - precision: 0.9234 - recall: 0.8044 - top3_accuracy: 0.9872 - top5_accuracy: 0.9965 - val_accuracy: 0.9062 - val_auc: 0.9964 - val_loss: 0.9660 - val_precision: 0.9380 - val_recall: 0.8690 - val_top3_accuracy: 0.9921 - val_top5_accuracy: 0.9978 - learning_rate: 1.0000e-04\n",
            "Epoch 23/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 277ms/step - accuracy: 0.8694 - auc: 0.9930 - loss: 1.0387 - precision: 0.9231 - recall: 0.7987 - top3_accuracy: 0.9870 - top5_accuracy: 0.9963 - val_accuracy: 0.9047 - val_auc: 0.9964 - val_loss: 0.9647 - val_precision: 0.9365 - val_recall: 0.8620 - val_top3_accuracy: 0.9911 - val_top5_accuracy: 0.9985 - learning_rate: 1.0000e-04\n",
            "Epoch 24/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m309s\u001b[0m 286ms/step - accuracy: 0.8762 - auc: 0.9935 - loss: 1.0250 - precision: 0.9271 - recall: 0.8048 - top3_accuracy: 0.9884 - top5_accuracy: 0.9966 - val_accuracy: 0.9054 - val_auc: 0.9965 - val_loss: 0.9586 - val_precision: 0.9385 - val_recall: 0.8683 - val_top3_accuracy: 0.9915 - val_top5_accuracy: 0.9977 - learning_rate: 1.0000e-04\n",
            "Epoch 25/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m305s\u001b[0m 286ms/step - accuracy: 0.8777 - auc: 0.9935 - loss: 1.0204 - precision: 0.9253 - recall: 0.8114 - top3_accuracy: 0.9875 - top5_accuracy: 0.9972 - val_accuracy: 0.9059 - val_auc: 0.9967 - val_loss: 0.9491 - val_precision: 0.9385 - val_recall: 0.8714 - val_top3_accuracy: 0.9923 - val_top5_accuracy: 0.9980 - learning_rate: 1.0000e-04\n",
            "Epoch 26/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 284ms/step - accuracy: 0.8792 - auc: 0.9939 - loss: 1.0106 - precision: 0.9293 - recall: 0.8126 - top3_accuracy: 0.9883 - top5_accuracy: 0.9974 - val_accuracy: 0.9107 - val_auc: 0.9969 - val_loss: 0.9390 - val_precision: 0.9391 - val_recall: 0.8752 - val_top3_accuracy: 0.9925 - val_top5_accuracy: 0.9980 - learning_rate: 1.0000e-04\n",
            "Epoch 27/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m294s\u001b[0m 275ms/step - accuracy: 0.8818 - auc: 0.9940 - loss: 1.0047 - precision: 0.9279 - recall: 0.8150 - top3_accuracy: 0.9896 - top5_accuracy: 0.9971 - val_accuracy: 0.9042 - val_auc: 0.9963 - val_loss: 0.9509 - val_precision: 0.9364 - val_recall: 0.8735 - val_top3_accuracy: 0.9911 - val_top5_accuracy: 0.9982 - learning_rate: 1.0000e-04\n",
            "Epoch 28/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m292s\u001b[0m 274ms/step - accuracy: 0.8761 - auc: 0.9939 - loss: 1.0034 - precision: 0.9278 - recall: 0.8142 - top3_accuracy: 0.9878 - top5_accuracy: 0.9968 - val_accuracy: 0.9085 - val_auc: 0.9966 - val_loss: 0.9380 - val_precision: 0.9356 - val_recall: 0.8762 - val_top3_accuracy: 0.9931 - val_top5_accuracy: 0.9980 - learning_rate: 1.0000e-04\n",
            "Epoch 29/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m295s\u001b[0m 277ms/step - accuracy: 0.8824 - auc: 0.9940 - loss: 0.9944 - precision: 0.9309 - recall: 0.8171 - top3_accuracy: 0.9887 - top5_accuracy: 0.9964 - val_accuracy: 0.9050 - val_auc: 0.9965 - val_loss: 0.9354 - val_precision: 0.9319 - val_recall: 0.8784 - val_top3_accuracy: 0.9916 - val_top5_accuracy: 0.9977 - learning_rate: 1.0000e-04\n",
            "Epoch 30/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m301s\u001b[0m 280ms/step - accuracy: 0.8847 - auc: 0.9942 - loss: 0.9878 - precision: 0.9306 - recall: 0.8213 - top3_accuracy: 0.9893 - top5_accuracy: 0.9968 - val_accuracy: 0.9136 - val_auc: 0.9969 - val_loss: 0.9203 - val_precision: 0.9425 - val_recall: 0.8759 - val_top3_accuracy: 0.9925 - val_top5_accuracy: 0.9983 - learning_rate: 1.0000e-04\n",
            "Epoch 31/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 278ms/step - accuracy: 0.8847 - auc: 0.9946 - loss: 0.9783 - precision: 0.9325 - recall: 0.8200 - top3_accuracy: 0.9891 - top5_accuracy: 0.9969 - val_accuracy: 0.9085 - val_auc: 0.9967 - val_loss: 0.9195 - val_precision: 0.9425 - val_recall: 0.8757 - val_top3_accuracy: 0.9930 - val_top5_accuracy: 0.9982 - learning_rate: 1.0000e-04\n",
            "Epoch 32/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m297s\u001b[0m 278ms/step - accuracy: 0.8849 - auc: 0.9946 - loss: 0.9761 - precision: 0.9296 - recall: 0.8232 - top3_accuracy: 0.9899 - top5_accuracy: 0.9970 - val_accuracy: 0.9176 - val_auc: 0.9969 - val_loss: 0.9114 - val_precision: 0.9477 - val_recall: 0.8859 - val_top3_accuracy: 0.9928 - val_top5_accuracy: 0.9982 - learning_rate: 1.0000e-04\n",
            "Epoch 33/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m297s\u001b[0m 277ms/step - accuracy: 0.8860 - auc: 0.9946 - loss: 0.9698 - precision: 0.9346 - recall: 0.8269 - top3_accuracy: 0.9903 - top5_accuracy: 0.9972 - val_accuracy: 0.9117 - val_auc: 0.9966 - val_loss: 0.9156 - val_precision: 0.9402 - val_recall: 0.8794 - val_top3_accuracy: 0.9921 - val_top5_accuracy: 0.9983 - learning_rate: 1.0000e-04\n",
            "Epoch 34/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m303s\u001b[0m 282ms/step - accuracy: 0.8887 - auc: 0.9943 - loss: 0.9673 - precision: 0.9345 - recall: 0.8275 - top3_accuracy: 0.9873 - top5_accuracy: 0.9964 - val_accuracy: 0.9147 - val_auc: 0.9971 - val_loss: 0.9054 - val_precision: 0.9433 - val_recall: 0.8856 - val_top3_accuracy: 0.9925 - val_top5_accuracy: 0.9988 - learning_rate: 1.0000e-04\n",
            "Epoch 35/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m292s\u001b[0m 274ms/step - accuracy: 0.8874 - auc: 0.9945 - loss: 0.9645 - precision: 0.9354 - recall: 0.8288 - top3_accuracy: 0.9896 - top5_accuracy: 0.9973 - val_accuracy: 0.9139 - val_auc: 0.9968 - val_loss: 0.9065 - val_precision: 0.9432 - val_recall: 0.8817 - val_top3_accuracy: 0.9935 - val_top5_accuracy: 0.9982 - learning_rate: 1.0000e-04\n",
            "Epoch 36/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m294s\u001b[0m 274ms/step - accuracy: 0.8870 - auc: 0.9947 - loss: 0.9618 - precision: 0.9313 - recall: 0.8279 - top3_accuracy: 0.9900 - top5_accuracy: 0.9977 - val_accuracy: 0.9164 - val_auc: 0.9971 - val_loss: 0.9007 - val_precision: 0.9432 - val_recall: 0.8819 - val_top3_accuracy: 0.9926 - val_top5_accuracy: 0.9988 - learning_rate: 1.0000e-04\n",
            "Epoch 37/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m291s\u001b[0m 274ms/step - accuracy: 0.8877 - auc: 0.9948 - loss: 0.9575 - precision: 0.9315 - recall: 0.8276 - top3_accuracy: 0.9896 - top5_accuracy: 0.9980 - val_accuracy: 0.9122 - val_auc: 0.9970 - val_loss: 0.9022 - val_precision: 0.9413 - val_recall: 0.8829 - val_top3_accuracy: 0.9915 - val_top5_accuracy: 0.9978 - learning_rate: 1.0000e-04\n",
            "Epoch 38/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m301s\u001b[0m 282ms/step - accuracy: 0.8926 - auc: 0.9949 - loss: 0.9506 - precision: 0.9332 - recall: 0.8341 - top3_accuracy: 0.9912 - top5_accuracy: 0.9978 - val_accuracy: 0.9139 - val_auc: 0.9971 - val_loss: 0.8961 - val_precision: 0.9439 - val_recall: 0.8826 - val_top3_accuracy: 0.9925 - val_top5_accuracy: 0.9983 - learning_rate: 1.0000e-04\n",
            "Epoch 39/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m292s\u001b[0m 272ms/step - accuracy: 0.8906 - auc: 0.9951 - loss: 0.9448 - precision: 0.9337 - recall: 0.8331 - top3_accuracy: 0.9909 - top5_accuracy: 0.9982 - val_accuracy: 0.9129 - val_auc: 0.9969 - val_loss: 0.8926 - val_precision: 0.9429 - val_recall: 0.8826 - val_top3_accuracy: 0.9925 - val_top5_accuracy: 0.9978 - learning_rate: 1.0000e-04\n",
            "Epoch 40/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 275ms/step - accuracy: 0.8953 - auc: 0.9952 - loss: 0.9369 - precision: 0.9368 - recall: 0.8357 - top3_accuracy: 0.9920 - top5_accuracy: 0.9979 - val_accuracy: 0.9186 - val_auc: 0.9971 - val_loss: 0.8880 - val_precision: 0.9453 - val_recall: 0.8854 - val_top3_accuracy: 0.9930 - val_top5_accuracy: 0.9982 - learning_rate: 1.0000e-04\n",
            "Epoch 41/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 273ms/step - accuracy: 0.8908 - auc: 0.9950 - loss: 0.9433 - precision: 0.9348 - recall: 0.8309 - top3_accuracy: 0.9899 - top5_accuracy: 0.9969 - val_accuracy: 0.9174 - val_auc: 0.9969 - val_loss: 0.8877 - val_precision: 0.9440 - val_recall: 0.8839 - val_top3_accuracy: 0.9936 - val_top5_accuracy: 0.9983 - learning_rate: 1.0000e-04\n",
            "Epoch 42/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m289s\u001b[0m 272ms/step - accuracy: 0.8950 - auc: 0.9951 - loss: 0.9363 - precision: 0.9369 - recall: 0.8366 - top3_accuracy: 0.9915 - top5_accuracy: 0.9974 - val_accuracy: 0.9117 - val_auc: 0.9966 - val_loss: 0.8930 - val_precision: 0.9414 - val_recall: 0.8822 - val_top3_accuracy: 0.9925 - val_top5_accuracy: 0.9985 - learning_rate: 1.0000e-04\n",
            "Epoch 43/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 283ms/step - accuracy: 0.8944 - auc: 0.9953 - loss: 0.9321 - precision: 0.9365 - recall: 0.8394 - top3_accuracy: 0.9912 - top5_accuracy: 0.9973 - val_accuracy: 0.9243 - val_auc: 0.9974 - val_loss: 0.8744 - val_precision: 0.9489 - val_recall: 0.8925 - val_top3_accuracy: 0.9920 - val_top5_accuracy: 0.9983 - learning_rate: 1.0000e-04\n",
            "Epoch 44/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m292s\u001b[0m 273ms/step - accuracy: 0.8947 - auc: 0.9953 - loss: 0.9277 - precision: 0.9357 - recall: 0.8405 - top3_accuracy: 0.9909 - top5_accuracy: 0.9978 - val_accuracy: 0.9151 - val_auc: 0.9971 - val_loss: 0.8801 - val_precision: 0.9450 - val_recall: 0.8836 - val_top3_accuracy: 0.9931 - val_top5_accuracy: 0.9987 - learning_rate: 1.0000e-04\n",
            "Epoch 45/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m294s\u001b[0m 275ms/step - accuracy: 0.8980 - auc: 0.9955 - loss: 0.9212 - precision: 0.9398 - recall: 0.8469 - top3_accuracy: 0.9916 - top5_accuracy: 0.9978 - val_accuracy: 0.9183 - val_auc: 0.9972 - val_loss: 0.8724 - val_precision: 0.9443 - val_recall: 0.8883 - val_top3_accuracy: 0.9933 - val_top5_accuracy: 0.9983 - learning_rate: 1.0000e-04\n",
            "Epoch 46/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m292s\u001b[0m 272ms/step - accuracy: 0.8989 - auc: 0.9955 - loss: 0.9210 - precision: 0.9379 - recall: 0.8444 - top3_accuracy: 0.9917 - top5_accuracy: 0.9977 - val_accuracy: 0.9218 - val_auc: 0.9973 - val_loss: 0.8687 - val_precision: 0.9490 - val_recall: 0.8911 - val_top3_accuracy: 0.9933 - val_top5_accuracy: 0.9987 - learning_rate: 1.0000e-04\n",
            "Epoch 47/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m291s\u001b[0m 271ms/step - accuracy: 0.8984 - auc: 0.9955 - loss: 0.9175 - precision: 0.9365 - recall: 0.8442 - top3_accuracy: 0.9914 - top5_accuracy: 0.9976 - val_accuracy: 0.9211 - val_auc: 0.9973 - val_loss: 0.8653 - val_precision: 0.9495 - val_recall: 0.8946 - val_top3_accuracy: 0.9935 - val_top5_accuracy: 0.9982 - learning_rate: 1.0000e-04\n",
            "Epoch 48/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m295s\u001b[0m 275ms/step - accuracy: 0.8977 - auc: 0.9953 - loss: 0.9183 - precision: 0.9382 - recall: 0.8459 - top3_accuracy: 0.9910 - top5_accuracy: 0.9971 - val_accuracy: 0.9162 - val_auc: 0.9971 - val_loss: 0.8712 - val_precision: 0.9421 - val_recall: 0.8881 - val_top3_accuracy: 0.9933 - val_top5_accuracy: 0.9985 - learning_rate: 1.0000e-04\n",
            "Epoch 49/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m295s\u001b[0m 277ms/step - accuracy: 0.9008 - auc: 0.9955 - loss: 0.9101 - precision: 0.9401 - recall: 0.8491 - top3_accuracy: 0.9916 - top5_accuracy: 0.9973 - val_accuracy: 0.9231 - val_auc: 0.9971 - val_loss: 0.8625 - val_precision: 0.9502 - val_recall: 0.8940 - val_top3_accuracy: 0.9928 - val_top5_accuracy: 0.9983 - learning_rate: 1.0000e-04\n",
            "Epoch 50/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 271ms/step - accuracy: 0.9014 - auc: 0.9956 - loss: 0.9074 - precision: 0.9420 - recall: 0.8509 - top3_accuracy: 0.9907 - top5_accuracy: 0.9976 - val_accuracy: 0.9235 - val_auc: 0.9973 - val_loss: 0.8575 - val_precision: 0.9485 - val_recall: 0.8970 - val_top3_accuracy: 0.9936 - val_top5_accuracy: 0.9985 - learning_rate: 1.0000e-04\n",
            "Epoch 51/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 272ms/step - accuracy: 0.9015 - auc: 0.9958 - loss: 0.9046 - precision: 0.9416 - recall: 0.8525 - top3_accuracy: 0.9916 - top5_accuracy: 0.9975 - val_accuracy: 0.9224 - val_auc: 0.9974 - val_loss: 0.8588 - val_precision: 0.9489 - val_recall: 0.8935 - val_top3_accuracy: 0.9936 - val_top5_accuracy: 0.9983 - learning_rate: 1.0000e-04\n",
            "Epoch 52/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 273ms/step - accuracy: 0.8984 - auc: 0.9956 - loss: 0.9088 - precision: 0.9370 - recall: 0.8508 - top3_accuracy: 0.9910 - top5_accuracy: 0.9973 - val_accuracy: 0.9157 - val_auc: 0.9971 - val_loss: 0.8639 - val_precision: 0.9432 - val_recall: 0.8879 - val_top3_accuracy: 0.9925 - val_top5_accuracy: 0.9983 - learning_rate: 1.0000e-04\n",
            "Epoch 53/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m301s\u001b[0m 280ms/step - accuracy: 0.9017 - auc: 0.9956 - loss: 0.9048 - precision: 0.9388 - recall: 0.8487 - top3_accuracy: 0.9917 - top5_accuracy: 0.9979 - val_accuracy: 0.9270 - val_auc: 0.9976 - val_loss: 0.8468 - val_precision: 0.9513 - val_recall: 0.8963 - val_top3_accuracy: 0.9941 - val_top5_accuracy: 0.9987 - learning_rate: 1.0000e-04\n",
            "Epoch 54/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 272ms/step - accuracy: 0.9009 - auc: 0.9957 - loss: 0.9023 - precision: 0.9399 - recall: 0.8520 - top3_accuracy: 0.9916 - top5_accuracy: 0.9977 - val_accuracy: 0.9255 - val_auc: 0.9975 - val_loss: 0.8542 - val_precision: 0.9528 - val_recall: 0.8923 - val_top3_accuracy: 0.9928 - val_top5_accuracy: 0.9985 - learning_rate: 1.0000e-04\n",
            "Epoch 55/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 276ms/step - accuracy: 0.9032 - auc: 0.9959 - loss: 0.8979 - precision: 0.9433 - recall: 0.8554 - top3_accuracy: 0.9917 - top5_accuracy: 0.9976 - val_accuracy: 0.9280 - val_auc: 0.9973 - val_loss: 0.8481 - val_precision: 0.9527 - val_recall: 0.8975 - val_top3_accuracy: 0.9928 - val_top5_accuracy: 0.9983 - learning_rate: 1.0000e-04\n",
            "Epoch 56/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m294s\u001b[0m 275ms/step - accuracy: 0.9104 - auc: 0.9962 - loss: 0.8868 - precision: 0.9442 - recall: 0.8605 - top3_accuracy: 0.9926 - top5_accuracy: 0.9983 - val_accuracy: 0.9219 - val_auc: 0.9971 - val_loss: 0.8556 - val_precision: 0.9463 - val_recall: 0.8946 - val_top3_accuracy: 0.9933 - val_top5_accuracy: 0.9975 - learning_rate: 1.0000e-04\n",
            "Epoch 57/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m295s\u001b[0m 275ms/step - accuracy: 0.9037 - auc: 0.9957 - loss: 0.8957 - precision: 0.9395 - recall: 0.8567 - top3_accuracy: 0.9918 - top5_accuracy: 0.9983 - val_accuracy: 0.9251 - val_auc: 0.9975 - val_loss: 0.8463 - val_precision: 0.9503 - val_recall: 0.8972 - val_top3_accuracy: 0.9920 - val_top5_accuracy: 0.9988 - learning_rate: 1.0000e-04\n",
            "Epoch 58/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 275ms/step - accuracy: 0.9074 - auc: 0.9962 - loss: 0.8895 - precision: 0.9435 - recall: 0.8593 - top3_accuracy: 0.9918 - top5_accuracy: 0.9977 - val_accuracy: 0.9179 - val_auc: 0.9973 - val_loss: 0.8518 - val_precision: 0.9483 - val_recall: 0.8935 - val_top3_accuracy: 0.9940 - val_top5_accuracy: 0.9988 - learning_rate: 1.0000e-04\n",
            "Epoch 59/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m298s\u001b[0m 277ms/step - accuracy: 0.9049 - auc: 0.9959 - loss: 0.8896 - precision: 0.9438 - recall: 0.8579 - top3_accuracy: 0.9921 - top5_accuracy: 0.9978 - val_accuracy: 0.9296 - val_auc: 0.9977 - val_loss: 0.8383 - val_precision: 0.9502 - val_recall: 0.9015 - val_top3_accuracy: 0.9943 - val_top5_accuracy: 0.9983 - learning_rate: 1.0000e-04\n",
            "Epoch 60/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 272ms/step - accuracy: 0.9061 - auc: 0.9961 - loss: 0.8872 - precision: 0.9441 - recall: 0.8578 - top3_accuracy: 0.9923 - top5_accuracy: 0.9978 - val_accuracy: 0.9209 - val_auc: 0.9974 - val_loss: 0.8469 - val_precision: 0.9462 - val_recall: 0.8928 - val_top3_accuracy: 0.9928 - val_top5_accuracy: 0.9982 - learning_rate: 1.0000e-04\n",
            "Epoch 61/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 273ms/step - accuracy: 0.9049 - auc: 0.9960 - loss: 0.8866 - precision: 0.9420 - recall: 0.8593 - top3_accuracy: 0.9911 - top5_accuracy: 0.9980 - val_accuracy: 0.9258 - val_auc: 0.9974 - val_loss: 0.8417 - val_precision: 0.9510 - val_recall: 0.8968 - val_top3_accuracy: 0.9935 - val_top5_accuracy: 0.9985 - learning_rate: 1.0000e-04\n",
            "Epoch 62/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m294s\u001b[0m 276ms/step - accuracy: 0.9103 - auc: 0.9962 - loss: 0.8809 - precision: 0.9440 - recall: 0.8631 - top3_accuracy: 0.9925 - top5_accuracy: 0.9983 - val_accuracy: 0.9246 - val_auc: 0.9974 - val_loss: 0.8434 - val_precision: 0.9499 - val_recall: 0.8958 - val_top3_accuracy: 0.9936 - val_top5_accuracy: 0.9978 - learning_rate: 1.0000e-04\n",
            "Epoch 63/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m291s\u001b[0m 271ms/step - accuracy: 0.9055 - auc: 0.9963 - loss: 0.8823 - precision: 0.9426 - recall: 0.8598 - top3_accuracy: 0.9923 - top5_accuracy: 0.9982 - val_accuracy: 0.9275 - val_auc: 0.9975 - val_loss: 0.8376 - val_precision: 0.9524 - val_recall: 0.8955 - val_top3_accuracy: 0.9940 - val_top5_accuracy: 0.9987 - learning_rate: 1.0000e-04\n",
            "Epoch 64/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m291s\u001b[0m 273ms/step - accuracy: 0.9066 - auc: 0.9964 - loss: 0.8789 - precision: 0.9443 - recall: 0.8624 - top3_accuracy: 0.9906 - top5_accuracy: 0.9988 - val_accuracy: 0.9288 - val_auc: 0.9974 - val_loss: 0.8344 - val_precision: 0.9521 - val_recall: 0.9047 - val_top3_accuracy: 0.9936 - val_top5_accuracy: 0.9983 - learning_rate: 1.0000e-04\n",
            "Epoch 65/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m292s\u001b[0m 272ms/step - accuracy: 0.9039 - auc: 0.9961 - loss: 0.8826 - precision: 0.9427 - recall: 0.8580 - top3_accuracy: 0.9921 - top5_accuracy: 0.9984 - val_accuracy: 0.9223 - val_auc: 0.9972 - val_loss: 0.8447 - val_precision: 0.9490 - val_recall: 0.8970 - val_top3_accuracy: 0.9926 - val_top5_accuracy: 0.9982 - learning_rate: 1.0000e-04\n",
            "Epoch 66/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 275ms/step - accuracy: 0.9090 - auc: 0.9962 - loss: 0.8775 - precision: 0.9438 - recall: 0.8612 - top3_accuracy: 0.9926 - top5_accuracy: 0.9977 - val_accuracy: 0.9243 - val_auc: 0.9976 - val_loss: 0.8364 - val_precision: 0.9501 - val_recall: 0.8987 - val_top3_accuracy: 0.9940 - val_top5_accuracy: 0.9985 - learning_rate: 1.0000e-04\n",
            "Epoch 67/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m295s\u001b[0m 276ms/step - accuracy: 0.9048 - auc: 0.9961 - loss: 0.8796 - precision: 0.9406 - recall: 0.8596 - top3_accuracy: 0.9924 - top5_accuracy: 0.9983 - val_accuracy: 0.9278 - val_auc: 0.9971 - val_loss: 0.8315 - val_precision: 0.9516 - val_recall: 0.9018 - val_top3_accuracy: 0.9941 - val_top5_accuracy: 0.9982 - learning_rate: 1.0000e-04\n",
            "Epoch 68/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m292s\u001b[0m 272ms/step - accuracy: 0.9071 - auc: 0.9962 - loss: 0.8772 - precision: 0.9438 - recall: 0.8622 - top3_accuracy: 0.9924 - top5_accuracy: 0.9979 - val_accuracy: 0.9266 - val_auc: 0.9975 - val_loss: 0.8308 - val_precision: 0.9518 - val_recall: 0.9003 - val_top3_accuracy: 0.9941 - val_top5_accuracy: 0.9987 - learning_rate: 1.0000e-04\n",
            "Epoch 69/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 273ms/step - accuracy: 0.9095 - auc: 0.9964 - loss: 0.8717 - precision: 0.9457 - recall: 0.8663 - top3_accuracy: 0.9923 - top5_accuracy: 0.9981 - val_accuracy: 0.9283 - val_auc: 0.9975 - val_loss: 0.8332 - val_precision: 0.9513 - val_recall: 0.8995 - val_top3_accuracy: 0.9940 - val_top5_accuracy: 0.9983 - learning_rate: 1.0000e-04\n",
            "Epoch 70/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m295s\u001b[0m 275ms/step - accuracy: 0.9112 - auc: 0.9963 - loss: 0.8717 - precision: 0.9458 - recall: 0.8637 - top3_accuracy: 0.9929 - top5_accuracy: 0.9976 - val_accuracy: 0.9199 - val_auc: 0.9971 - val_loss: 0.8446 - val_precision: 0.9460 - val_recall: 0.8945 - val_top3_accuracy: 0.9928 - val_top5_accuracy: 0.9978 - learning_rate: 1.0000e-04\n",
            "Epoch 71/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 275ms/step - accuracy: 0.9099 - auc: 0.9963 - loss: 0.8697 - precision: 0.9435 - recall: 0.8656 - top3_accuracy: 0.9922 - top5_accuracy: 0.9979 - val_accuracy: 0.9206 - val_auc: 0.9970 - val_loss: 0.8411 - val_precision: 0.9471 - val_recall: 0.8963 - val_top3_accuracy: 0.9940 - val_top5_accuracy: 0.9982 - learning_rate: 1.0000e-04\n",
            "Epoch 72/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 276ms/step - accuracy: 0.9068 - auc: 0.9961 - loss: 0.8774 - precision: 0.9443 - recall: 0.8617 - top3_accuracy: 0.9922 - top5_accuracy: 0.9975 - val_accuracy: 0.9281 - val_auc: 0.9975 - val_loss: 0.8263 - val_precision: 0.9513 - val_recall: 0.9005 - val_top3_accuracy: 0.9941 - val_top5_accuracy: 0.9985 - learning_rate: 1.0000e-04\n",
            "Epoch 73/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 273ms/step - accuracy: 0.9093 - auc: 0.9964 - loss: 0.8697 - precision: 0.9452 - recall: 0.8644 - top3_accuracy: 0.9924 - top5_accuracy: 0.9983 - val_accuracy: 0.9256 - val_auc: 0.9975 - val_loss: 0.8343 - val_precision: 0.9496 - val_recall: 0.9017 - val_top3_accuracy: 0.9926 - val_top5_accuracy: 0.9978 - learning_rate: 1.0000e-04\n",
            "Epoch 74/100\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m291s\u001b[0m 273ms/step - accuracy: 0.9123 - auc: 0.9962 - loss: 0.8680 - precision: 0.9447 - recall: 0.8693 - top3_accuracy: 0.9921 - top5_accuracy: 0.9975 - val_accuracy: 0.9296 - val_auc: 0.9974 - val_loss: 0.8250 - val_precision: 0.9551 - val_recall: 0.9042 - val_top3_accuracy: 0.9923 - val_top5_accuracy: 0.9980 - learning_rate: 1.0000e-04\n",
            "Epoch 74: early stopping\n",
            "Restoring model weights from the end of the best epoch: 59.\n",
            "Epoch 101/120\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m954s\u001b[0m 564ms/step - accuracy: 0.7931 - auc: 0.9783 - loss: 1.1432 - precision: 0.8693 - recall: 0.7076 - top3_accuracy: 0.9527 - top5_accuracy: 0.9817 - val_accuracy: 0.9447 - val_auc: 0.9982 - val_loss: 0.7960 - val_precision: 0.9638 - val_recall: 0.9224 - val_top3_accuracy: 0.9963 - val_top5_accuracy: 0.9983 - learning_rate: 1.0000e-05\n",
            "Epoch 102/120\n",
            "\u001b[1m 348/1058\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:59\u001b[0m 337ms/step - accuracy: 0.9332 - auc: 0.9975 - loss: 0.8316 - precision: 0.9570 - recall: 0.9000 - top3_accuracy: 0.9953 - top5_accuracy: 0.9991"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "🌸 ✿ ✿ ✿ ✿ ✿ 🌸🌸 ✿ ✿ ✿ ✿ ✿ 🌸🌸 ✿ ✿ ✿ ✿ ✿ 🌸🌸 ✿ ✿ ✿ ✿ ✿ 🌸🌸 ✿ ✿ ✿ ✿ ✿ 🌸🌸 ✿ ✿ ✿ ✿ ✿ 🌸🌸 ✿ ✿ ✿ ✿ ✿ 🌸🌸 ✿ ✿ ✿ ✿ ✿ 🌸🌸 ✿ ✿ ✿ ✿ ✿ 🌸🌸 ✿ ✿ ✿ ✿ ✿ 🌸🌸 ✿ ✿ ✿ ✿ ✿ 🌸\n",
        "\n",
        "🌿 ───────────────────────────── 🌿🌿 ───────────────────────────── 🌿🌿 ───────────────────────────── 🌿🌿 ───────────────────────────── 🌿\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"text-align: center;\">\n",
        "  <h1><b>🏁 Training Session Completed Successfully</b></h1>\n",
        "</div>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
