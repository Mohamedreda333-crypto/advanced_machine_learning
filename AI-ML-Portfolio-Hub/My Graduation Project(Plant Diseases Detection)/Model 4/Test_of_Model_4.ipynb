{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNrSz7F2YbWJ",
        "outputId": "1bc15512-7056-4069-e949-588767c77207"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lgVWUQBCYeuU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from google.colab import drive\n",
        "from datetime import datetime\n",
        "import matplotlib as mpl\n",
        "import logging\n",
        "from PIL import Image\n",
        "\n",
        "# Configure matplotlib for professional style\n",
        "mpl.rcParams['figure.facecolor'] = 'white'\n",
        "mpl.rcParams['axes.grid'] = False\n",
        "mpl.rcParams['font.size'] = 12\n",
        "mpl.rcParams['axes.titlesize'] = 16\n",
        "mpl.rcParams['axes.titleweight'] = 'bold'\n",
        "mpl.rcParams['axes.labelsize'] = 14\n",
        "mpl.rcParams['xtick.labelsize'] = 10\n",
        "mpl.rcParams['ytick.labelsize'] = 10\n",
        "mpl.rcParams['legend.fontsize'] = 10\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('model_testing.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eX7StRUEYkIs"
      },
      "outputs": [],
      "source": [
        "class ModelTester:\n",
        "    def __init__(self, model_path, data_dir, img_size=(384, 384), batch_size=32):\n",
        "        self.model_path = model_path\n",
        "        self.data_dir = data_dir\n",
        "        self.img_size = img_size\n",
        "        self.batch_size = batch_size\n",
        "        self.model = None\n",
        "        self.class_names = None\n",
        "        self.test_ds = None\n",
        "        self.results_dir = None\n",
        "        self._setup_directories()\n",
        "\n",
        "    def _setup_directories(self):\n",
        "\n",
        "\n",
        "        # Create timestamped results directory\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        self.results_dir = f\"/content/drive/MyDrive/Graduation Project/saved_models/Model_Evaluation_Results_Model4/{timestamp}\"\n",
        "        os.makedirs(self.results_dir, exist_ok=True)\n",
        "        os.makedirs(os.path.join(self.results_dir, \"plots\"), exist_ok=True)\n",
        "        os.makedirs(os.path.join(self.results_dir, \"metrics\"), exist_ok=True)\n",
        "        os.makedirs(os.path.join(self.results_dir, \"misclassified_samples\"), exist_ok=True)\n",
        "\n",
        "        logger.info(f\"Results will be saved to: {self.results_dir}\")\n",
        "\n",
        "    def load_model(self):\n",
        "        \"\"\"Load the trained model from disk\"\"\"\n",
        "        try:\n",
        "            logger.info(f\"Loading model from {self.model_path}\")\n",
        "            self.model = keras.models.load_model(self.model_path)\n",
        "            logger.info(\"Model loaded successfully\")\n",
        "\n",
        "            # Try to plot model architecture\n",
        "            try:\n",
        "                from tensorflow.keras.utils import plot_model\n",
        "                plot_path = os.path.join(self.results_dir, \"plots\", \"model_architecture.png\")\n",
        "                plot_model(\n",
        "                    self.model,\n",
        "                    to_file=plot_path,\n",
        "                    show_shapes=True,\n",
        "                    show_layer_names=True,\n",
        "                    rankdir='TB',\n",
        "                    expand_nested=False, \n",
        "                    dpi=96\n",
        "                )\n",
        "                logger.info(f\"Model architecture plot saved to {plot_path}\")\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Could not generate model architecture plot: {str(e)}\")\n",
        "                # Install Graphviz if not available\n",
        "                logger.info(\"Attempting to install Graphviz...\")\n",
        "                try:\n",
        "                    !apt-get install graphviz\n",
        "                    !pip install pydot\n",
        "                    from tensorflow.keras.utils import plot_model\n",
        "                    plot_model(\n",
        "                        self.model,\n",
        "                        to_file=plot_path,\n",
        "                        show_shapes=True,\n",
        "                        show_layer_names=True,\n",
        "                        rankdir='TB',\n",
        "                        expand_nested=False,\n",
        "                        dpi=96\n",
        "                    )\n",
        "                    logger.info(f\"Model architecture plot saved to {plot_path}\")\n",
        "                except:\n",
        "                    logger.warning(\"Graphviz installation failed. Skipping model plot.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading model: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def create_test_dataset(self):\n",
        "        \"\"\"Create a test dataset from the data directory\"\"\"\n",
        "        try:\n",
        "            logger.info(\"Creating test dataset\")\n",
        "\n",
        "            # Create test dataset with explicit validation split\n",
        "            test_ds = keras.utils.image_dataset_from_directory(\n",
        "                self.data_dir,\n",
        "                validation_split=0.1,\n",
        "                subset='validation',\n",
        "                seed=42,\n",
        "                image_size=self.img_size,\n",
        "                batch_size=self.batch_size,\n",
        "                label_mode='categorical',\n",
        "                shuffle=True  \n",
        "            )\n",
        "\n",
        "            # Get class names from the directory structure\n",
        "            self.class_names = sorted(os.listdir(self.data_dir))\n",
        "            logger.info(f\"Found {len(self.class_names)} classes: {self.class_names}\")\n",
        "\n",
        "            # Ensure we have samples from all classes\n",
        "            class_counts = {class_name: 0 for class_name in self.class_names}\n",
        "            for _, labels in test_ds:\n",
        "                class_indices = tf.argmax(labels, axis=1)\n",
        "                for idx in class_indices.numpy():\n",
        "                    class_counts[self.class_names[idx]] += 1\n",
        "\n",
        "            logger.info(f\"Class distribution in validation set: {class_counts}\")\n",
        "\n",
        "            # Optimize dataset performance\n",
        "            self.test_ds = test_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "            # Plot class distribution\n",
        "            self._plot_class_distribution(test_ds)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error creating test dataset: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _plot_class_distribution(self, dataset):\n",
        "        \"\"\"Plot class distribution in the test set\"\"\"\n",
        "        logger.info(\"Plotting class distribution\")\n",
        "\n",
        "        # Count samples per class\n",
        "        class_counts = {class_name: 0 for class_name in self.class_names}\n",
        "        for _, labels in dataset:\n",
        "            class_indices = tf.argmax(labels, axis=1)\n",
        "            for idx in class_indices.numpy():\n",
        "                class_counts[self.class_names[idx]] += 1\n",
        "\n",
        "        # Create plot with adjusted size for 12 classes\n",
        "        plt.figure(figsize=(16, 8))\n",
        "        bars = plt.bar(class_counts.keys(), class_counts.values(), color='#1f77b4')\n",
        "\n",
        "        # Add value labels\n",
        "        for bar in bars:\n",
        "            height = bar.get_height()\n",
        "            plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                    f'{height}',\n",
        "                    ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "        plt.title('Test Set Class Distribution', pad=20)\n",
        "        plt.xlabel('Class', labelpad=10)\n",
        "        plt.ylabel('Number of Samples', labelpad=10)\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save plot\n",
        "        plot_path = os.path.join(self.results_dir, \"plots\", \"class_distribution.png\")\n",
        "        plt.savefig(plot_path, bbox_inches='tight', dpi=300)\n",
        "        plt.close()\n",
        "        logger.info(f\"Class distribution plot saved to {plot_path}\")\n",
        "\n",
        "    def evaluate_model(self):\n",
        "        \"\"\"Evaluate model performance on test set\"\"\"\n",
        "        if not self.model or not self.test_ds:\n",
        "            raise ValueError(\"Model and test dataset must be loaded first\")\n",
        "\n",
        "        logger.info(\"Evaluating model on test set\")\n",
        "\n",
        "        # Evaluate metrics\n",
        "        results = self.model.evaluate(self.test_ds, verbose=1)\n",
        "\n",
        "        # Create a dictionary of metrics\n",
        "        metrics = dict(zip(self.model.metrics_names, results))\n",
        "\n",
        "        logger.info(\"\\nTest set evaluation:\")\n",
        "        for name, value in metrics.items():\n",
        "            logger.info(f\"{name}: {value:.4f}\")\n",
        "\n",
        "        # Save metrics to file\n",
        "        metrics_path = os.path.join(self.results_dir, \"metrics\", \"test_metrics.txt\")\n",
        "        with open(metrics_path, 'w') as f:\n",
        "            for name, value in metrics.items():\n",
        "                f.write(f\"{name}: {value:.4f}\\n\")\n",
        "\n",
        "        logger.info(f\"Metrics saved to {metrics_path}\")\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def generate_classification_report(self):\n",
        "        \"\"\"Generate a detailed classification report\"\"\"\n",
        "        if not self.model or not self.test_ds:\n",
        "            raise ValueError(\"Model and test dataset must be loaded first\")\n",
        "\n",
        "        logger.info(\"Generating classification report\")\n",
        "\n",
        "        # Get true labels and predictions\n",
        "        y_true = []\n",
        "        y_pred = []\n",
        "        y_prob = []\n",
        "        misclassified_samples = []\n",
        "\n",
        "        # Ensure we evaluate all batches\n",
        "        for images, labels in self.test_ds:\n",
        "            # Get true labels\n",
        "            y_true.extend(np.argmax(labels.numpy(), axis=1))\n",
        "\n",
        "            # Get predictions\n",
        "            preds = self.model.predict(images, verbose=0)\n",
        "            y_pred.extend(np.argmax(preds, axis=1))\n",
        "            y_prob.extend(preds)\n",
        "\n",
        "            # Collect misclassified samples\n",
        "            true_classes = np.argmax(labels.numpy(), axis=1)\n",
        "            pred_classes = np.argmax(preds, axis=1)\n",
        "            for i in range(len(true_classes)):\n",
        "                if true_classes[i] != pred_classes[i]:\n",
        "                    misclassified_samples.append({\n",
        "                        'image': images[i].numpy(),\n",
        "                        'true_class': true_classes[i],\n",
        "                        'pred_class': pred_classes[i],\n",
        "                        'probabilities': preds[i]\n",
        "                    })\n",
        "\n",
        "        target_names = self.class_names\n",
        "        labels = list(range(len(self.class_names)))\n",
        "\n",
        "        # Generate classification report\n",
        "        report = classification_report(\n",
        "            y_true,\n",
        "            y_pred,\n",
        "            labels=labels,\n",
        "            target_names=target_names,\n",
        "            digits=4,\n",
        "            output_dict=False\n",
        "        )\n",
        "\n",
        "        report_dict = classification_report(\n",
        "            y_true,\n",
        "            y_pred,\n",
        "            labels=labels,\n",
        "            target_names=target_names,\n",
        "            digits=4,\n",
        "            output_dict=True\n",
        "        )\n",
        "\n",
        "        logger.info(\"\\nClassification Report:\\n\" + report)\n",
        "\n",
        "        # Save report to file\n",
        "        report_path = os.path.join(self.results_dir, \"metrics\", \"classification_report.txt\")\n",
        "        with open(report_path, 'w') as f:\n",
        "            f.write(report)\n",
        "\n",
        "        # Plot precision-recall metrics\n",
        "        self._plot_precision_recall(report_dict)\n",
        "\n",
        "        # Plot confusion matrix with all classes\n",
        "        self._plot_confusion_matrix(y_true, y_pred, labels=labels, target_names=target_names)\n",
        "\n",
        "        # Save misclassified samples\n",
        "        self._save_misclassified_samples(misclassified_samples)\n",
        "\n",
        "        return report, y_true, y_pred\n",
        "\n",
        "    def _plot_precision_recall(self, report_dict):\n",
        "        \"\"\"Plot precision and recall metrics per class\"\"\"\n",
        "        logger.info(\"Plotting precision and recall metrics\")\n",
        "\n",
        "        # Prepare data - only include actual classes (skip averages)\n",
        "        metrics = []\n",
        "        for class_name in self.class_names:\n",
        "            if class_name in report_dict:  # Skip 'accuracy', 'macro avg', etc.\n",
        "                metrics.append({\n",
        "                    'Class': class_name,\n",
        "                    'Precision': report_dict[class_name]['precision'],\n",
        "                    'Recall': report_dict[class_name]['recall'],\n",
        "                    'F1-Score': report_dict[class_name]['f1-score']\n",
        "                })\n",
        "\n",
        "        df = pd.DataFrame(metrics)\n",
        "        df = df.sort_values('F1-Score', ascending=False)\n",
        "\n",
        "        # Create plot with adjusted size for 12 classes\n",
        "        plt.figure(figsize=(18, 8))\n",
        "\n",
        "        x = np.arange(len(df))\n",
        "        width = 0.25\n",
        "\n",
        "        plt.bar(x - width, df['Precision'], width, label='Precision', color='#2ca02c')\n",
        "        plt.bar(x, df['Recall'], width, label='Recall', color='#1f77b4')\n",
        "        plt.bar(x + width, df['F1-Score'], width, label='F1-Score', color='#ff7f0e')\n",
        "\n",
        "        plt.title('Precision, Recall, and F1-Score by Class', pad=20)\n",
        "        plt.xlabel('Class', labelpad=10)\n",
        "        plt.ylabel('Score', labelpad=10)\n",
        "        plt.xticks(x, df['Class'], rotation=45, ha='right')\n",
        "        plt.ylim(0, 1.1)\n",
        "        plt.legend(loc='upper right', bbox_to_anchor=(1.15, 1))\n",
        "\n",
        "        # Add value labels\n",
        "        for i, (prec, rec, f1) in enumerate(zip(df['Precision'], df['Recall'], df['F1-Score'])):\n",
        "            plt.text(i - width, prec + 0.02, f\"{prec:.2f}\", ha='center', fontsize=8)\n",
        "            plt.text(i, rec + 0.02, f\"{rec:.2f}\", ha='center', fontsize=8)\n",
        "            plt.text(i + width, f1 + 0.02, f\"{f1:.2f}\", ha='center', fontsize=8)\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save plot\n",
        "        plot_path = os.path.join(self.results_dir, \"plots\", \"precision_recall.png\")\n",
        "        plt.savefig(plot_path, bbox_inches='tight', dpi=300)\n",
        "        plt.close()\n",
        "        logger.info(f\"Precision-recall plot saved to {plot_path}\")\n",
        "\n",
        "    def _plot_confusion_matrix(self, y_true, y_pred, normalize=True, cmap='Blues', labels=None, target_names=None):\n",
        "        \"\"\"Plot a professional confusion matrix with all classes\"\"\"\n",
        "        logger.info(\"Generating confusion matrix\")\n",
        "\n",
        "        if labels is None:\n",
        "            labels = list(range(len(self.class_names)))\n",
        "        if target_names is None:\n",
        "            target_names = self.class_names\n",
        "\n",
        "        # Compute confusion matrix using all specified labels\n",
        "        cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "        # Fill in zeros for any classes not present in the validation set\n",
        "        if normalize:\n",
        "            with np.errstate(divide='ignore', invalid='ignore'):\n",
        "                cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "                cm[np.isnan(cm)] = 0  # Set NaNs to 0 for classes with no true samples\n",
        "            fmt = '.2f'\n",
        "            title = 'Normalized Confusion Matrix'\n",
        "        else:\n",
        "            fmt = 'd'\n",
        "            title = 'Confusion Matrix (Counts)'\n",
        "\n",
        "        # Create figure with adjusted size\n",
        "        fig_size = (12 + len(target_names) * 0.5, 10 + len(target_names) * 0.5)\n",
        "        fig, ax = plt.subplots(figsize=fig_size)\n",
        "\n",
        "        # Create heatmap\n",
        "        sns.heatmap(cm, annot=True, fmt=fmt, cmap=cmap, ax=ax,\n",
        "                    cbar_kws={'label': 'Normalized Count' if normalize else 'Count'},\n",
        "                    xticklabels=target_names,\n",
        "                    yticklabels=target_names)\n",
        "\n",
        "        # Add title and labels\n",
        "        ax.set_title(title, fontsize=16, pad=20)\n",
        "        ax.set_xlabel('Predicted Label', fontsize=14, labelpad=10)\n",
        "        ax.set_ylabel('True Label', fontsize=14, labelpad=10)\n",
        "\n",
        "        # Rotate tick labels\n",
        "        plt.setp(ax.get_xticklabels(), rotation=45, ha='right', fontsize=10)\n",
        "        plt.setp(ax.get_yticklabels(), rotation=0, fontsize=10)\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save figure\n",
        "        plot_path = os.path.join(self.results_dir, \"plots\", \"confusion_matrix.png\")\n",
        "        plt.savefig(plot_path, bbox_inches='tight', dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "        logger.info(f\"Confusion matrix saved to {plot_path}\")\n",
        "\n",
        "    def _save_misclassified_samples(self, misclassified_samples, num_samples=24):\n",
        "        \"\"\"Save misclassified samples with their probabilities\"\"\"\n",
        "        if not misclassified_samples:\n",
        "            logger.info(\"No misclassified samples found\")\n",
        "            return\n",
        "\n",
        "        logger.info(f\"Saving {min(num_samples, len(misclassified_samples))} misclassified samples\")\n",
        "\n",
        "        # Select random samples if we have too many\n",
        "        if len(misclassified_samples) > num_samples:\n",
        "            misclassified_samples = np.random.choice(misclassified_samples, size=num_samples, replace=False)\n",
        "\n",
        "        # Create figure\n",
        "        plt.figure(figsize=(20, 20))\n",
        "        n_cols = 4\n",
        "        n_rows = int(np.ceil(len(misclassified_samples) / n_cols))\n",
        "\n",
        "        for i, sample in enumerate(misclassified_samples):\n",
        "            plt.subplot(n_rows, n_cols, i+1)\n",
        "\n",
        "            # Display image\n",
        "            img = (sample['image'] - sample['image'].min()) / (sample['image'].max() - sample['image'].min())\n",
        "            plt.imshow(img)\n",
        "\n",
        "            # Get top 3 predicted classes\n",
        "            top3_idx = np.argsort(sample['probabilities'])[-3:][::-1]\n",
        "            top3_classes = [self.class_names[idx] for idx in top3_idx]\n",
        "            top3_probs = [sample['probabilities'][idx] for idx in top3_idx]\n",
        "\n",
        "            # Create prediction info text\n",
        "            pred_text = \"\\n\".join([f\"{cls}: {prob:.2f}\" for cls, prob in zip(top3_classes, top3_probs)])\n",
        "\n",
        "            # Set title with color coding\n",
        "            title = f\"True: {self.class_names[sample['true_class']]}\\nPred: {self.class_names[sample['pred_class']]}\\n\\n{pred_text}\"\n",
        "            color = 'green' if sample['true_class'] == sample['pred_class'] else 'red'\n",
        "            plt.title(title, color=color, fontsize=8, pad=4)\n",
        "\n",
        "            plt.axis('off')\n",
        "\n",
        "        plt.suptitle('Misclassified Samples with Prediction Probabilities (Top 3)', fontsize=16, y=1.02)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save figure\n",
        "        plot_path = os.path.join(self.results_dir, \"plots\", \"misclassified_samples.png\")\n",
        "        plt.savefig(plot_path, bbox_inches='tight', dpi=150)\n",
        "        plt.close()\n",
        "\n",
        "        # Save individual misclassified samples\n",
        "        for i, sample in enumerate(misclassified_samples[:24]):  \n",
        "            img = (sample['image'] - sample['image'].min()) / (sample['image'].max() - sample['image'].min())\n",
        "            img_path = os.path.join(self.results_dir, \"misclassified_samples\", f\"sample_{i}.png\")\n",
        "            plt.imsave(img_path, img)\n",
        "\n",
        "        logger.info(f\"Misclassified samples saved to {plot_path}\")\n",
        "\n",
        "    def run_full_evaluation(self):\n",
        "        \"\"\"Run the complete evaluation pipeline\"\"\"\n",
        "        try:\n",
        "            # Load model and data\n",
        "            self.load_model()\n",
        "            self.create_test_dataset()\n",
        "\n",
        "            # Evaluate metrics\n",
        "            metrics = self.evaluate_model()\n",
        "\n",
        "            # Generate reports and plots\n",
        "            report, y_true, y_pred = self.generate_classification_report()\n",
        "\n",
        "            # Save results to file\n",
        "            results_path = os.path.join(self.results_dir, \"evaluation_results.txt\")\n",
        "            with open(results_path, \"w\") as f:\n",
        "                f.write(\"Model Evaluation Results\\n\")\n",
        "                f.write(\"=\"*50 + \"\\n\\n\")\n",
        "                f.write(f\"Evaluation Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
        "                f.write(f\"Number of Classes: {len(self.class_names)}\\n\")\n",
        "                f.write(f\"Classes: {', '.join(self.class_names)}\\n\\n\")\n",
        "                f.write(\"Metrics:\\n\")\n",
        "                for name, value in metrics.items():\n",
        "                    f.write(f\"{name}: {value:.4f}\\n\")\n",
        "                f.write(\"\\nClassification Report:\\n\")\n",
        "                f.write(report)\n",
        "\n",
        "            logger.info(f\"Full evaluation completed. Results saved to {self.results_dir}\")\n",
        "\n",
        "            # Create a summary report\n",
        "            self._create_summary_report(metrics, report)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Evaluation failed: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _create_summary_report(self, metrics, report):\n",
        "        \"\"\"Create a visual summary report of the evaluation\"\"\"\n",
        "        logger.info(\"Creating summary report\")\n",
        "\n",
        "        # Create figure\n",
        "        fig = plt.figure(figsize=(18, 14), facecolor='white')\n",
        "\n",
        "        # Add title\n",
        "        fig.suptitle(\"Model Evaluation Summary\", fontsize=18, fontweight='bold', y=0.97)\n",
        "\n",
        "        # Create grid for subplots\n",
        "        gs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.2)\n",
        "\n",
        "        # Plot 1: Main metrics\n",
        "        ax1 = fig.add_subplot(gs[0, 0])\n",
        "        main_metrics = {k: v for k, v in metrics.items() if k in ['loss', 'accuracy', 'precision', 'recall']}\n",
        "        colors = ['#d62728', '#2ca02c', '#9467bd', '#ff7f0e']\n",
        "        ax1.bar(main_metrics.keys(), main_metrics.values(), color=colors)\n",
        "        ax1.set_title('Key Metrics', fontsize=14)\n",
        "        ax1.set_ylim(0, 1.1)\n",
        "        for i, v in enumerate(main_metrics.values()):\n",
        "            ax1.text(i, v + 0.02, f\"{v:.4f}\", ha='center')\n",
        "\n",
        "        # Plot 2: Confusion matrix thumbnail\n",
        "        ax2 = fig.add_subplot(gs[0, 1])\n",
        "        cm_path = os.path.join(self.results_dir, \"plots\", \"confusion_matrix.png\")\n",
        "        if os.path.exists(cm_path):\n",
        "            cm_img = plt.imread(cm_path)\n",
        "            ax2.imshow(cm_img)\n",
        "            ax2.axis('off')\n",
        "            ax2.set_title('Confusion Matrix', fontsize=14)\n",
        "        else:\n",
        "            ax2.text(0.5, 0.5, \"Confusion Matrix\\nNot Available\", ha='center', va='center')\n",
        "            ax2.axis('off')\n",
        "\n",
        "        # Plot 3: Precision-Recall thumbnail\n",
        "        ax3 = fig.add_subplot(gs[1, 0])\n",
        "        pr_path = os.path.join(self.results_dir, \"plots\", \"precision_recall.png\")\n",
        "        if os.path.exists(pr_path):\n",
        "            pr_img = plt.imread(pr_path)\n",
        "            ax3.imshow(pr_img)\n",
        "            ax3.axis('off')\n",
        "            ax3.set_title('Precision-Recall Metrics', fontsize=14)\n",
        "        else:\n",
        "            ax3.text(0.5, 0.5, \"Precision-Recall Plot\\nNot Available\", ha='center', va='center')\n",
        "            ax3.axis('off')\n",
        "\n",
        "        # Plot 4: Class distribution thumbnail\n",
        "        ax4 = fig.add_subplot(gs[1, 1])\n",
        "        cd_path = os.path.join(self.results_dir, \"plots\", \"class_distribution.png\")\n",
        "        if os.path.exists(cd_path):\n",
        "            cd_img = plt.imread(cd_path)\n",
        "            ax4.imshow(cd_img)\n",
        "            ax4.axis('off')\n",
        "            ax4.set_title('Class Distribution', fontsize=14)\n",
        "        else:\n",
        "            ax4.text(0.5, 0.5, \"Class Distribution\\nNot Available\", ha='center', va='center')\n",
        "            ax4.axis('off')\n",
        "\n",
        "        # Add timestamp and class info\n",
        "        fig.text(0.5, 0.02,\n",
        "                f\"Generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} | Classes: {', '.join(self.class_names)}\",\n",
        "                ha='center', fontsize=10)\n",
        "\n",
        "        # Save summary report\n",
        "        summary_path = os.path.join(self.results_dir, \"summary_report.png\")\n",
        "        plt.savefig(summary_path, bbox_inches='tight', dpi=150)\n",
        "        plt.close()\n",
        "\n",
        "        logger.info(f\"Summary report saved to {summary_path}\")\n",
        "\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.suptitle('Misclassified Samples with Prediction Probabilities (Top 3)', fontsize=16, y=1.02)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save figure\n",
        "        plot_path = os.path.join(self.results_dir, \"plots\", \"misclassified_samples.png\")\n",
        "        plt.savefig(plot_path, bbox_inches='tight', dpi=150)\n",
        "        plt.close()\n",
        "\n",
        "        # Save individual misclassified samples\n",
        "        for i, sample in enumerate(misclassified_samples[:24]):  # Limit to 24 to avoid too many files\n",
        "            img = (sample['image'] - sample['image'].min()) / (sample['image'].max() - sample['image'].min())\n",
        "            img_path = os.path.join(self.results_dir, \"misclassified_samples\", f\"sample_{i}.png\")\n",
        "            plt.imsave(img_path, img)\n",
        "\n",
        "        logger.info(f\"Misclassified samples saved to {plot_path}\")\n",
        "\n",
        "    def run_full_evaluation(self):\n",
        "        \"\"\"Run the complete evaluation pipeline\"\"\"\n",
        "        try:\n",
        "            # Load model and data\n",
        "            self.load_model()\n",
        "            self.create_test_dataset()\n",
        "\n",
        "            # Evaluate metrics\n",
        "            metrics = self.evaluate_model()\n",
        "\n",
        "            # Generate reports and plots\n",
        "            report, y_true, y_pred = self.generate_classification_report()\n",
        "\n",
        "            # Save results to file\n",
        "            results_path = os.path.join(self.results_dir, \"evaluation_results.txt\")\n",
        "            with open(results_path, \"w\") as f:\n",
        "                f.write(\"Model Evaluation Results\\n\")\n",
        "                f.write(\"=\"*50 + \"\\n\\n\")\n",
        "                f.write(f\"Evaluation Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
        "                f.write(f\"Number of Classes: {len(self.class_names)}\\n\")\n",
        "                f.write(f\"Classes: {', '.join(self.class_names)}\\n\\n\")\n",
        "                f.write(\"Metrics:\\n\")\n",
        "                for name, value in metrics.items():\n",
        "                    f.write(f\"{name}: {value:.4f}\\n\")\n",
        "                f.write(\"\\nClassification Report:\\n\")\n",
        "                f.write(report)\n",
        "\n",
        "            logger.info(f\"Full evaluation completed. Results saved to {self.results_dir}\")\n",
        "\n",
        "            # Create a summary report\n",
        "            self._create_summary_report(metrics, report)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Evaluation failed: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _create_summary_report(self, metrics, report):\n",
        "        \"\"\"Create a visual summary report of the evaluation\"\"\"\n",
        "        logger.info(\"Creating summary report\")\n",
        "\n",
        "        # Create figure\n",
        "        fig = plt.figure(figsize=(18, 14), facecolor='white')\n",
        "\n",
        "        # Add title\n",
        "        fig.suptitle(\"Model Evaluation Summary\", fontsize=18, fontweight='bold', y=0.97)\n",
        "\n",
        "        # Create grid for subplots\n",
        "        gs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.2)\n",
        "\n",
        "        # Plot 1: Main metrics\n",
        "        ax1 = fig.add_subplot(gs[0, 0])\n",
        "        main_metrics = {k: v for k, v in metrics.items() if k in ['loss', 'accuracy', 'precision', 'recall']}\n",
        "        colors = ['#d62728', '#2ca02c', '#9467bd', '#ff7f0e']\n",
        "        ax1.bar(main_metrics.keys(), main_metrics.values(), color=colors)\n",
        "        ax1.set_title('Key Metrics', fontsize=14)\n",
        "        ax1.set_ylim(0, 1.1)\n",
        "        for i, v in enumerate(main_metrics.values()):\n",
        "            ax1.text(i, v + 0.02, f\"{v:.4f}\", ha='center')\n",
        "\n",
        "        # Plot 2: Confusion matrix thumbnail\n",
        "        ax2 = fig.add_subplot(gs[0, 1])\n",
        "        cm_path = os.path.join(self.results_dir, \"plots\", \"confusion_matrix.png\")\n",
        "        if os.path.exists(cm_path):\n",
        "            cm_img = plt.imread(cm_path)\n",
        "            ax2.imshow(cm_img)\n",
        "            ax2.axis('off')\n",
        "            ax2.set_title('Confusion Matrix', fontsize=14)\n",
        "        else:\n",
        "            ax2.text(0.5, 0.5, \"Confusion Matrix\\nNot Available\", ha='center', va='center')\n",
        "            ax2.axis('off')\n",
        "\n",
        "        # Plot 3: Precision-Recall thumbnail\n",
        "        ax3 = fig.add_subplot(gs[1, 0])\n",
        "        pr_path = os.path.join(self.results_dir, \"plots\", \"precision_recall.png\")\n",
        "        if os.path.exists(pr_path):\n",
        "            pr_img = plt.imread(pr_path)\n",
        "            ax3.imshow(pr_img)\n",
        "            ax3.axis('off')\n",
        "            ax3.set_title('Precision-Recall Metrics', fontsize=14)\n",
        "        else:\n",
        "            ax3.text(0.5, 0.5, \"Precision-Recall Plot\\nNot Available\", ha='center', va='center')\n",
        "            ax3.axis('off')\n",
        "\n",
        "        # Plot 4: Class distribution thumbnail\n",
        "        ax4 = fig.add_subplot(gs[1, 1])\n",
        "        cd_path = os.path.join(self.results_dir, \"plots\", \"class_distribution.png\")\n",
        "        if os.path.exists(cd_path):\n",
        "            cd_img = plt.imread(cd_path)\n",
        "            ax4.imshow(cd_img)\n",
        "            ax4.axis('off')\n",
        "            ax4.set_title('Class Distribution', fontsize=14)\n",
        "        else:\n",
        "            ax4.text(0.5, 0.5, \"Class Distribution\\nNot Available\", ha='center', va='center')\n",
        "            ax4.axis('off')\n",
        "\n",
        "        # Add timestamp and class info\n",
        "        fig.text(0.5, 0.02,\n",
        "                f\"Generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} | Classes: {', '.join(self.class_names)}\",\n",
        "                ha='center', fontsize=10)\n",
        "\n",
        "        # Save summary report\n",
        "        summary_path = os.path.join(self.results_dir, \"summary_report.png\")\n",
        "        plt.savefig(summary_path, bbox_inches='tight', dpi=150)\n",
        "        plt.close()\n",
        "\n",
        "        logger.info(f\"Summary report saved to {summary_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOkAcI2UbBig",
        "outputId": "261f8136-5010-461e-9a85-ea44f982d69e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 39804 files belonging to 12 classes.\n",
            "Using 3980 files for validation.\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 409ms/step - accuracy: 0.9824 - auc: 0.9996 - loss: 0.6965 - precision: 0.9895 - recall: 0.9765 - top3_accuracy: 0.9985 - top5_accuracy: 0.9996\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Update these paths with your actual paths\n",
        "    MODEL_PATH = \"/content/drive/MyDrive/Graduation Project/saved_models/optimized_model_v4/final_model.keras\"\n",
        "    DATA_DIR = \"/content/drive/MyDrive/Graduation Project/disease_data\"\n",
        "\n",
        "    tester = ModelTester(\n",
        "        model_path=MODEL_PATH,\n",
        "        data_dir=DATA_DIR,\n",
        "        img_size=(384, 384),\n",
        "        batch_size=32\n",
        "    )\n",
        "\n",
        "    tester.run_full_evaluation()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
