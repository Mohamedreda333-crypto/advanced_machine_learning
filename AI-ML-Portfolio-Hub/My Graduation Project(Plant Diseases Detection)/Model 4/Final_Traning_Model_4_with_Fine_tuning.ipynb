{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"text-align: center;\">\n",
        "  <h1><b>🌱 Plant Disease Classifier — Model 4 (bacterial_spot',\n",
        "            'powdery_mildew', 'gray_leaf_spot', 'common_rust',.......)🧠</b></h1>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 🖼️ Model 4 Diseases Images\n",
        "\n",
        "<div style=\"display: flex; justify-content: center; align-items: center; gap: 20px;\">\n",
        "\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"https://storage.googleapis.com/kagglesdsdata/datasets/277323/658267/color/Tomato___Bacterial_spot/00639d29-2d1a-4fcf-9bd3-a2b3109c74c4___UF.GRC_BS_Lab%20Leaf%201054.JPG?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20250610%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20250610T041845Z&X-Goog-Expires=345600&X-Goog-SignedHeaders=host&X-Goog-Signature=9d5a274b2d8a1bdb38f4cca593051b599c88d9f9085e84f8211a16aee4e3c6fa5b86a84f8dc747e697e9be6e2bd0a746a53fd7f52b33c67b58b26377d8e1afd0953206fed910dab1949f1850aa8bfd2a0d11b7ac527258e09d0b8f0396d88013864e1d31079b62d908ead25336ae09d16c6dcb9a30113ae5b58dc4926d7057c6ae847d4763e54b73356247fbc2cb88f1bc87b1d5fe53a2efcd8b60c11494df98d43f7a14a52be0795ed059fd001326e3468e94c3643162316a850fa261e8db2bbebb5a10d96a514dab738c3323595013330e625d996fbee5db8f1c901c3af5738d8138a45bb1032ab432ac4a31f2538f90fb855337808679a3e31b3c4194f99b\" width=\"200\"/>\n",
        "    <p>✅ Tomato Bacterial Spot 🍅 </p>\n",
        "  </div>\n",
        "\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"https://storage.googleapis.com/kagglesdsdata/datasets/277323/658267/color/Corn_%28maize%29___Common_rust_/RS_Rust%201565.JPG?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20250610%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20250610T041954Z&X-Goog-Expires=345600&X-Goog-SignedHeaders=host&X-Goog-Signature=8d843cf6db9a32d1ee30b91a4c5d06c3cd07ce3fdbf8d589bcecd0b5b13f6dfe266e61ecc9cf74cc172be9ab5f7fbb2ec89361db36f46305281bbd347df6f299fbbfa1a33c36522430d9f818fe92887eef2abcef64bf3db85cf3c873cdcaa548328ca65f48c51cb25733bd80d70bb6c415af918717b1ce375075f306ef8ebd139ddcec1df31f881de73f9de8daa3a3e654d7b07f3acc28a7bbc004f0416f7734e20b513510350ab19d7c2e48c7c744e3d19ad291bfe2216322939be3711a8ed1e54edfeb3ac3e5a281ae6c0c7afe815535972e88f9664180263292fddedac1cf662a804bbb70944a6d503c194579368b679f894841ce9bc8fd775a5fb7c3621d\n",
        "    \" width=\"200\"/>\n",
        "    <p>✅ Corn Northern Leaf Blight 🌽</p>\n",
        "  </div>\n",
        "\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"https://storage.googleapis.com/kagglesdsdata/datasets/277323/658267/color/Apple___Cedar_apple_rust/112cfc79-00ab-4920-a235-977c134bdb5d___FREC_C.Rust%204365.JPG?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20250610%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20250610T042207Z&X-Goog-Expires=345600&X-Goog-SignedHeaders=host&X-Goog-Signature=771746e3537bdb06267693455e084ba91be86a3618cdfd81740f57d258519afd37d141a55705ebc3a2e2e8775ae7114391695ebacc75f235f0e20a43642e3f933c17106873be5879477fa7af9849daf578a56e849c5f4db3560a0d56142d0100ddc47bf3b073bc2f77cac199b63e67e219e72aa5b2ed2e89d94a4b0c112d6d4f0129a0f2c7971e1539e15a580fc3f0a1938b5158f05e6991f0791a91dc466930211e1cb3d60df002e3d883627326de2d34003a9dd79d151068272a2738b5a0cecbe9865f0ca70a1491df36d501c729c7f1c2bbd4af489c6fd5eaaa25b2dc86954f187ea76a5109857fe471dba680e72fd3b754953e2f2a93b5414cd168a39dd1\" width=\"200\"/>\n",
        "    <p>✅ Apple Cadar Rust 🍎</p>\n",
        "  </div>\n",
        "\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"https://storage.googleapis.com/kagglesdsdata/datasets/277323/658267/color/Grape___Esca_%28Black_Measles%29/011f307f-e06b-4604-9419-d940f7b00290___FAM_B.Msls%201096.JPG?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20250610%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20250610T042122Z&X-Goog-Expires=345600&X-Goog-SignedHeaders=host&X-Goog-Signature=981c40a4a9a8d6824dad6c4d709c1fc1b3e331ed8c4032ddc93a4ce504cc21b9e9c29ef469d541eb7b85a52aaef0bbd654085583deb5d62a1e0a7a8f166c81296663c690f68812bcc38ba39cb591ef66ac7b9cfb50d244d1a78b8296be27dc69826ad0ad1972aae5fbd3f06efb9237a042a2b4a5fcc1067a86a91d9f76f2b0da681edcad462802f1791e345524bb4f0dcea2bbf9e41420d6a9301f2ba0fe58d3bbed1bb56c4a05722da9dbedc408f43d5c05268dbddfd15a1f691893741fca6561a1e5da55111afbcb0e91cb7c0e731fefe38cb81d2d7ecee6ec09c1cac2190a975fb4c7541f2ce0303a1eec4a453cb2a79f08c2207982fdef2d0d24101a1909\" width=\"200\"/>\n",
        "    <p>✅ Grape Esca 🍇 </p>\n",
        "  </div>\n",
        "\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "🌸 ✿ ✿ ✿ ✿ ✿ 🌸🌸 ✿ ✿ ✿ ✿ ✿ 🌸🌸 ✿ ✿ ✿ ✿ ✿ 🌸🌸 ✿ ✿ ✿ ✿ ✿ 🌸🌸 ✿ ✿ ✿ ✿ ✿ 🌸🌸 ✿ ✿ ✿ ✿ ✿ 🌸🌸 ✿ ✿ ✿ ✿ ✿ 🌸🌸 ✿ ✿ ✿ ✿ ✿ 🌸🌸 ✿ ✿ ✿ ✿ ✿ 🌸🌸 ✿ ✿ ✿ ✿ ✿ 🌸🌸 ✿ ✿ ✿ ✿ ✿ 🌸\n",
        "\n",
        "🌿 ───────────────────────────── 🌿🌿 ───────────────────────────── 🌿🌿 ───────────────────────────── 🌿🌿 ───────────────────────────── 🌿\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ***Install Required Package — `opendatasets`***\n",
        "⬇️ Installing the package:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5zGc-5YVkZy",
        "outputId": "4eb82589-1505-4c42-8f70-fc58f54a3a60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from opendatasets) (4.67.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (from opendatasets) (1.7.4.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from opendatasets) (8.2.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2025.4.26)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (5.29.4)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (0.5.1)\n",
            "Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n"
          ]
        }
      ],
      "source": [
        "!pip install opendatasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ***🌱Download PlantDoc Dataset from Kaggle***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzqVFvx0Vm-C",
        "outputId": "6f52efb8-ce32-463d-fd5c-f7bc6350e4f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: medokhamis\n",
            "Your Kaggle Key: ··········\n",
            "Dataset URL: https://www.kaggle.com/datasets/nirmalsankalana/plantdoc-dataset\n",
            "Downloading plantdoc-dataset.zip to ./plantdoc-dataset\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 896M/896M [00:00<00:00, 1.37GB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import opendatasets as od\n",
        "od.download (\"https://www.kaggle.com/datasets/nirmalsankalana/plantdoc-dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ***🌱Download PlantVillage Dataset from Kaggle***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJlpQJL7Vo3s",
        "outputId": "a9871412-125e-4512-8199-8c0ff7de6e2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: medokhamis\n",
            "Your Kaggle Key: ··········\n",
            "Dataset URL: https://www.kaggle.com/datasets/emmarex/plantdisease\n",
            "Downloading plantdisease.zip to ./plantdisease\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 658M/658M [00:00<00:00, 1.36GB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import opendatasets as od\n",
        "od.download (\"https://www.kaggle.com/datasets/emmarex/plantdisease\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ***🚀 Mount Google Drive in Colab***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeYrDUKKVrhB",
        "outputId": "9481f406-6729-452b-8c2f-b3e3177c873d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "🌿 ───────────────────────────── 🌿🌿 ───────────────────────────── 🌿🌿 ───────────────────────────── 🌿🌿 ───────────────────────────── 🌿\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ***📂 Key Project Directory Paths***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2lYd3VLVvFG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, callbacks, optimizers, applications, regularizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.utils import class_weight\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import psutil\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('plant_disease_classification.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Enable memory optimizations\n",
        "policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
        "tf.keras.mixed_precision.set_global_policy(policy)\n",
        "tf.config.optimizer.set_jit(True)\n",
        "\n",
        "# Configure GPU memory growth\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    except RuntimeError as e:\n",
        "        logger.warning(f\"Could not set GPU memory growth: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "🌿 ───────────────────────────── 🌿🌿 ───────────────────────────── 🌿🌿 ───────────────────────────── 🌿🌿 ───────────────────────────── 🌿\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ***🧠 DiseaseConfig Class – High Accuracy + Memory Efficient Setup***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbMLknHWWA-7"
      },
      "outputs": [],
      "source": [
        "class DiseaseConfig:\n",
        "    \"\"\"Optimized configuration for high accuracy model with memory efficiency\"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initializes all configuration parameters and prepares required directories.\n",
        "        \n",
        "        Arguments:\n",
        "            None\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        self.BASE_DIR = \"/content/drive/MyDrive/Graduation Project\"\n",
        "        self.DATA_DIR = os.path.join(self.BASE_DIR, \"disease_data\")\n",
        "\n",
        "        # Ensure these match your actual directory names exactly\n",
        "        self.ALL_POSSIBLE_CLASSES = {\n",
        "            'scab', 'black_rot', 'cedar_apple_rust', 'bacterial_spot',\n",
        "            'powdery_mildew', 'gray_leaf_spot', 'common_rust',\n",
        "            'northern_leaf_blight', 'esca', 'leaf_blight', 'early_blight',\n",
        "            'late_blight', 'leaf_scorch', 'leaf_mold', 'septoria_leaf_spot',\n",
        "            'target_spot', 'mosaic_virus', 'yellow_leaf_curl_virus'\n",
        "        }\n",
        "\n",
        "        # Optimized model parameters\n",
        "        self.IMG_SIZE = (384, 384)  \n",
        "        self.BATCH_SIZE = 32\n",
        "        self.EPOCHS = 100\n",
        "        self.SEED = 42\n",
        "        self.MIN_SAMPLES_PER_CLASS = 1500\n",
        "\n",
        "        # Enhanced Regularization\n",
        "        self.DROPOUT_RATE = 0.5\n",
        "        self.L2_REG = 0.0001\n",
        "        self.LABEL_SMOOTHING = 0.1 \n",
        "        self.STOCHASTIC_DEPTH_RATE = 0.2  \n",
        "\n",
        "        # Augmentation\n",
        "        self.AUGMENTATION_FACTOR = 10\n",
        "\n",
        "        # Paths\n",
        "        self.MODEL_NAME = \"optimized_model_v4\"\n",
        "        self.MODEL_DIR = os.path.join(self.BASE_DIR, f\"saved_models/{self.MODEL_NAME}\")\n",
        "        self.LOG_DIR = os.path.join(self.BASE_DIR, f\"training_logs/{self.MODEL_NAME}\")\n",
        "        self.FINAL_MODEL_PATH = os.path.join(self.MODEL_DIR, \"final_model.keras\")\n",
        "        self.SAVED_MODEL_PATH = os.path.join(self.MODEL_DIR, \"final_saved_model\")\n",
        "\n",
        "        self.ACTIVE_CLASSES = set()\n",
        "        self.class_dirs = {}\n",
        "        self.setup_dirs()\n",
        "        # Call the new verification method during initialization\n",
        "        self.verify_directory_structure()\n",
        "        self.update_active_classes()\n",
        "\n",
        "    def setup_dirs(self):\n",
        "        \"\"\"\n",
        "        Creates required directories for data, model saving, and training logs.\n",
        "\n",
        "        Arguments:\n",
        "            None\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        os.makedirs(self.DATA_DIR, exist_ok=True)\n",
        "        os.makedirs(self.MODEL_DIR, exist_ok=True)\n",
        "        os.makedirs(self.LOG_DIR, exist_ok=True)\n",
        "\n",
        "    def update_active_classes(self):\n",
        "        \"\"\"\n",
        "        Updates the set of active classes by verifying which subdirectories:\n",
        "        - Are in the list of expected classes\n",
        "        - Contain at least one valid image\n",
        "\n",
        "        Arguments:\n",
        "            None\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        if os.path.exists(self.DATA_DIR):\n",
        "            available_classes = set()\n",
        "            for item in os.listdir(self.DATA_DIR):\n",
        "                full_path = os.path.join(self.DATA_DIR, item)\n",
        "                # Check if it's a directory and if it's in ALL_POSSIBLE_CLASSES\n",
        "                if os.path.isdir(full_path) and item in self.ALL_POSSIBLE_CLASSES:\n",
        "                    # Check if the directory contains any image files\n",
        "                    if any(fname.lower().endswith(('.jpg', '.jpeg', '.png')) for fname in os.listdir(full_path)):\n",
        "                        available_classes.add(item)\n",
        "                        self.class_dirs[item] = full_path\n",
        "            # Update ACTIVE_CLASSES to only include valid directories with images that are in ALL_POSSIBLE_CLASSES\n",
        "            self.ACTIVE_CLASSES = available_classes if available_classes else self.ALL_POSSIBLE_CLASSES.copy()\n",
        "\n",
        "\n",
        "    # Add this method to verify directory structure\n",
        "    def verify_directory_structure(self):\n",
        "        \"\"\"Verifies that the data directory exists and contains valid class subdirectories.\"\"\"\n",
        "        \"\"\"\n",
        "        Verifies that the dataset directory exists and is structured correctly.\n",
        "        Logs warnings for:\n",
        "        - Missing class directories\n",
        "        - Unexpected folders not in the class list\n",
        "\n",
        "        Arguments:\n",
        "            None\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "\n",
        "        Raises:\n",
        "            FileNotFoundError: If the data directory is not found\n",
        "            OSError: If contents of the data directory cannot be listed\n",
        "        \"\"\"\n",
        "        if not os.path.exists(self.DATA_DIR):\n",
        "            raise FileNotFoundError(f\"Data directory {self.DATA_DIR} not found. Please ensure your dataset is downloaded and extracted to this location.\")\n",
        "\n",
        "        try:\n",
        "            dir_items = os.listdir(self.DATA_DIR)\n",
        "        except OSError as e:\n",
        "            raise OSError(f\"Error listing contents of data directory {self.DATA_DIR}: {e}\")\n",
        "\n",
        "        # Find all subdirectories in the data directory\n",
        "        valid_classes_in_dir = {d for d in dir_items if os.path.isdir(os.path.join(self.DATA_DIR, d))}\n",
        "\n",
        "        if not valid_classes_in_dir:\n",
        "            logger.warning(f\"No subdirectories found in {self.DATA_DIR}. Please ensure your dataset is organized into subfolders, where each subfolder name is a class name.\")\n",
        "            # Exit if no subdirectories are found\n",
        "            return \n",
        "\n",
        "        # Check for directories that are in the data directory but not in ALL_POSSIBLE_CLASSES\n",
        "        extra_dirs = valid_classes_in_dir - self.ALL_POSSIBLE_CLASSES\n",
        "        if extra_dirs:\n",
        "            logger.warning(f\"Unexpected directories found in {self.DATA_DIR}: {extra_dirs}. These directories will be ignored.\")\n",
        "\n",
        "        # Check for classes defined in ALL_POSSIBLE_CLASSES but not found as directories\n",
        "        missing_dirs = self.ALL_POSSIBLE_CLASSES - valid_classes_in_dir\n",
        "        if missing_dirs:\n",
        "            logger.warning(f\"Expected directories missing from {self.DATA_DIR}: {missing_dirs}. These classes will not be included in training.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "🌿 ───────────────────────────── 🌿🌿 ───────────────────────────── 🌿🌿 ───────────────────────────── 🌿🌿 ───────────────────────────── 🌿\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ***🔁 StochasticDepth Layer – Regularization via Random Skipping***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVxJ1OmScPES"
      },
      "outputs": [],
      "source": [
        "class StochasticDepth(layers.Layer):\n",
        "    \"\"\"\n",
        "    Implements Stochastic Depth as a custom Keras layer for regularization.\n",
        "    \n",
        "    This technique randomly drops entire residual blocks during training \n",
        "    (based on drop_rate), which helps prevent overfitting and improves \n",
        "    model generalization—especially in very deep networks.\n",
        "\n",
        "    Attributes:\n",
        "        drop_rate (float): The probability of dropping a sample (range 0 to 1).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, drop_rate, **kwargs):\n",
        "        \"\"\"\n",
        "        Initializes the StochasticDepth layer.\n",
        "\n",
        "        Args:\n",
        "            drop_rate (float): Probability of dropping the layer's output during training.\n",
        "            **kwargs: Additional keyword arguments passed to the base Layer class.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        super().__init__(**kwargs)\n",
        "        self.drop_rate = drop_rate\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        \"\"\"\n",
        "        Applies stochastic depth to the input during training.\n",
        "\n",
        "        Args:\n",
        "            inputs (Tensor): Input tensor of shape (batch_size, H, W, C).\n",
        "            training (bool, optional): Whether the model is in training mode.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Output tensor with stochastic depth applied if training is True.\n",
        "                    Otherwise, returns the input tensor unchanged.\n",
        "        \"\"\"\n",
        "        if not training:\n",
        "            return inputs\n",
        "\n",
        "        keep_prob = 1 - self.drop_rate\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        random_tensor = keep_prob\n",
        "        random_tensor += tf.random.uniform([batch_size, 1, 1, 1], dtype=inputs.dtype)\n",
        "        binary_tensor = tf.floor(random_tensor)\n",
        "        output = tf.math.divide(inputs, keep_prob) * binary_tensor\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "🌿 ───────────────────────────── 🌿🌿 ───────────────────────────── 🌿🌿 ───────────────────────────── 🌿🌿 ───────────────────────────── 🌿\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ***🧪 AugmentationEngine – Advanced Data Augmentation with Memory Efficiency***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kg3_hhAZfbFV"
      },
      "outputs": [],
      "source": [
        "class AugmentationEngine:\n",
        "    \"\"\"\n",
        "    Enhanced augmentation engine designed for robust data preprocessing \n",
        "    with efficient memory usage and diverse transformations.\n",
        "\n",
        "    Attributes:\n",
        "        config (DiseaseConfig): Configuration object containing global parameters.\n",
        "        datagen (ImageDataGenerator): Keras generator for core augmentation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        \"\"\"\n",
        "        Initialize the AugmentationEngine and configure the augmentation pipeline.\n",
        "\n",
        "        Args:\n",
        "            config (DiseaseConfig): Configuration object containing model and preprocessing settings.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "        self.setup_augmentation_pipeline()\n",
        "\n",
        "    def setup_augmentation_pipeline(self):\n",
        "        \"\"\"\n",
        "        Set up the core Keras image data augmentation pipeline with custom color distortion.\n",
        "\n",
        "        Args:\n",
        "            None\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        self.datagen = ImageDataGenerator(\n",
        "            rotation_range=45,\n",
        "            width_shift_range=0.2,\n",
        "            height_shift_range=0.2,\n",
        "            shear_range=0.2,\n",
        "            zoom_range=[0.7, 1.3],  \n",
        "            horizontal_flip=True,\n",
        "            vertical_flip=True,\n",
        "            brightness_range=[0.5, 1.5],\n",
        "            channel_shift_range=50.0,\n",
        "            fill_mode='reflect',\n",
        "            preprocessing_function=lambda x: self.random_color_distortion(x)\n",
        "        )\n",
        "\n",
        "    def random_color_distortion(self, image, s=1.0):\n",
        "        \"\"\"\n",
        "        Apply mild random distortions to color properties for data diversity.\n",
        "\n",
        "        Args:\n",
        "            image (Tensor): Input image tensor with pixel values (dtype uint8 or float32).\n",
        "            s (float): Strength multiplier for distortion intensity.\n",
        "\n",
        "        Returns:\n",
        "            ndarray: Augmented image with altered color properties (same shape).\n",
        "        \"\"\"\n",
        "        image = tf.image.random_brightness(image, max_delta=0.8 * s)\n",
        "        image = tf.image.random_contrast(image, lower=1 - 0.8 * s, upper=1 + 0.8 * s)\n",
        "        image = tf.image.random_saturation(image, lower=1 - 0.8 * s, upper=1 + 0.8 * s)\n",
        "        image = tf.image.random_hue(image, max_delta=0.2 * s)\n",
        "        image = tf.clip_by_value(image, 0, 255)\n",
        "        return image.numpy() if tf.is_tensor(image) else image\n",
        "\n",
        "    def apply_random_filter(self, image):\n",
        "        \"\"\"\n",
        "        Apply one of several random post-processing filters (blur, sharpen, noise).\n",
        "\n",
        "        Args:\n",
        "            image (ndarray): RGB image array in shape (H, W, 3).\n",
        "\n",
        "        Returns:\n",
        "            ndarray: Image after optional filtering.\n",
        "        \"\"\"\n",
        "        choice = np.random.choice(['blur', 'sharpen', 'noise', 'none'], p=[0.3, 0.3, 0.2, 0.2]) \n",
        "\n",
        "        if choice == 'blur':\n",
        "            from PIL import ImageFilter\n",
        "            pil_img = Image.fromarray(image)\n",
        "            pil_img = pil_img.filter(ImageFilter.GaussianBlur(radius=np.random.uniform(0.5, 1.5)))\n",
        "            return np.array(pil_img)\n",
        "\n",
        "        elif choice == 'sharpen':\n",
        "            from PIL import ImageFilter\n",
        "            pil_img = Image.fromarray(image)\n",
        "            pil_img = pil_img.filter(ImageFilter.UnsharpMask(\n",
        "                radius=np.random.uniform(1, 3),\n",
        "                percent=np.random.uniform(100, 300),\n",
        "                threshold=np.random.uniform(1, 5)\n",
        "            ))\n",
        "            return np.array(pil_img)\n",
        "\n",
        "        elif choice == 'noise':\n",
        "            noise = np.random.normal(0, 10, image.shape).astype('uint8')\n",
        "            return np.clip(image + noise, 0, 255)\n",
        "\n",
        "        else:\n",
        "            return image\n",
        "\n",
        "    def preprocess_image(self, img_path):\n",
        "        \"\"\"\n",
        "        Load and validate an image with resizing and RGB conversion.\n",
        "\n",
        "        Args:\n",
        "            img_path (str): Path to the image file.\n",
        "\n",
        "        Returns:\n",
        "            ndarray or None: Reshaped image array with shape (1, H, W, 3), or None on failure.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            with Image.open(img_path) as img:\n",
        "                img = img.resize(self.config.IMG_SIZE)\n",
        "\n",
        "                if img.mode != 'RGB':\n",
        "                    img = img.convert('RGB')\n",
        "\n",
        "                img_array = np.array(img)\n",
        "\n",
        "                if img_array.ndim == 2:\n",
        "                    img_array = np.stack((img_array,) * 3, axis=-1)\n",
        "                elif img_array.shape[2] == 4:\n",
        "                    img_array = img_array[:, :, :3]\n",
        "\n",
        "                return img_array.reshape((1,) + img_array.shape)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading image {img_path}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def generate_augmented_images(self, img_array, count):\n",
        "        \"\"\"\n",
        "        Generate a set of augmented images from a base image using the configured pipeline.\n",
        "\n",
        "        Args:\n",
        "            img_array (ndarray): Image array of shape (1, H, W, 3) as preprocessed.\n",
        "            count (int): Number of augmented versions to generate.\n",
        "\n",
        "        Returns:\n",
        "            list: List of augmented image arrays (each shape: (H, W, 3)).\n",
        "        \"\"\"\n",
        "        augmented_images = []\n",
        "        aug_iter = self.datagen.flow(img_array, batch_size=1)\n",
        "\n",
        "        for _ in range(min(count, 8)):  # Limit for memory safety\n",
        "            augmented = next(aug_iter)[0].astype('uint8')\n",
        "\n",
        "            if np.random.rand() > 0.7:\n",
        "                augmented = self.apply_random_filter(augmented)\n",
        "\n",
        "            augmented_images.append(augmented)\n",
        "            if len(augmented_images) >= count:\n",
        "                break\n",
        "\n",
        "        return augmented_images\n",
        "\n",
        "    def save_augmented_images(self, augmented_images, class_dir):\n",
        "        \"\"\"\n",
        "        Save augmented images into a specified class directory.\n",
        "\n",
        "        Args:\n",
        "            augmented_images (list): List of numpy image arrays to save.\n",
        "            class_dir (str): Directory path where images will be saved.\n",
        "\n",
        "        Returns:\n",
        "            int: Number of images successfully saved.\n",
        "        \"\"\"\n",
        "        saved_count = 0\n",
        "        for i, img in enumerate(augmented_images):\n",
        "            try:\n",
        "                timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S%f\")\n",
        "                aug_path = os.path.join(class_dir, f\"aug_{timestamp}_{i}.jpg\")\n",
        "                Image.fromarray(img).save(aug_path, quality=95, optimize=True)\n",
        "                saved_count += 1\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error saving augmented image: {str(e)}\")\n",
        "\n",
        "        return saved_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "🌿 ───────────────────────────── 🌿🌿 ───────────────────────────── 🌿🌿 ───────────────────────────── 🌿🌿 ───────────────────────────── 🌿\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ***🧬 DiseaseProcessor – Class Balancing & Memory-Aware Augmentation Controller***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BFuvP7mWKBl"
      },
      "outputs": [],
      "source": [
        "class DiseaseProcessor:\n",
        "    \"\"\"\n",
        "    Class responsible for balancing image dataset classes via data augmentation\n",
        "    using memory-efficient techniques.\n",
        "\n",
        "    Attributes:\n",
        "        config (DiseaseConfig): Configuration object containing settings like image size, class directories, etc.\n",
        "        augmentor (AugmentationEngine): Image augmentation engine used to generate synthetic images.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: DiseaseConfig):\n",
        "        \"\"\"\n",
        "        Initialize the DiseaseProcessor with configuration.\n",
        "\n",
        "        Args:\n",
        "            config (DiseaseConfig): Configuration object for disease processing.\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "        self.augmentor = AugmentationEngine(config)\n",
        "\n",
        "    def balance_classes(self):\n",
        "        \"\"\"\n",
        "        Automatically balances image classes by generating augmented images until \n",
        "        each class reaches the target count.\n",
        "\n",
        "        The function logs memory usage and class balancing progress.\n",
        "\n",
        "        Returns:\n",
        "            bool: True if the process completes successfully.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If no images are found in any class directory.\n",
        "        \"\"\"\n",
        "        logger.info(f\"Current RAM usage: {psutil.virtual_memory().percent}%\")\n",
        "        class_counts = self._count_class_samples()\n",
        "\n",
        "        if not class_counts:\n",
        "            raise ValueError(\"No images found in any class directory\")\n",
        "\n",
        "        # Set the target number of samples for each class\n",
        "        target_count = max(max(class_counts.values()), self.config.MIN_SAMPLES_PER_CLASS)\n",
        "\n",
        "        # Augment each active class\n",
        "        for class_name in tqdm(self.config.ACTIVE_CLASSES, desc=\"Augmenting classes\"):\n",
        "            class_dir = self.config.class_dirs[class_name]\n",
        "            final_count = self._augment_class(class_dir, target_count, class_counts[class_name])\n",
        "            logger.info(f\"Class {class_name}: {final_count} images (RAM: {psutil.virtual_memory().percent}%)\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _count_class_samples(self):\n",
        "        \"\"\"\n",
        "        Counts the number of valid image files in each active class directory.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary where keys are class names and values are counts of images.\n",
        "        \"\"\"\n",
        "        class_counts = {}\n",
        "        for class_name in self.config.ACTIVE_CLASSES:\n",
        "            class_dir = self.config.class_dirs[class_name]\n",
        "            count = len([f for f in os.listdir(class_dir)\n",
        "                         if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "            class_counts[class_name] = count\n",
        "        return class_counts\n",
        "\n",
        "    def _augment_class(self, class_dir, target_count, current_count):\n",
        "        \"\"\"\n",
        "        Performs augmentation for a specific class directory until the class \n",
        "        reaches the desired target count.\n",
        "\n",
        "        Uses small batches and monitors memory to stay efficient.\n",
        "\n",
        "        Args:\n",
        "            class_dir (str): Path to the class directory.\n",
        "            target_count (int): Target number of images for this class.\n",
        "            current_count (int): Current number of images in the class.\n",
        "\n",
        "        Returns:\n",
        "            int: Final number of images in the class after augmentation.\n",
        "        \"\"\"\n",
        "        if current_count >= target_count:\n",
        "            return current_count\n",
        "\n",
        "        # Collect all current image filenames\n",
        "        current_files = [f for f in os.listdir(class_dir)\n",
        "                         if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "        needed = target_count - current_count\n",
        "        aug_per_image = max(1, min(8, needed // max(1, current_count)))  # Augment 1–8 per image\n",
        "        saved_count = 0\n",
        "\n",
        "        for img_file in current_files:\n",
        "            if needed <= 0:\n",
        "                break\n",
        "\n",
        "            img_path = os.path.join(class_dir, img_file)\n",
        "            img_array = self.augmentor.preprocess_image(img_path)\n",
        "            if img_array is None:\n",
        "                continue\n",
        "\n",
        "            # Generate a small batch of augmented images\n",
        "            batch_size = min(aug_per_image, needed, 4)  # Cap to 4 for memory\n",
        "            augmented_images = self.augmentor.generate_augmented_images(img_array, batch_size)\n",
        "\n",
        "            # Save images and update counters\n",
        "            saved = self.augmentor.save_augmented_images(augmented_images, class_dir)\n",
        "            saved_count += saved\n",
        "            needed -= saved\n",
        "\n",
        "            # Log memory usage every 50 saved images\n",
        "            if saved_count % 50 == 0:\n",
        "                logger.info(f\"Progress: {saved_count} added, {needed} needed (RAM: {psutil.virtual_memory().percent}%)\")\n",
        "\n",
        "        return current_count + saved_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "🌿 ───────────────────────────── 🌿🌿 ───────────────────────────── 🌿🌿 ───────────────────────────── 🌿🌿 ───────────────────────────── 🌿\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ***🚀 High Accuracy Model: EfficientNetV2L + Custom Head***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pfrjc-yvWwnk"
      },
      "outputs": [],
      "source": [
        "class HighAccuracyModel:\n",
        "    \"\"\"\n",
        "    Memory-optimized high accuracy model using EfficientNetV2L with custom training logic.\n",
        "\n",
        "    Attributes:\n",
        "        config (DiseaseConfig): Configuration object containing hyperparameters and paths.\n",
        "        model (tf.keras.Model): The compiled Keras model.\n",
        "        history (History): Training history after model.fit().\n",
        "        class_names (List[str]): List of class labels inferred from dataset directories.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: DiseaseConfig):\n",
        "        \"\"\"\n",
        "        Initialize the model with given configuration.\n",
        "\n",
        "        Args:\n",
        "            config (DiseaseConfig): Configuration for model, data, and training.\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "        self.model = None\n",
        "        self.history = None\n",
        "        self.class_names = None\n",
        "\n",
        "\n",
        "    def build(self):\n",
        "        \"\"\"\n",
        "        Build and compile a memory-efficient deep learning model.\n",
        "\n",
        "        Returns:\n",
        "            tf.keras.Model: The compiled EfficientNetV2L model with a custom classification head.\n",
        "        \"\"\"\n",
        "        # Clean up previous model\n",
        "        if hasattr(self, 'model'):\n",
        "            del self.model\n",
        "            keras.backend.clear_session()\n",
        "            tf.compat.v1.reset_default_graph()\n",
        "\n",
        "        # Load EfficientNetV2L as feature extractor\n",
        "        base_model = applications.EfficientNetV2L(\n",
        "            include_top=False,\n",
        "            weights='imagenet',\n",
        "            input_shape=(*self.config.IMG_SIZE, 3),\n",
        "            pooling='avg'\n",
        "        )\n",
        "        base_model.trainable = False\n",
        "\n",
        "        # Define model input\n",
        "        inputs = keras.Input(shape=(*self.config.IMG_SIZE, 3), dtype=tf.float32)\n",
        "        x = tf.keras.applications.efficientnet_v2.preprocess_input(inputs)\n",
        "        x = base_model(x)\n",
        "\n",
        "        # Handle tuple output from base model (some versions return a tuple)\n",
        "        if isinstance(x, tuple):\n",
        "            logger.warning(f\"Base model returned a tuple. Using the first element.\")\n",
        "            x = x[0]\n",
        "\n",
        "        # Flatten if needed\n",
        "        if len(x.shape) > 2:\n",
        "            x = layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "        # Custom classification head\n",
        "        x = layers.Dense(1536, kernel_regularizer=regularizers.l2(self.config.L2_REG))(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Activation('swish')(x)\n",
        "        x = layers.Dropout(self.config.DROPOUT_RATE)(x)\n",
        "\n",
        "        x = layers.Dense(768, kernel_regularizer=regularizers.l2(self.config.L2_REG))(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Activation('swish')(x)\n",
        "        x = layers.Dropout(self.config.DROPOUT_RATE / 2)(x)\n",
        "\n",
        "        x = layers.Dense(384, kernel_regularizer=regularizers.l2(self.config.L2_REG))(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Activation('swish')(x)\n",
        "\n",
        "        outputs = layers.Dense(len(self.config.ACTIVE_CLASSES), activation='softmax', dtype=tf.float32)(x)\n",
        "\n",
        "        self.model = keras.Model(inputs, outputs)\n",
        "\n",
        "        try:\n",
        "            self.model.summary()\n",
        "        except Exception as build_error:\n",
        "            logger.error(f\"Model build failed: {build_error}\")\n",
        "            raise build_error\n",
        "\n",
        "        # Compile model with AdamW optimizer and multiple metrics\n",
        "        optimizer = optimizers.AdamW(\n",
        "            learning_rate=0.0001,\n",
        "            weight_decay=0.0001,\n",
        "            beta_1=0.9,\n",
        "            beta_2=0.999,\n",
        "            global_clipnorm=1.0\n",
        "        )\n",
        "\n",
        "        self.model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss=keras.losses.CategoricalCrossentropy(label_smoothing=self.config.LABEL_SMOOTHING),\n",
        "            metrics=[\n",
        "                'accuracy',\n",
        "                keras.metrics.Precision(name='precision'),\n",
        "                keras.metrics.Recall(name='recall'),\n",
        "                keras.metrics.AUC(name='auc'),\n",
        "                keras.metrics.TopKCategoricalAccuracy(k=3, name='top3_accuracy'),\n",
        "                keras.metrics.TopKCategoricalAccuracy(k=5, name='top5_accuracy')\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        return self.model\n",
        "\n",
        "\n",
        "    def create_datasets(self):\n",
        "        \"\"\"\n",
        "        Create memory-efficient training, validation, and test datasets with augmentation.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[tf.data.Dataset, tf.data.Dataset, tf.data.Dataset, List[str]]:\n",
        "                train_ds, val_ds, test_ds, class_names\n",
        "        \"\"\"\n",
        "        # Set TensorFlow data optimizations\n",
        "        options = tf.data.Options()\n",
        "        options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\n",
        "        options.experimental_optimization.parallel_batch = True\n",
        "        options.experimental_optimization.map_parallelization = True\n",
        "        options.experimental_optimization.inject_prefetch = False\n",
        "\n",
        "        logger.info(f\"Creating datasets (RAM: {psutil.virtual_memory().percent}%)\")\n",
        "\n",
        "        # Load training dataset\n",
        "        train_ds_initial = keras.utils.image_dataset_from_directory(\n",
        "            self.config.DATA_DIR,\n",
        "            validation_split=0.15,\n",
        "            subset='training',\n",
        "            seed=self.config.SEED,\n",
        "            image_size=self.config.IMG_SIZE,\n",
        "            batch_size=self.config.BATCH_SIZE,\n",
        "            label_mode='categorical'\n",
        "        )\n",
        "        self.class_names = train_ds_initial.class_names\n",
        "        train_ds = train_ds_initial.with_options(options)\n",
        "\n",
        "        # Load validation and test datasets\n",
        "        val_ds = keras.utils.image_dataset_from_directory(\n",
        "            self.config.DATA_DIR,\n",
        "            validation_split=0.15,\n",
        "            subset='validation',\n",
        "            seed=self.config.SEED,\n",
        "            image_size=self.config.IMG_SIZE,\n",
        "            batch_size=self.config.BATCH_SIZE,\n",
        "            label_mode='categorical'\n",
        "        ).with_options(options)\n",
        "\n",
        "        test_ds = keras.utils.image_dataset_from_directory(\n",
        "            self.config.DATA_DIR,\n",
        "            validation_split=0.1,\n",
        "            subset='validation',\n",
        "            seed=self.config.SEED + 1,\n",
        "            image_size=self.config.IMG_SIZE,\n",
        "            batch_size=self.config.BATCH_SIZE,\n",
        "            label_mode='categorical',\n",
        "            shuffle=False\n",
        "        ).with_options(options)\n",
        "\n",
        "        # Define augmentation pipeline\n",
        "        augmentation_layers = keras.Sequential([\n",
        "            layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "            layers.RandomRotation(0.1),\n",
        "            layers.RandomZoom(0.1),\n",
        "            layers.RandomContrast(0.1),\n",
        "            layers.RandomTranslation(0.05, 0.05),\n",
        "            layers.RandomBrightness(0.1),\n",
        "        ])\n",
        "\n",
        "        # Apply augmentations to training data only\n",
        "        train_ds = train_ds.map(\n",
        "            lambda x, y: (augmentation_layers(x, training=True), y),\n",
        "            num_parallel_calls=tf.data.AUTOTUNE\n",
        "        )\n",
        "\n",
        "        # Prefetch for performance\n",
        "        train_ds = train_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "        val_ds = val_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "        test_ds = test_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "        return train_ds, val_ds, test_ds, self.class_names\n",
        "\n",
        "\n",
        "    def get_callbacks(self):\n",
        "        \"\"\"\n",
        "        Configure callbacks for training monitoring and fault recovery.\n",
        "\n",
        "        Returns:\n",
        "            List[keras.callbacks.Callback]: List of Keras callbacks.\n",
        "        \"\"\"\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "        log_dir = os.path.join(self.config.LOG_DIR, timestamp)\n",
        "        os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "        return [\n",
        "            callbacks.EarlyStopping(\n",
        "                monitor='val_auc',\n",
        "                patience=15,\n",
        "                mode='max',\n",
        "                restore_best_weights=True,\n",
        "                verbose=1\n",
        "            ),\n",
        "            callbacks.ModelCheckpoint(\n",
        "                filepath=os.path.join(self.config.MODEL_DIR, f'best_model_{timestamp}.keras'),\n",
        "                monitor='val_auc',\n",
        "                save_best_only=True,\n",
        "                mode='max',\n",
        "                save_weights_only=False\n",
        "            ),\n",
        "            callbacks.ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.5,\n",
        "                patience=5,\n",
        "                min_lr=1e-6,\n",
        "                verbose=1\n",
        "            ),\n",
        "            callbacks.TerminateOnNaN(),\n",
        "            callbacks.BackupAndRestore(backup_dir=os.path.join(log_dir, 'backup')),\n",
        "            callbacks.CSVLogger(filename=os.path.join(log_dir, 'training_log.csv'))\n",
        "        ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "🌿 ───────────────────────────── 🌿🌿 ───────────────────────────── 🌿🌿 ───────────────────────────── 🌿🌿 ───────────────────────────── 🌿\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ***🔧 Main Training Pipeline Function***\n",
        "This cell contains the main logic for the training pipeline: preprocessing, balancing classes, and training the model with logging and error handling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3B1zPDmjW6aa"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    \"\"\"\n",
        "    Main entry point for the plant disease detection pipeline.\n",
        "\n",
        "    Arguments:\n",
        "        None\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "\n",
        "    Raises:\n",
        "        Exception: If any step in the pipeline fails, the exception is logged and re-raised.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Log initial system memory usage\n",
        "        logger.info(f\"Starting pipeline (RAM: {psutil.virtual_memory().percent}%)\")\n",
        "\n",
        "        # Initialize configuration settings and verify required directories exist\n",
        "        config = DiseaseConfig()\n",
        "        config.verify_directory_structure()\n",
        "\n",
        "        # Create a processor for handling data preprocessing and augmentation\n",
        "        processor = DiseaseProcessor(config)\n",
        "\n",
        "        # Balance the dataset across all classes to prevent bias during training\n",
        "        processor.balance_classes()\n",
        "\n",
        "        # Update configuration with the currently active classes in the dataset\n",
        "        config.update_active_classes()\n",
        "\n",
        "        # Initialize and train the high-accuracy deep learning model\n",
        "        model = HighAccuracyModel(config)\n",
        "        history = model.train()\n",
        "\n",
        "        # Log success message upon completion\n",
        "        logger.info(\"Pipeline completed successfully\")\n",
        "\n",
        "    except Exception as e:\n",
        "        # Log any exception that occurs during pipeline execution and re-raise it\n",
        "        logger.error(f\"Pipeline failed: {str(e)}\")\n",
        "        raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fKUMhXYmXkPd",
        "outputId": "74516c4f-6722-4ace-8f35-3b3d1efee9fa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:__main__:Expected directories missing from /content/drive/MyDrive/Graduation Project/disease_data: {'powdery_mildew', 'esca', 'northern_leaf_blight', 'yellow_leaf_curl_virus', 'leaf_scorch', 'target_spot'}. These classes will not be included in training.\n",
            "WARNING:__main__:Expected directories missing from /content/drive/MyDrive/Graduation Project/disease_data: {'powdery_mildew', 'esca', 'northern_leaf_blight', 'yellow_leaf_curl_virus', 'leaf_scorch', 'target_spot'}. These classes will not be included in training.\n",
            "Augmenting classes: 100%|██████████| 12/12 [00:00<00:00, 11980.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-l_notop.h5\n",
            "\u001b[1m473176280/473176280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ efficientnetv2-l (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │   <span style=\"color: #00af00; text-decoration-color: #00af00\">117,746,848</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,967,616</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,144</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,180,416</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,072</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,296</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,620</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m, \u001b[38;5;34m384\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ efficientnetv2-l (\u001b[38;5;33mFunctional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │   \u001b[38;5;34m117,746,848\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1536\u001b[0m)           │     \u001b[38;5;34m1,967,616\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1536\u001b[0m)           │         \u001b[38;5;34m6,144\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation (\u001b[38;5;33mActivation\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1536\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1536\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)            │     \u001b[38;5;34m1,180,416\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)            │         \u001b[38;5;34m3,072\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_1 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m)            │       \u001b[38;5;34m295,296\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m)            │         \u001b[38;5;34m1,536\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_2 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m)             │         \u001b[38;5;34m4,620\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">121,205,548</span> (462.36 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m121,205,548\u001b[0m (462.36 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,453,324</span> (13.17 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,453,324\u001b[0m (13.17 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">117,752,224</span> (449.19 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m117,752,224\u001b[0m (449.19 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 39804 files belonging to 12 classes.\n",
            "Using 33834 files for training.\n",
            "Found 39804 files belonging to 12 classes.\n",
            "Using 5970 files for validation.\n",
            "Found 39804 files belonging to 12 classes.\n",
            "Using 3980 files for validation.\n",
            "Epoch 101/120\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1366s\u001b[0m 1s/step - accuracy: 0.9347 - auc: 0.9979 - loss: 0.8240 - precision: 0.9584 - recall: 0.9029 - top3_accuracy: 0.9943 - top5_accuracy: 0.9984 - val_accuracy: 0.9658 - val_auc: 0.9989 - val_loss: 0.7502 - val_precision: 0.9769 - val_recall: 0.9513 - val_top3_accuracy: 0.9965 - val_top5_accuracy: 0.9988 - learning_rate: 1.0000e-05\n",
            "Epoch 102/120\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m393s\u001b[0m 370ms/step - accuracy: 0.9605 - auc: 0.9990 - loss: 0.7670 - precision: 0.9759 - recall: 0.9386 - top3_accuracy: 0.9975 - top5_accuracy: 0.9994 - val_accuracy: 0.9720 - val_auc: 0.9994 - val_loss: 0.7330 - val_precision: 0.9797 - val_recall: 0.9616 - val_top3_accuracy: 0.9973 - val_top5_accuracy: 0.9990 - learning_rate: 1.0000e-05\n",
            "Epoch 103/120\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m426s\u001b[0m 397ms/step - accuracy: 0.9726 - auc: 0.9995 - loss: 0.7386 - precision: 0.9845 - recall: 0.9576 - top3_accuracy: 0.9983 - top5_accuracy: 0.9995 - val_accuracy: 0.9760 - val_auc: 0.9996 - val_loss: 0.7220 - val_precision: 0.9836 - val_recall: 0.9673 - val_top3_accuracy: 0.9977 - val_top5_accuracy: 0.9990 - learning_rate: 1.0000e-05\n",
            "Epoch 104/120\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m379s\u001b[0m 357ms/step - accuracy: 0.9806 - auc: 0.9997 - loss: 0.7201 - precision: 0.9881 - recall: 0.9672 - top3_accuracy: 0.9988 - top5_accuracy: 0.9998 - val_accuracy: 0.9784 - val_auc: 0.9995 - val_loss: 0.7140 - val_precision: 0.9839 - val_recall: 0.9705 - val_top3_accuracy: 0.9972 - val_top5_accuracy: 0.9990 - learning_rate: 1.0000e-05\n",
            "Epoch 105/120\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m421s\u001b[0m 395ms/step - accuracy: 0.9822 - auc: 0.9998 - loss: 0.7099 - precision: 0.9897 - recall: 0.9716 - top3_accuracy: 0.9994 - top5_accuracy: 0.9998 - val_accuracy: 0.9792 - val_auc: 0.9996 - val_loss: 0.7074 - val_precision: 0.9873 - val_recall: 0.9732 - val_top3_accuracy: 0.9970 - val_top5_accuracy: 0.9990 - learning_rate: 1.0000e-05\n",
            "Epoch 106/120\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m423s\u001b[0m 395ms/step - accuracy: 0.9875 - auc: 0.9998 - loss: 0.6986 - precision: 0.9933 - recall: 0.9787 - top3_accuracy: 0.9994 - top5_accuracy: 0.9997 - val_accuracy: 0.9811 - val_auc: 0.9997 - val_loss: 0.7012 - val_precision: 0.9878 - val_recall: 0.9739 - val_top3_accuracy: 0.9973 - val_top5_accuracy: 0.9990 - learning_rate: 1.0000e-05\n",
            "Epoch 107/120\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m420s\u001b[0m 393ms/step - accuracy: 0.9904 - auc: 0.9999 - loss: 0.6892 - precision: 0.9944 - recall: 0.9833 - top3_accuracy: 0.9997 - top5_accuracy: 0.9999 - val_accuracy: 0.9814 - val_auc: 0.9997 - val_loss: 0.6984 - val_precision: 0.9870 - val_recall: 0.9767 - val_top3_accuracy: 0.9977 - val_top5_accuracy: 0.9992 - learning_rate: 1.0000e-05\n",
            "Epoch 108/120\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 356ms/step - accuracy: 0.9924 - auc: 0.9999 - loss: 0.6835 - precision: 0.9956 - recall: 0.9859 - top3_accuracy: 0.9996 - top5_accuracy: 0.9998 - val_accuracy: 0.9846 - val_auc: 0.9996 - val_loss: 0.6907 - val_precision: 0.9890 - val_recall: 0.9786 - val_top3_accuracy: 0.9978 - val_top5_accuracy: 0.9993 - learning_rate: 1.0000e-05\n",
            "Epoch 109/120\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m384s\u001b[0m 358ms/step - accuracy: 0.9930 - auc: 1.0000 - loss: 0.6769 - precision: 0.9963 - recall: 0.9875 - top3_accuracy: 0.9997 - top5_accuracy: 1.0000 - val_accuracy: 0.9841 - val_auc: 0.9996 - val_loss: 0.6879 - val_precision: 0.9878 - val_recall: 0.9787 - val_top3_accuracy: 0.9977 - val_top5_accuracy: 0.9985 - learning_rate: 1.0000e-05\n",
            "Epoch 110/120\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 360ms/step - accuracy: 0.9942 - auc: 1.0000 - loss: 0.6716 - precision: 0.9960 - recall: 0.9899 - top3_accuracy: 0.9999 - top5_accuracy: 1.0000 - val_accuracy: 0.9832 - val_auc: 0.9995 - val_loss: 0.6858 - val_precision: 0.9884 - val_recall: 0.9807 - val_top3_accuracy: 0.9973 - val_top5_accuracy: 0.9988 - learning_rate: 1.0000e-05\n",
            "Epoch 111/120\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m380s\u001b[0m 356ms/step - accuracy: 0.9959 - auc: 1.0000 - loss: 0.6654 - precision: 0.9974 - recall: 0.9917 - top3_accuracy: 0.9999 - top5_accuracy: 1.0000 - val_accuracy: 0.9849 - val_auc: 0.9995 - val_loss: 0.6801 - val_precision: 0.9894 - val_recall: 0.9826 - val_top3_accuracy: 0.9977 - val_top5_accuracy: 0.9988 - learning_rate: 1.0000e-05\n",
            "Epoch 112/120\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m380s\u001b[0m 355ms/step - accuracy: 0.9950 - auc: 1.0000 - loss: 0.6630 - precision: 0.9973 - recall: 0.9911 - top3_accuracy: 0.9999 - top5_accuracy: 1.0000 - val_accuracy: 0.9858 - val_auc: 0.9993 - val_loss: 0.6777 - val_precision: 0.9907 - val_recall: 0.9822 - val_top3_accuracy: 0.9975 - val_top5_accuracy: 0.9985 - learning_rate: 1.0000e-05\n",
            "Epoch 113/120\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 358ms/step - accuracy: 0.9957 - auc: 1.0000 - loss: 0.6586 - precision: 0.9974 - recall: 0.9922 - top3_accuracy: 0.9999 - top5_accuracy: 1.0000 - val_accuracy: 0.9843 - val_auc: 0.9992 - val_loss: 0.6779 - val_precision: 0.9890 - val_recall: 0.9804 - val_top3_accuracy: 0.9970 - val_top5_accuracy: 0.9985 - learning_rate: 1.0000e-05\n",
            "Epoch 114/120\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 356ms/step - accuracy: 0.9959 - auc: 0.9999 - loss: 0.6549 - precision: 0.9976 - recall: 0.9927 - top3_accuracy: 0.9998 - top5_accuracy: 0.9999 - val_accuracy: 0.9843 - val_auc: 0.9995 - val_loss: 0.6744 - val_precision: 0.9883 - val_recall: 0.9804 - val_top3_accuracy: 0.9972 - val_top5_accuracy: 0.9992 - learning_rate: 1.0000e-05\n",
            "Epoch 115/120\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m381s\u001b[0m 356ms/step - accuracy: 0.9979 - auc: 1.0000 - loss: 0.6486 - precision: 0.9985 - recall: 0.9957 - top3_accuracy: 0.9999 - top5_accuracy: 1.0000 - val_accuracy: 0.9844 - val_auc: 0.9991 - val_loss: 0.6711 - val_precision: 0.9895 - val_recall: 0.9812 - val_top3_accuracy: 0.9970 - val_top5_accuracy: 0.9983 - learning_rate: 1.0000e-05\n",
            "Epoch 116/120\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m383s\u001b[0m 357ms/step - accuracy: 0.9975 - auc: 1.0000 - loss: 0.6454 - precision: 0.9984 - recall: 0.9955 - top3_accuracy: 0.9999 - top5_accuracy: 1.0000 - val_accuracy: 0.9843 - val_auc: 0.9992 - val_loss: 0.6686 - val_precision: 0.9887 - val_recall: 0.9821 - val_top3_accuracy: 0.9977 - val_top5_accuracy: 0.9992 - learning_rate: 1.0000e-05\n",
            "Epoch 117/120\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m383s\u001b[0m 360ms/step - accuracy: 0.9979 - auc: 1.0000 - loss: 0.6416 - precision: 0.9988 - recall: 0.9964 - top3_accuracy: 0.9999 - top5_accuracy: 1.0000 - val_accuracy: 0.9854 - val_auc: 0.9992 - val_loss: 0.6652 - val_precision: 0.9900 - val_recall: 0.9821 - val_top3_accuracy: 0.9972 - val_top5_accuracy: 0.9980 - learning_rate: 1.0000e-05\n",
            "Epoch 118/120\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m384s\u001b[0m 358ms/step - accuracy: 0.9975 - auc: 1.0000 - loss: 0.6384 - precision: 0.9986 - recall: 0.9955 - top3_accuracy: 1.0000 - top5_accuracy: 1.0000 - val_accuracy: 0.9853 - val_auc: 0.9989 - val_loss: 0.6646 - val_precision: 0.9890 - val_recall: 0.9814 - val_top3_accuracy: 0.9972 - val_top5_accuracy: 0.9985 - learning_rate: 1.0000e-05\n",
            "Epoch 119/120\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m378s\u001b[0m 355ms/step - accuracy: 0.9979 - auc: 1.0000 - loss: 0.6345 - precision: 0.9988 - recall: 0.9964 - top3_accuracy: 0.9999 - top5_accuracy: 1.0000 - val_accuracy: 0.9856 - val_auc: 0.9991 - val_loss: 0.6581 - val_precision: 0.9902 - val_recall: 0.9824 - val_top3_accuracy: 0.9975 - val_top5_accuracy: 0.9983 - learning_rate: 1.0000e-05\n",
            "Epoch 120/120\n",
            "\u001b[1m1058/1058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m383s\u001b[0m 360ms/step - accuracy: 0.9969 - auc: 1.0000 - loss: 0.6326 - precision: 0.9984 - recall: 0.9954 - top3_accuracy: 0.9999 - top5_accuracy: 1.0000 - val_accuracy: 0.9861 - val_auc: 0.9988 - val_loss: 0.6554 - val_precision: 0.9895 - val_recall: 0.9826 - val_top3_accuracy: 0.9968 - val_top5_accuracy: 0.9977 - learning_rate: 1.0000e-05\n",
            "Restoring model weights from the end of the best epoch: 107.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:__main__:Error saving model: Invalid filepath extension for saving. Please add either a `.keras` extension for the native Keras format (recommended) or a `.h5` extension. Use `model.export(filepath)` if you want to export a SavedModel for use with TFLite/TFServing/etc. Received: filepath=/content/drive/MyDrive/Graduation Project/saved_models/optimized_model_v4/final_saved_model.\n",
            "ERROR:__main__:Pipeline failed: Invalid filepath extension for saving. Please add either a `.keras` extension for the native Keras format (recommended) or a `.h5` extension. Use `model.export(filepath)` if you want to export a SavedModel for use with TFLite/TFServing/etc. Received: filepath=/content/drive/MyDrive/Graduation Project/saved_models/optimized_model_v4/final_saved_model.\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Invalid filepath extension for saving. Please add either a `.keras` extension for the native Keras format (recommended) or a `.h5` extension. Use `model.export(filepath)` if you want to export a SavedModel for use with TFLite/TFServing/etc. Received: filepath=/content/drive/MyDrive/Graduation Project/saved_models/optimized_model_v4/final_saved_model.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-972361fa1b80>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-22c29484bd58>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHighAccuracyModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-38a866ed0b51>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0;31m# Save final model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_final_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-38a866ed0b51>\u001b[0m in \u001b[0;36msave_final_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0;31m# Use the existing self.model object or the reloaded one if reload was done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m                  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVED_MODEL_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m                  \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"SavedModel format saved to {self.config.SAVED_MODEL_PATH}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, zipped, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         )\n\u001b[0;32m--> 114\u001b[0;31m     raise ValueError(\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;34m\"Invalid filepath extension for saving. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;34m\"Please add either a `.keras` extension for the native Keras \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Invalid filepath extension for saving. Please add either a `.keras` extension for the native Keras format (recommended) or a `.h5` extension. Use `model.export(filepath)` if you want to export a SavedModel for use with TFLite/TFServing/etc. Received: filepath=/content/drive/MyDrive/Graduation Project/saved_models/optimized_model_v4/final_saved_model."
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "🌸 ✿ ✿ ✿ ✿ ✿ 🌸🌸 ✿ ✿ ✿ ✿ ✿ 🌸🌸 ✿ ✿ ✿ ✿ ✿ 🌸🌸 ✿ ✿ ✿ ✿ ✿ 🌸🌸 ✿ ✿ ✿ ✿ ✿ 🌸🌸 ✿ ✿ ✿ ✿ ✿ 🌸🌸 ✿ ✿ ✿ ✿ ✿ 🌸🌸 ✿ ✿ ✿ ✿ ✿ 🌸🌸 ✿ ✿ ✿ ✿ ✿ 🌸🌸 ✿ ✿ ✿ ✿ ✿ 🌸🌸 ✿ ✿ ✿ ✿ ✿ 🌸\n",
        "\n",
        "🌿 ───────────────────────────── 🌿🌿 ───────────────────────────── 🌿🌿 ───────────────────────────── 🌿🌿 ───────────────────────────── 🌿\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"text-align: center;\">\n",
        "  <h1><b>🏁 Training Session Completed Successfully</b></h1>\n",
        "</div>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
