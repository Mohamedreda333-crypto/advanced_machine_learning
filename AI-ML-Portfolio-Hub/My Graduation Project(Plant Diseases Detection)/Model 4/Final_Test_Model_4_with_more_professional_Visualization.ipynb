{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNrSz7F2YbWJ",
        "outputId": "bd6d9583-fccc-4fcd-be0f-40d8c5b80eb2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from google.colab import drive\n",
        "from datetime import datetime\n",
        "import matplotlib as mpl\n",
        "import logging\n",
        "from PIL import Image\n",
        "from math import pi\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from itertools import cycle\n",
        "from matplotlib.patches import Circle, RegularPolygon\n",
        "from matplotlib.path import Path\n",
        "from matplotlib.projections.polar import PolarAxes\n",
        "from matplotlib.projections import register_projection\n",
        "from matplotlib.spines import Spine\n",
        "from matplotlib.transforms import Affine2D\n",
        "\n",
        "# Configure matplotlib for professional style\n",
        "mpl.rcParams['figure.facecolor'] = 'white'\n",
        "mpl.rcParams['axes.grid'] = False\n",
        "mpl.rcParams['font.size'] = 12\n",
        "mpl.rcParams['axes.titlesize'] = 16\n",
        "mpl.rcParams['axes.titleweight'] = 'bold'\n",
        "mpl.rcParams['axes.labelsize'] = 14\n",
        "mpl.rcParams['xtick.labelsize'] = 10\n",
        "mpl.rcParams['ytick.labelsize'] = 10\n",
        "mpl.rcParams['legend.fontsize'] = 10\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('model_testing.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "lgVWUQBCYeuU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelTester:\n",
        "    def __init__(self, model_path, data_dir, img_size=(384, 384), batch_size=32):\n",
        "        self.model_path = model_path\n",
        "        self.data_dir = data_dir\n",
        "        self.img_size = img_size\n",
        "        self.batch_size = batch_size\n",
        "        self.model = None\n",
        "        self.class_names = None\n",
        "        self.test_ds = None\n",
        "        self.results_dir = None\n",
        "        self._setup_directories()\n",
        "\n",
        "    def _setup_directories(self):\n",
        "\n",
        "\n",
        "        # Create timestamped results directory\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        self.results_dir = f\"/content/drive/MyDrive/Graduation Project/saved_models/Model_Evaluation_Results_Model4/{timestamp}\"\n",
        "        os.makedirs(self.results_dir, exist_ok=True)\n",
        "        os.makedirs(os.path.join(self.results_dir, \"plots\"), exist_ok=True)\n",
        "        os.makedirs(os.path.join(self.results_dir, \"metrics\"), exist_ok=True)\n",
        "        os.makedirs(os.path.join(self.results_dir, \"misclassified_samples\"), exist_ok=True)\n",
        "\n",
        "        logger.info(f\"Results will be saved to: {self.results_dir}\")\n",
        "\n",
        "    def load_model(self):\n",
        "        \"\"\"Load the trained model from disk\"\"\"\n",
        "        try:\n",
        "            logger.info(f\"Loading model from {self.model_path}\")\n",
        "            self.model = keras.models.load_model(self.model_path)\n",
        "            logger.info(\"Model loaded successfully\")\n",
        "\n",
        "            # Try to plot model architecture\n",
        "            try:\n",
        "                from tensorflow.keras.utils import plot_model\n",
        "                plot_path = os.path.join(self.results_dir, \"plots\", \"model_architecture.png\")\n",
        "                plot_model(\n",
        "                    self.model,\n",
        "                    to_file=plot_path,\n",
        "                    show_shapes=True,\n",
        "                    show_layer_names=True,\n",
        "                    rankdir='TB',\n",
        "                    expand_nested=False,  # Changed to False to avoid Graphviz error\n",
        "                    dpi=96\n",
        "                )\n",
        "                logger.info(f\"Model architecture plot saved to {plot_path}\")\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Could not generate model architecture plot: {str(e)}\")\n",
        "                # Install Graphviz if not available\n",
        "                logger.info(\"Attempting to install Graphviz...\")\n",
        "                try:\n",
        "                    !apt-get install graphviz\n",
        "                    !pip install pydot\n",
        "                    from tensorflow.keras.utils import plot_model\n",
        "                    plot_model(\n",
        "                        self.model,\n",
        "                        to_file=plot_path,\n",
        "                        show_shapes=True,\n",
        "                        show_layer_names=True,\n",
        "                        rankdir='TB',\n",
        "                        expand_nested=False,\n",
        "                        dpi=96\n",
        "                    )\n",
        "                    logger.info(f\"Model architecture plot saved to {plot_path}\")\n",
        "                except:\n",
        "                    logger.warning(\"Graphviz installation failed. Skipping model plot.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading model: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def create_test_dataset(self):\n",
        "        \"\"\"Create a test dataset from the data directory\"\"\"\n",
        "        try:\n",
        "            logger.info(\"Creating test dataset\")\n",
        "\n",
        "            # Create test dataset with explicit validation split\n",
        "            test_ds = keras.utils.image_dataset_from_directory(\n",
        "                self.data_dir,\n",
        "                validation_split=0.1,\n",
        "                subset='validation',\n",
        "                seed=42,\n",
        "                image_size=self.img_size,\n",
        "                batch_size=self.batch_size,\n",
        "                label_mode='categorical',\n",
        "                shuffle=True  # Changed to True to ensure better class distribution\n",
        "            )\n",
        "\n",
        "            # Get class names from the directory structure\n",
        "            self.class_names = sorted(os.listdir(self.data_dir))\n",
        "            logger.info(f\"Found {len(self.class_names)} classes: {self.class_names}\")\n",
        "\n",
        "            # Ensure we have samples from all classes\n",
        "            class_counts = {class_name: 0 for class_name in self.class_names}\n",
        "            for _, labels in test_ds:\n",
        "                class_indices = tf.argmax(labels, axis=1)\n",
        "                for idx in class_indices.numpy():\n",
        "                    class_counts[self.class_names[idx]] += 1\n",
        "\n",
        "            logger.info(f\"Class distribution in validation set: {class_counts}\")\n",
        "\n",
        "            # Optimize dataset performance\n",
        "            self.test_ds = test_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "            # Plot class distribution\n",
        "            self._plot_class_distribution(test_ds)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error creating test dataset: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _plot_class_distribution(self, dataset):\n",
        "        \"\"\"Plot class distribution in the test set\"\"\"\n",
        "        logger.info(\"Plotting class distribution\")\n",
        "\n",
        "        # Count samples per class\n",
        "        class_counts = {class_name: 0 for class_name in self.class_names}\n",
        "        for _, labels in dataset:\n",
        "            class_indices = tf.argmax(labels, axis=1)\n",
        "            for idx in class_indices.numpy():\n",
        "                class_counts[self.class_names[idx]] += 1\n",
        "\n",
        "        # Create plot with adjusted size for 12 classes\n",
        "        plt.figure(figsize=(16, 8))\n",
        "        bars = plt.bar(class_counts.keys(), class_counts.values(), color='#1f77b4')\n",
        "\n",
        "        # Add value labels\n",
        "        for bar in bars:\n",
        "            height = bar.get_height()\n",
        "            plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                    f'{height}',\n",
        "                    ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "        plt.title('Test Set Class Distribution', pad=20)\n",
        "        plt.xlabel('Class', labelpad=10)\n",
        "        plt.ylabel('Number of Samples', labelpad=10)\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save plot\n",
        "        plot_path = os.path.join(self.results_dir, \"plots\", \"class_distribution.png\")\n",
        "        plt.savefig(plot_path, bbox_inches='tight', dpi=300)\n",
        "        plt.close()\n",
        "        logger.info(f\"Class distribution plot saved to {plot_path}\")\n",
        "\n",
        "    def evaluate_model(self):\n",
        "        \"\"\"Evaluate model performance on test set\"\"\"\n",
        "        if not self.model or not self.test_ds:\n",
        "            raise ValueError(\"Model and test dataset must be loaded first\")\n",
        "\n",
        "        logger.info(\"Evaluating model on test set\")\n",
        "\n",
        "        # Evaluate metrics\n",
        "        results = self.model.evaluate(self.test_ds, verbose=1)\n",
        "\n",
        "        # Create a dictionary of metrics\n",
        "        metrics = dict(zip(self.model.metrics_names, results))\n",
        "\n",
        "        logger.info(\"\\nTest set evaluation:\")\n",
        "        for name, value in metrics.items():\n",
        "            logger.info(f\"{name}: {value:.4f}\")\n",
        "\n",
        "        # Save metrics to file\n",
        "        metrics_path = os.path.join(self.results_dir, \"metrics\", \"test_metrics.txt\")\n",
        "        with open(metrics_path, 'w') as f:\n",
        "            for name, value in metrics.items():\n",
        "                f.write(f\"{name}: {value:.4f}\\n\")\n",
        "\n",
        "        logger.info(f\"Metrics saved to {metrics_path}\")\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def generate_classification_report(self):\n",
        "        \"\"\"Generate a detailed classification report\"\"\"\n",
        "        if not self.model or not self.test_ds:\n",
        "            raise ValueError(\"Model and test dataset must be loaded first\")\n",
        "\n",
        "        logger.info(\"Generating classification report\")\n",
        "\n",
        "        # Get true labels and predictions\n",
        "        y_true = []\n",
        "        y_pred = []\n",
        "        y_prob = []\n",
        "        misclassified_samples = []\n",
        "\n",
        "        # Ensure we evaluate all batches\n",
        "        for images, labels in self.test_ds:\n",
        "            # Get true labels\n",
        "            y_true.extend(np.argmax(labels.numpy(), axis=1))\n",
        "\n",
        "            # Get predictions\n",
        "            preds = self.model.predict(images, verbose=0)\n",
        "            y_pred.extend(np.argmax(preds, axis=1))\n",
        "            y_prob.extend(preds)\n",
        "\n",
        "            # Collect misclassified samples\n",
        "            true_classes = np.argmax(labels.numpy(), axis=1)\n",
        "            pred_classes = np.argmax(preds, axis=1)\n",
        "            for i in range(len(true_classes)):\n",
        "                if true_classes[i] != pred_classes[i]:\n",
        "                    misclassified_samples.append({\n",
        "                        'image': images[i].numpy(),\n",
        "                        'true_class': true_classes[i],\n",
        "                        'pred_class': pred_classes[i],\n",
        "                        'probabilities': preds[i]\n",
        "                    })\n",
        "\n",
        "        # Always use all class names, even if some weren't in the validation set\n",
        "        target_names = self.class_names\n",
        "        labels = list(range(len(self.class_names)))\n",
        "\n",
        "        # Generate classification report\n",
        "        report = classification_report(\n",
        "            y_true,\n",
        "            y_pred,\n",
        "            labels=labels,\n",
        "            target_names=target_names,\n",
        "            digits=4,\n",
        "            output_dict=False\n",
        "        )\n",
        "\n",
        "        report_dict = classification_report(\n",
        "            y_true,\n",
        "            y_pred,\n",
        "            labels=labels,\n",
        "            target_names=target_names,\n",
        "            digits=4,\n",
        "            output_dict=True\n",
        "        )\n",
        "\n",
        "        logger.info(\"\\nClassification Report:\\n\" + report)\n",
        "\n",
        "        # Save report to file\n",
        "        report_path = os.path.join(self.results_dir, \"metrics\", \"classification_report.txt\")\n",
        "        with open(report_path, 'w') as f:\n",
        "            f.write(report)\n",
        "\n",
        "        # Plot precision-recall metrics\n",
        "        self._plot_precision_recall(report_dict)\n",
        "\n",
        "        # Plot confusion matrix with all classes\n",
        "        self._plot_confusion_matrix(y_true, y_pred, labels=labels, target_names=target_names)\n",
        "\n",
        "        # Generate new plots\n",
        "        self._generate_radar_plot(report_dict)\n",
        "        self._plot_roc_curves(y_true, np.array(y_prob))\n",
        "        self._plot_top_k_accuracy(y_true, np.array(y_prob))\n",
        "        self._plot_class_confidence_distribution(y_true, np.array(y_prob))\n",
        "\n",
        "        # Save misclassified samples\n",
        "        self._save_misclassified_samples(misclassified_samples)\n",
        "\n",
        "        return report, y_true, y_pred\n",
        "\n",
        "    def _plot_precision_recall(self, report_dict):\n",
        "        \"\"\"Plot precision and recall metrics per class\"\"\"\n",
        "        logger.info(\"Plotting precision and recall metrics\")\n",
        "\n",
        "        # Prepare data - only include actual classes (skip averages)\n",
        "        metrics = []\n",
        "        for class_name in self.class_names:\n",
        "            if class_name in report_dict:  # Skip 'accuracy', 'macro avg', etc.\n",
        "                metrics.append({\n",
        "                    'Class': class_name,\n",
        "                    'Precision': report_dict[class_name]['precision'],\n",
        "                    'Recall': report_dict[class_name]['recall'],\n",
        "                    'F1-Score': report_dict[class_name]['f1-score']\n",
        "                })\n",
        "\n",
        "        df = pd.DataFrame(metrics)\n",
        "        df = df.sort_values('F1-Score', ascending=False)\n",
        "\n",
        "        # Create plot with adjusted size for 12 classes\n",
        "        plt.figure(figsize=(18, 8))\n",
        "\n",
        "        x = np.arange(len(df))\n",
        "        width = 0.25\n",
        "\n",
        "        plt.bar(x - width, df['Precision'], width, label='Precision', color='#2ca02c')\n",
        "        plt.bar(x, df['Recall'], width, label='Recall', color='#1f77b4')\n",
        "        plt.bar(x + width, df['F1-Score'], width, label='F1-Score', color='#ff7f0e')\n",
        "\n",
        "        plt.title('Precision, Recall, and F1-Score by Class', pad=20)\n",
        "        plt.xlabel('Class', labelpad=10)\n",
        "        plt.ylabel('Score', labelpad=10)\n",
        "        plt.xticks(x, df['Class'], rotation=45, ha='right')\n",
        "        plt.ylim(0, 1.1)\n",
        "        plt.legend(loc='upper right', bbox_to_anchor=(1.15, 1))\n",
        "\n",
        "        # Add value labels\n",
        "        for i, (prec, rec, f1) in enumerate(zip(df['Precision'], df['Recall'], df['F1-Score'])):\n",
        "            plt.text(i - width, prec + 0.02, f\"{prec:.2f}\", ha='center', fontsize=8)\n",
        "            plt.text(i, rec + 0.02, f\"{rec:.2f}\", ha='center', fontsize=8)\n",
        "            plt.text(i + width, f1 + 0.02, f\"{f1:.2f}\", ha='center', fontsize=8)\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save plot\n",
        "        plot_path = os.path.join(self.results_dir, \"plots\", \"precision_recall.png\")\n",
        "        plt.savefig(plot_path, bbox_inches='tight', dpi=300)\n",
        "        plt.close()\n",
        "        logger.info(f\"Precision-recall plot saved to {plot_path}\")\n",
        "\n",
        "    def _plot_confusion_matrix(self, y_true, y_pred, normalize=True, cmap='Blues', labels=None, target_names=None):\n",
        "        \"\"\"Plot a professional confusion matrix with all classes\"\"\"\n",
        "        logger.info(\"Generating confusion matrix\")\n",
        "\n",
        "        if labels is None:\n",
        "            labels = list(range(len(self.class_names)))\n",
        "        if target_names is None:\n",
        "            target_names = self.class_names\n",
        "\n",
        "        # Compute confusion matrix using all specified labels\n",
        "        cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "        # Fill in zeros for any classes not present in the validation set\n",
        "        if normalize:\n",
        "            with np.errstate(divide='ignore', invalid='ignore'):\n",
        "                cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "                cm[np.isnan(cm)] = 0  # Set NaNs to 0 for classes with no true samples\n",
        "            fmt = '.2f'\n",
        "            title = 'Normalized Confusion Matrix'\n",
        "        else:\n",
        "            fmt = 'd'\n",
        "            title = 'Confusion Matrix (Counts)'\n",
        "\n",
        "        # Create figure with adjusted size\n",
        "        fig_size = (12 + len(target_names) * 0.5, 10 + len(target_names) * 0.5)\n",
        "        fig, ax = plt.subplots(figsize=fig_size)\n",
        "\n",
        "        # Create heatmap\n",
        "        sns.heatmap(cm, annot=True, fmt=fmt, cmap=cmap, ax=ax,\n",
        "                    cbar_kws={'label': 'Normalized Count' if normalize else 'Count'},\n",
        "                    xticklabels=target_names,\n",
        "                    yticklabels=target_names)\n",
        "\n",
        "        # Add title and labels\n",
        "        ax.set_title(title, fontsize=16, pad=20)\n",
        "        ax.set_xlabel('Predicted Label', fontsize=14, labelpad=10)\n",
        "        ax.set_ylabel('True Label', fontsize=14, labelpad=10)\n",
        "\n",
        "        # Rotate tick labels\n",
        "        plt.setp(ax.get_xticklabels(), rotation=45, ha='right', fontsize=10)\n",
        "        plt.setp(ax.get_yticklabels(), rotation=0, fontsize=10)\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save figure\n",
        "        plot_path = os.path.join(self.results_dir, \"plots\", \"confusion_matrix.png\")\n",
        "        plt.savefig(plot_path, bbox_inches='tight', dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "        logger.info(f\"Confusion matrix saved to {plot_path}\")\n",
        "\n",
        "    def _save_misclassified_samples(self, misclassified_samples, num_samples=24):\n",
        "        \"\"\"Save misclassified samples with their probabilities\"\"\"\n",
        "        if not misclassified_samples:\n",
        "            logger.info(\"No misclassified samples found\")\n",
        "            return\n",
        "\n",
        "        logger.info(f\"Saving {min(num_samples, len(misclassified_samples))} misclassified samples\")\n",
        "\n",
        "        # Select random samples if we have too many\n",
        "        if len(misclassified_samples) > num_samples:\n",
        "            misclassified_samples = np.random.choice(misclassified_samples, size=num_samples, replace=False)\n",
        "\n",
        "        # Create figure\n",
        "        plt.figure(figsize=(20, 20))\n",
        "        n_cols = 4\n",
        "        n_rows = int(np.ceil(len(misclassified_samples) / n_cols))\n",
        "\n",
        "        for i, sample in enumerate(misclassified_samples):\n",
        "            plt.subplot(n_rows, n_cols, i+1)\n",
        "\n",
        "            # Display image\n",
        "            img = (sample['image'] - sample['image'].min()) / (sample['image'].max() - sample['image'].min())\n",
        "            plt.imshow(img)\n",
        "\n",
        "            # Get top 3 predicted classes\n",
        "            top3_idx = np.argsort(sample['probabilities'])[-3:][::-1]\n",
        "            top3_classes = [self.class_names[idx] for idx in top3_idx]\n",
        "            top3_probs = [sample['probabilities'][idx] for idx in top3_idx]\n",
        "\n",
        "            # Create prediction info text\n",
        "            pred_text = \"\\n\".join([f\"{cls}: {prob:.2f}\" for cls, prob in zip(top3_classes, top3_probs)])\n",
        "\n",
        "            # Set title with color coding\n",
        "            title = f\"True: {self.class_names[sample['true_class']]}\\nPred: {self.class_names[sample['pred_class']]}\\n\\n{pred_text}\"\n",
        "            color = 'green' if sample['true_class'] == sample['pred_class'] else 'red'\n",
        "            plt.title(title, color=color, fontsize=8, pad=4)\n",
        "\n",
        "            plt.axis('off')\n",
        "\n",
        "        plt.suptitle('Misclassified Samples with Prediction Probabilities (Top 3)', fontsize=16, y=1.02)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save figure\n",
        "        plot_path = os.path.join(self.results_dir, \"plots\", \"misclassified_samples.png\")\n",
        "        plt.savefig(plot_path, bbox_inches='tight', dpi=150)\n",
        "        plt.close()\n",
        "\n",
        "        # Save individual misclassified samples\n",
        "        for i, sample in enumerate(misclassified_samples[:24]):  # Limit to 24 to avoid too many files\n",
        "            img = (sample['image'] - sample['image'].min()) / (sample['image'].max() - sample['image'].min())\n",
        "            img_path = os.path.join(self.results_dir, \"misclassified_samples\", f\"sample_{i}.png\")\n",
        "            plt.imsave(img_path, img)\n",
        "\n",
        "        logger.info(f\"Misclassified samples saved to {plot_path}\")\n",
        "\n",
        "    def run_full_evaluation(self):\n",
        "        \"\"\"Run the complete evaluation pipeline\"\"\"\n",
        "        try:\n",
        "            # Load model and data\n",
        "            self.load_model()\n",
        "            self.create_test_dataset()\n",
        "\n",
        "            # Evaluate metrics\n",
        "            metrics = self.evaluate_model()\n",
        "\n",
        "            # Generate reports and plots\n",
        "            report, y_true, y_pred = self.generate_classification_report()\n",
        "\n",
        "            # Save results to file\n",
        "            results_path = os.path.join(self.results_dir, \"evaluation_results.txt\")\n",
        "            with open(results_path, \"w\") as f:\n",
        "                f.write(\"Model Evaluation Results\\n\")\n",
        "                f.write(\"=\"*50 + \"\\n\\n\")\n",
        "                f.write(f\"Evaluation Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
        "                f.write(f\"Number of Classes: {len(self.class_names)}\\n\")\n",
        "                f.write(f\"Classes: {', '.join(self.class_names)}\\n\\n\")\n",
        "                f.write(\"Metrics:\\n\")\n",
        "                for name, value in metrics.items():\n",
        "                    f.write(f\"{name}: {value:.4f}\\n\")\n",
        "                f.write(\"\\nClassification Report:\\n\")\n",
        "                f.write(report)\n",
        "\n",
        "            logger.info(f\"Full evaluation completed. Results saved to {self.results_dir}\")\n",
        "\n",
        "            # Create a summary report\n",
        "            self._create_summary_report(metrics, report)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Evaluation failed: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _create_summary_report(self, metrics, report):\n",
        "        \"\"\"Create a visual summary report of the evaluation\"\"\"\n",
        "        logger.info(\"Creating summary report\")\n",
        "\n",
        "        # Create figure\n",
        "        fig = plt.figure(figsize=(18, 14), facecolor='white')\n",
        "\n",
        "        # Add title\n",
        "        fig.suptitle(\"Model Evaluation Summary\", fontsize=18, fontweight='bold', y=0.97)\n",
        "\n",
        "        # Create grid for subplots\n",
        "        gs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.2)\n",
        "\n",
        "        # Plot 1: Main metrics\n",
        "        ax1 = fig.add_subplot(gs[0, 0])\n",
        "        main_metrics = {k: v for k, v in metrics.items() if k in ['loss', 'accuracy', 'precision', 'recall']}\n",
        "        colors = ['#d62728', '#2ca02c', '#9467bd', '#ff7f0e']\n",
        "        ax1.bar(main_metrics.keys(), main_metrics.values(), color=colors)\n",
        "        ax1.set_title('Key Metrics', fontsize=14)\n",
        "        ax1.set_ylim(0, 1.1)\n",
        "        for i, v in enumerate(main_metrics.values()):\n",
        "            ax1.text(i, v + 0.02, f\"{v:.4f}\", ha='center')\n",
        "\n",
        "        # Plot 2: Confusion matrix thumbnail\n",
        "        ax2 = fig.add_subplot(gs[0, 1])\n",
        "        cm_path = os.path.join(self.results_dir, \"plots\", \"confusion_matrix.png\")\n",
        "        if os.path.exists(cm_path):\n",
        "            cm_img = plt.imread(cm_path)\n",
        "            ax2.imshow(cm_img)\n",
        "            ax2.axis('off')\n",
        "            ax2.set_title('Confusion Matrix', fontsize=14)\n",
        "        else:\n",
        "            ax2.text(0.5, 0.5, \"Confusion Matrix\\nNot Available\", ha='center', va='center')\n",
        "            ax2.axis('off')\n",
        "\n",
        "        # Plot 3: Precision-Recall thumbnail\n",
        "        ax3 = fig.add_subplot(gs[1, 0])\n",
        "        pr_path = os.path.join(self.results_dir, \"plots\", \"precision_recall.png\")\n",
        "        if os.path.exists(pr_path):\n",
        "            pr_img = plt.imread(pr_path)\n",
        "            ax3.imshow(pr_img)\n",
        "            ax3.axis('off')\n",
        "            ax3.set_title('Precision-Recall Metrics', fontsize=14)\n",
        "        else:\n",
        "            ax3.text(0.5, 0.5, \"Precision-Recall Plot\\nNot Available\", ha='center', va='center')\n",
        "            ax3.axis('off')\n",
        "\n",
        "        # Plot 4: Class distribution thumbnail\n",
        "        ax4 = fig.add_subplot(gs[1, 1])\n",
        "        cd_path = os.path.join(self.results_dir, \"plots\", \"class_distribution.png\")\n",
        "        if os.path.exists(cd_path):\n",
        "            cd_img = plt.imread(cd_path)\n",
        "            ax4.imshow(cd_img)\n",
        "            ax4.axis('off')\n",
        "            ax4.set_title('Class Distribution', fontsize=14)\n",
        "        else:\n",
        "            ax4.text(0.5, 0.5, \"Class Distribution\\nNot Available\", ha='center', va='center')\n",
        "            ax4.axis('off')\n",
        "\n",
        "        # Add timestamp and class info\n",
        "        fig.text(0.5, 0.02,\n",
        "                f\"Generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} | Classes: {', '.join(self.class_names)}\",\n",
        "                ha='center', fontsize=10)\n",
        "\n",
        "        # Save summary report\n",
        "        summary_path = os.path.join(self.results_dir, \"summary_report.png\")\n",
        "        plt.savefig(summary_path, bbox_inches='tight', dpi=150)\n",
        "        plt.close()\n",
        "\n",
        "        logger.info(f\"Summary report saved to {summary_path}\")\n",
        "\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.suptitle('Misclassified Samples with Prediction Probabilities (Top 3)', fontsize=16, y=1.02)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save figure\n",
        "        plot_path = os.path.join(self.results_dir, \"plots\", \"misclassified_samples.png\")\n",
        "        plt.savefig(plot_path, bbox_inches='tight', dpi=150)\n",
        "        plt.close()\n",
        "\n",
        "        # Save individual misclassified samples\n",
        "        for i, sample in enumerate(misclassified_samples[:24]):  # Limit to 24 to avoid too many files\n",
        "            img = (sample['image'] - sample['image'].min()) / (sample['image'].max() - sample['image'].min())\n",
        "            img_path = os.path.join(self.results_dir, \"misclassified_samples\", f\"sample_{i}.png\")\n",
        "            plt.imsave(img_path, img)\n",
        "\n",
        "        logger.info(f\"Misclassified samples saved to {plot_path}\")\n",
        "    def _generate_radar_plot(self, report_dict):\n",
        "        \"\"\"Generate a radar plot for model performance metrics\"\"\"\n",
        "        logger.info(\"Generating radar plot of performance metrics\")\n",
        "\n",
        "        try:\n",
        "            # Prepare data for radar plot\n",
        "            metrics = ['precision', 'recall', 'f1-score']\n",
        "            classes = [cls for cls in self.class_names if cls in report_dict]\n",
        "\n",
        "            # Extract values for each class\n",
        "            data = []\n",
        "            for cls in classes:\n",
        "                class_data = []\n",
        "                for metric in metrics:\n",
        "                    class_data.append(report_dict[cls][metric])\n",
        "                data.append(class_data)\n",
        "\n",
        "            # Calculate averages\n",
        "            avg_values = [np.mean([data[i][j] for i in range(len(classes))])\n",
        "                        for j in range(len(metrics))]\n",
        "            data.append(avg_values)\n",
        "            classes.append('Average')\n",
        "\n",
        "            # Create radar plot\n",
        "            fig = plt.figure(figsize=(10, 10))\n",
        "            ax = self._radar_factory(len(metrics), frame='polygon')\n",
        "\n",
        "            # Plot each class\n",
        "            colors = plt.cm.viridis(np.linspace(0, 1, len(classes)))\n",
        "            for d, color, label in zip(data, colors, classes):\n",
        "                ax.plot(angles, d, color=color, linewidth=2, label=label)\n",
        "                ax.fill(angles, d, color=color, alpha=0.25)\n",
        "\n",
        "            # Add legend and title\n",
        "            ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
        "            plt.title('Class Performance Radar Plot', size=16, pad=20)\n",
        "\n",
        "            # Save plot\n",
        "            plot_path = os.path.join(self.results_dir, \"plots\", \"radar_plot.png\")\n",
        "            plt.savefig(plot_path, bbox_inches='tight', dpi=300)\n",
        "            plt.close()\n",
        "            logger.info(f\"Radar plot saved to {plot_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Could not generate radar plot: {str(e)}\")\n",
        "    def _radar_factory(self, num_vars, frame='circle'):\n",
        "        \"\"\"Create a radar chart with num_vars axes\"\"\"\n",
        "        theta = np.linspace(0, 2*np.pi, num_vars, endpoint=False)\n",
        "\n",
        "        class RadarAxes(PolarAxes):\n",
        "            name = 'radar'\n",
        "            def __init__(self, *args, **kwargs):\n",
        "                super().__init__(*args, **kwargs)\n",
        "                self.set_theta_zero_location('N')\n",
        "\n",
        "            def fill(self, *args, closed=True, **kwargs):\n",
        "                return super().fill(closed=closed, *args, **kwargs)\n",
        "\n",
        "            def plot(self, *args, **kwargs):\n",
        "                lines = super().plot(*args, **kwargs)\n",
        "                for line in lines:\n",
        "                    self._close_line(line)\n",
        "\n",
        "            def _close_line(self, line):\n",
        "                x, y = line.get_data()\n",
        "                if x[0] != x[-1]:\n",
        "                    x = np.append(x, x[0])\n",
        "                    y = np.append(y, y[0])\n",
        "                    line.set_data(x, y)\n",
        "\n",
        "            def set_varlabels(self, labels):\n",
        "                self.set_thetagrids(np.degrees(theta), labels)\n",
        "\n",
        "            def _gen_axes_patch(self):\n",
        "                if frame == 'circle':\n",
        "                    return Circle((0.5, 0.5), 0.5)\n",
        "                elif frame == 'polygon':\n",
        "                    return RegularPolygon((0.5, 0.5), num_vars,\n",
        "                                        radius=0.5, edgecolor=\"k\")\n",
        "                else:\n",
        "                    raise ValueError(\"Unknown value for 'frame': %s\" % frame)\n",
        "\n",
        "            def _gen_axes_spines(self):\n",
        "                if frame == 'circle':\n",
        "                    return super()._gen_axes_spines()\n",
        "                elif frame == 'polygon':\n",
        "                    spine = Spine(axes=self,\n",
        "                                spine_type='circle',\n",
        "                                path=Path.unit_regular_polygon(num_vars))\n",
        "                    spine.set_transform(Affine2D().scale(0.5).translate(0.5, 0.5))\n",
        "                    return {'polar': spine}\n",
        "                else:\n",
        "                    raise ValueError(\"Unknown value for 'frame': %s\" % frame)\n",
        "\n",
        "        register_projection(RadarAxes)\n",
        "        return plt.subplot(111, projection='radar')\n",
        "\n",
        "    def _plot_roc_curves(self, y_true, y_prob):\n",
        "        \"\"\"Plot ROC curves for multiclass classification\"\"\"\n",
        "        logger.info(\"Generating ROC curves\")\n",
        "\n",
        "        try:\n",
        "            # Binarize the output\n",
        "            y_true_bin = label_binarize(y_true, classes=list(range(len(self.class_names))))\n",
        "            n_classes = y_true_bin.shape[1]\n",
        "\n",
        "            # Compute ROC curve and ROC area for each class\n",
        "            fpr = dict()\n",
        "            tpr = dict()\n",
        "            roc_auc = dict()\n",
        "            for i in range(n_classes):\n",
        "                fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_prob[:, i])\n",
        "                roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "            # Compute micro-average ROC curve and ROC area\n",
        "            fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true_bin.ravel(), y_prob.ravel())\n",
        "            roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "\n",
        "            # First aggregate all false positive rates\n",
        "            all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
        "\n",
        "            # Then interpolate all ROC curves at these points\n",
        "            mean_tpr = np.zeros_like(all_fpr)\n",
        "            for i in range(n_classes):\n",
        "                mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
        "\n",
        "            # Finally average it and compute AUC\n",
        "            mean_tpr /= n_classes\n",
        "\n",
        "            fpr[\"macro\"] = all_fpr\n",
        "            tpr[\"macro\"] = mean_tpr\n",
        "            roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
        "\n",
        "            # Plot all ROC curves\n",
        "            plt.figure(figsize=(10, 8))\n",
        "            colors = cycle(['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728',\n",
        "                          '#9467bd', '#8c564b', '#e377c2', '#7f7f7f',\n",
        "                          '#bcbd22', '#17becf', '#1a55FF', '#FF33CC'])\n",
        "\n",
        "            for i, color in zip(range(n_classes), colors):\n",
        "                plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
        "                        label='ROC curve of class {0} (AUC = {1:0.2f})'\n",
        "                        ''.format(self.class_names[i], roc_auc[i]))\n",
        "\n",
        "            plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "            plt.xlim([0.0, 1.0])\n",
        "            plt.ylim([0.0, 1.05])\n",
        "            plt.xlabel('False Positive Rate', fontsize=12)\n",
        "            plt.ylabel('True Positive Rate', fontsize=12)\n",
        "            plt.title('Receiver Operating Characteristic (ROC) Curves', fontsize=16, pad=20)\n",
        "            plt.legend(loc=\"lower right\", bbox_to_anchor=(1.4, 0), fontsize=10)\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "            # Save plot\n",
        "            plot_path = os.path.join(self.results_dir, \"plots\", \"roc_curves.png\")\n",
        "            plt.savefig(plot_path, bbox_inches='tight', dpi=300)\n",
        "            plt.close()\n",
        "            logger.info(f\"ROC curves plot saved to {plot_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Could not generate ROC curves: {str(e)}\")\n",
        "\n",
        "    def _plot_top_k_accuracy(self, y_true, y_prob, max_k=5):\n",
        "        \"\"\"Plot top-k accuracy for different values of k\"\"\"\n",
        "        logger.info(\"Generating top-k accuracy plot\")\n",
        "\n",
        "        try:\n",
        "            # Calculate top-k accuracy for different k values\n",
        "            top_k_acc = []\n",
        "            for k in range(1, max_k + 1):\n",
        "                correct = 0\n",
        "                for i in range(len(y_true)):\n",
        "                    top_k_pred = np.argsort(y_prob[i])[-k:]\n",
        "                    if y_true[i] in top_k_pred:\n",
        "                        correct += 1\n",
        "                top_k_acc.append(correct / len(y_true))\n",
        "\n",
        "            # Create plot\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.plot(range(1, max_k + 1), top_k_acc, marker='o',\n",
        "                    color='#1f77b4', linewidth=2, markersize=8)\n",
        "\n",
        "            # Add value labels\n",
        "            for k, acc in zip(range(1, max_k + 1), top_k_acc):\n",
        "                plt.text(k, acc + 0.01, f\"{acc:.3f}\", ha='center', fontsize=10)\n",
        "\n",
        "            plt.title('Top-k Classification Accuracy', fontsize=16, pad=20)\n",
        "            plt.xlabel('k (number of top predictions considered)', fontsize=12)\n",
        "            plt.ylabel('Accuracy', fontsize=12)\n",
        "            plt.xticks(range(1, max_k + 1))\n",
        "            plt.ylim(0, 1.1)\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "            # Save plot\n",
        "            plot_path = os.path.join(self.results_dir, \"plots\", \"top_k_accuracy.png\")\n",
        "            plt.savefig(plot_path, bbox_inches='tight', dpi=300)\n",
        "            plt.close()\n",
        "            logger.info(f\"Top-k accuracy plot saved to {plot_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Could not generate top-k accuracy plot: {str(e)}\")\n",
        "\n",
        "    def _plot_class_confidence_distribution(self, y_true, y_prob):\n",
        "        \"\"\"Plot distribution of prediction confidence by class\"\"\"\n",
        "        logger.info(\"Generating class confidence distribution plot\")\n",
        "\n",
        "        try:\n",
        "            # Prepare data\n",
        "            confidences = []\n",
        "            classes = []\n",
        "            for true_class, probs in zip(y_true, y_prob):\n",
        "                confidences.append(probs[true_class])\n",
        "                classes.append(self.class_names[true_class])\n",
        "\n",
        "            # Create DataFrame for easier plotting\n",
        "            df = pd.DataFrame({'Class': classes, 'Confidence': confidences})\n",
        "\n",
        "            # Create plot\n",
        "            plt.figure(figsize=(12, 8))\n",
        "            sns.boxplot(x='Class', y='Confidence', data=df, palette='viridis')\n",
        "            plt.title('Distribution of Prediction Confidence by Class', fontsize=16, pad=20)\n",
        "            plt.xlabel('Class', fontsize=12)\n",
        "            plt.ylabel('Confidence', fontsize=12)\n",
        "            plt.xticks(rotation=45, ha='right')\n",
        "            plt.ylim(0, 1.1)\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "            # Save plot\n",
        "            plot_path = os.path.join(self.results_dir, \"plots\", \"confidence_distribution.png\")\n",
        "            plt.savefig(plot_path, bbox_inches='tight', dpi=300)\n",
        "            plt.close()\n",
        "            logger.info(f\"Confidence distribution plot saved to {plot_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Could not generate confidence distribution plot: {str(e)}\")\n",
        "\n",
        "\n",
        "    def run_full_evaluation(self):\n",
        "        \"\"\"Run the complete evaluation pipeline\"\"\"\n",
        "        try:\n",
        "            # Load model and data\n",
        "            self.load_model()\n",
        "            self.create_test_dataset()\n",
        "\n",
        "            # Evaluate metrics\n",
        "            metrics = self.evaluate_model()\n",
        "\n",
        "            # Generate reports and plots\n",
        "            report, y_true, y_pred = self.generate_classification_report()\n",
        "\n",
        "            # Save results to file\n",
        "            results_path = os.path.join(self.results_dir, \"evaluation_results.txt\")\n",
        "            with open(results_path, \"w\") as f:\n",
        "                f.write(\"Model Evaluation Results\\n\")\n",
        "                f.write(\"=\"*50 + \"\\n\\n\")\n",
        "                f.write(f\"Evaluation Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
        "                f.write(f\"Number of Classes: {len(self.class_names)}\\n\")\n",
        "                f.write(f\"Classes: {', '.join(self.class_names)}\\n\\n\")\n",
        "                f.write(\"Metrics:\\n\")\n",
        "                for name, value in metrics.items():\n",
        "                    f.write(f\"{name}: {value:.4f}\\n\")\n",
        "                f.write(\"\\nClassification Report:\\n\")\n",
        "                f.write(report)\n",
        "\n",
        "            logger.info(f\"Full evaluation completed. Results saved to {self.results_dir}\")\n",
        "\n",
        "            # Create a summary report\n",
        "            self._create_summary_report(metrics, report)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Evaluation failed: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _create_summary_report(self, metrics, report):\n",
        "        \"\"\"Create a visual summary report of the evaluation\"\"\"\n",
        "        logger.info(\"Creating summary report\")\n",
        "\n",
        "        # Create figure with more subplots\n",
        "        fig = plt.figure(figsize=(20, 24), facecolor='white')\n",
        "\n",
        "        # Add title\n",
        "        fig.suptitle(\"Model Evaluation Summary\", fontsize=18, fontweight='bold', y=0.98)\n",
        "\n",
        "        # Create grid for subplots (now 3 rows, 3 columns)\n",
        "        gs = fig.add_gridspec(3, 3, hspace=0.4, wspace=0.3)\n",
        "\n",
        "        # Plot 1: Main metrics\n",
        "        ax1 = fig.add_subplot(gs[0, 0])\n",
        "        main_metrics = {k: v for k, v in metrics.items() if k in ['loss', 'accuracy', 'precision', 'recall']}\n",
        "        colors = ['#d62728', '#2ca02c', '#9467bd', '#ff7f0e']\n",
        "        ax1.bar(main_metrics.keys(), main_metrics.values(), color=colors)\n",
        "        ax1.set_title('Key Metrics', fontsize=14)\n",
        "        ax1.set_ylim(0, 1.1)\n",
        "        for i, v in enumerate(main_metrics.values()):\n",
        "            ax1.text(i, v + 0.02, f\"{v:.4f}\", ha='center')\n",
        "\n",
        "        # Plot 2: Confusion matrix thumbnail\n",
        "        ax2 = fig.add_subplot(gs[0, 1])\n",
        "        cm_path = os.path.join(self.results_dir, \"plots\", \"confusion_matrix.png\")\n",
        "        if os.path.exists(cm_path):\n",
        "            cm_img = plt.imread(cm_path)\n",
        "            ax2.imshow(cm_img)\n",
        "            ax2.axis('off')\n",
        "            ax2.set_title('Confusion Matrix', fontsize=14)\n",
        "        else:\n",
        "            ax2.text(0.5, 0.5, \"Confusion Matrix\\nNot Available\", ha='center', va='center')\n",
        "            ax2.axis('off')\n",
        "\n",
        "        # Plot 3: Precision-Recall thumbnail\n",
        "        ax3 = fig.add_subplot(gs[0, 2])\n",
        "        pr_path = os.path.join(self.results_dir, \"plots\", \"precision_recall.png\")\n",
        "        if os.path.exists(pr_path):\n",
        "            pr_img = plt.imread(pr_path)\n",
        "            ax3.imshow(pr_img)\n",
        "            ax3.axis('off')\n",
        "            ax3.set_title('Precision-Recall Metrics', fontsize=14)\n",
        "        else:\n",
        "            ax3.text(0.5, 0.5, \"Precision-Recall Plot\\nNot Available\", ha='center', va='center')\n",
        "            ax3.axis('off')\n",
        "\n",
        "        # Plot 4: Radar plot thumbnail\n",
        "        ax4 = fig.add_subplot(gs[1, 0])\n",
        "        radar_path = os.path.join(self.results_dir, \"plots\", \"radar_plot.png\")\n",
        "        if os.path.exists(radar_path):\n",
        "            radar_img = plt.imread(radar_path)\n",
        "            ax4.imshow(radar_img)\n",
        "            ax4.axis('off')\n",
        "            ax4.set_title('Performance Radar Plot', fontsize=14)\n",
        "        else:\n",
        "            ax4.text(0.5, 0.5, \"Radar Plot\\nNot Available\", ha='center', va='center')\n",
        "            ax4.axis('off')\n",
        "\n",
        "        # Plot 5: ROC curves thumbnail\n",
        "        ax5 = fig.add_subplot(gs[1, 1])\n",
        "        roc_path = os.path.join(self.results_dir, \"plots\", \"roc_curves.png\")\n",
        "        if os.path.exists(roc_path):\n",
        "            roc_img = plt.imread(roc_path)\n",
        "            ax5.imshow(roc_img)\n",
        "            ax5.axis('off')\n",
        "            ax5.set_title('ROC Curves', fontsize=14)\n",
        "        else:\n",
        "            ax5.text(0.5, 0.5, \"ROC Curves\\nNot Available\", ha='center', va='center')\n",
        "            ax5.axis('off')\n",
        "\n",
        "        # Plot 6: Top-k accuracy thumbnail\n",
        "        ax6 = fig.add_subplot(gs[1, 2])\n",
        "        topk_path = os.path.join(self.results_dir, \"plots\", \"top_k_accuracy.png\")\n",
        "        if os.path.exists(topk_path):\n",
        "            topk_img = plt.imread(topk_path)\n",
        "            ax6.imshow(topk_img)\n",
        "            ax6.axis('off')\n",
        "            ax6.set_title('Top-k Accuracy', fontsize=14)\n",
        "        else:\n",
        "            ax6.text(0.5, 0.5, \"Top-k Accuracy\\nNot Available\", ha='center', va='center')\n",
        "            ax6.axis('off')\n",
        "\n",
        "        # Plot 7: Confidence distribution thumbnail\n",
        "        ax7 = fig.add_subplot(gs[2, 0])\n",
        "        conf_path = os.path.join(self.results_dir, \"plots\", \"confidence_distribution.png\")\n",
        "        if os.path.exists(conf_path):\n",
        "            conf_img = plt.imread(conf_path)\n",
        "            ax7.imshow(conf_img)\n",
        "            ax7.axis('off')\n",
        "            ax7.set_title('Confidence Distribution', fontsize=14)\n",
        "        else:\n",
        "            ax7.text(0.5, 0.5, \"Confidence Distribution\\nNot Available\", ha='center', va='center')\n",
        "            ax7.axis('off')\n",
        "\n",
        "        # Plot 8: Class distribution thumbnail\n",
        "        ax8 = fig.add_subplot(gs[2, 1])\n",
        "        cd_path = os.path.join(self.results_dir, \"plots\", \"class_distribution.png\")\n",
        "        if os.path.exists(cd_path):\n",
        "            cd_img = plt.imread(cd_path)\n",
        "            ax8.imshow(cd_img)\n",
        "            ax8.axis('off')\n",
        "            ax8.set_title('Class Distribution', fontsize=14)\n",
        "        else:\n",
        "            ax8.text(0.5, 0.5, \"Class Distribution\\nNot Available\", ha='center', va='center')\n",
        "            ax8.axis('off')\n",
        "\n",
        "        # Plot 9: Misclassified samples thumbnail\n",
        "        ax9 = fig.add_subplot(gs[2, 2])\n",
        "        mis_path = os.path.join(self.results_dir, \"plots\", \"misclassified_samples.png\")\n",
        "        if os.path.exists(mis_path):\n",
        "            mis_img = plt.imread(mis_path)\n",
        "            ax9.imshow(mis_img)\n",
        "            ax9.axis('off')\n",
        "            ax9.set_title('Misclassified Samples', fontsize=14)\n",
        "        else:\n",
        "            ax9.text(0.5, 0.5, \"Misclassified Samples\\nNot Available\", ha='center', va='center')\n",
        "            ax9.axis('off')\n",
        "\n",
        "        # Add timestamp and class info\n",
        "        fig.text(0.5, 0.02,\n",
        "                f\"Generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} | Classes: {', '.join(self.class_names)}\",\n",
        "                ha='center', fontsize=10)\n",
        "\n",
        "        # Save summary report\n",
        "        summary_path = os.path.join(self.results_dir, \"summary_report.png\")\n",
        "        plt.savefig(summary_path, bbox_inches='tight', dpi=150)\n",
        "        plt.close()\n",
        "\n",
        "        logger.info(f\"Summary report saved to {summary_path}\")"
      ],
      "metadata": {
        "id": "eX7StRUEYkIs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Update these paths with your actual paths\n",
        "    MODEL_PATH = \"/content/drive/MyDrive/Graduation Project/saved_models/optimized_model_v4/final_model.keras\"\n",
        "    DATA_DIR = \"/content/drive/MyDrive/Graduation Project/disease_data\"\n",
        "\n",
        "    tester = ModelTester(\n",
        "        model_path=MODEL_PATH,\n",
        "        data_dir=DATA_DIR,\n",
        "        img_size=(384, 384),\n",
        "        batch_size=32\n",
        "    )\n",
        "\n",
        "    tester.run_full_evaluation()"
      ],
      "metadata": {
        "id": "lOkAcI2UbBig"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}