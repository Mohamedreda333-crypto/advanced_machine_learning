{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"text-align: center;\">\n",
        "  <h1><b>ğŸŒ¿ Plant Verifier - Model 1 (Valid vs Invalid Image Classifier) ğŸ§ </b></h1>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ–¼ï¸ Valid vs Invalid Images\n",
        "\n",
        "<div style=\"display: flex; justify-content: center; align-items: center; gap: 20px;\">\n",
        "\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"https://www.researchgate.net/publication/348032743/figure/fig2/AS:11431281412894617@1745962357263/Sample-images-from-PlantVillage-dataset-For-the-details-of-different-classes-check.tif\" width=\"200\"/>\n",
        "    <p>âœ… Valid 1</p>\n",
        "  </div>\n",
        "\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQqlDYnE1brgKEW41g3MPyZu2Xw9GRKruZaT7Nv84gzVI47PUuYLGrlOo3GU78C44BYYMg&usqp=CAU\n",
        "    \" width=\"200\"/>\n",
        "    <p>âœ… Valid 2</p>\n",
        "  </div>\n",
        "\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"https://images.unsplash.com/photo-1746311473391-0c0bf08ad9b9?w=600&auto=format&fit=crop&q=60&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHx0b3BpYy1mZWVkfDd8NnNNVmpUTFNrZVF8fGVufDB8fHx8fA%3D%3D\" width=\"200\"/>\n",
        "    <p>âŒ Invalid 1</p>\n",
        "  </div>\n",
        "\n",
        "  <div style=\"text-align: center;\">\n",
        "    <img src=\"https://plus.unsplash.com/premium_photo-1713200811001-af93d0dcdfc2?w=600&auto=format&fit=crop&q=60&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHx0b3BpYy1mZWVkfDV8aVVJc25WdGpCMFl8fGVufDB8fHx8fA%3D%3D\" width=\"200\"/>\n",
        "    <p>âŒ Invalid 2</p>\n",
        "  </div>\n",
        "\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸\n",
        "\n",
        "ğŸŒ¿ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸŒ¿ğŸŒ¿ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸŒ¿ğŸŒ¿ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸŒ¿ğŸŒ¿ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸŒ¿\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Our Details**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### â€â€§â‚ŠËšâœ¿[My Name]âœ¿Ëš : ** Mohamed Reda Ramadan Khamis**\n",
        "### â€â€§â‚ŠËšâœ¿[Phone Number]âœ¿Ëš: **01554725661**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸\n",
        "\n",
        "ğŸŒ¿ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸŒ¿ğŸŒ¿ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸŒ¿ğŸŒ¿ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸŒ¿ğŸŒ¿ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸŒ¿"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ***Install Required Package â€” `opendatasets`***\n",
        "â¬‡ï¸ Installing the package:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-syJRtOnlXc",
        "outputId": "4f20cf3c-34f1-4cfd-c406-74e8270285c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from opendatasets) (4.67.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (from opendatasets) (1.7.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from opendatasets) (8.1.8)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2025.1.31)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (3.4.1)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (5.29.4)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2.3.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (0.5.1)\n",
            "Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n"
          ]
        }
      ],
      "source": [
        "!pip install opendatasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ***ğŸŒ±Download PlantVillage Dataset from Kaggle***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vy9BxhQvq2Bc",
        "outputId": "c0b42de8-66d6-460b-e9c2-fb35df3af376"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username:"
          ]
        }
      ],
      "source": [
        "import opendatasets as od\n",
        "od.download (\"https://www.kaggle.com/datasets/emmarex/plantdisease\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ***ğŸŒ±Download PlantDoc Dataset from Kaggle***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t97kNYIErUWv"
      },
      "outputs": [],
      "source": [
        "import opendatasets as od\n",
        "od.download (\"https://www.kaggle.com/datasets/nirmalsankalana/plantdoc-dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ***ğŸš€ Mount Google Drive in Colab***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6L0zFpkhryrT"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keWEeItxs1ti"
      },
      "outputs": [],
      "source": [
        "BASE_DIR = '/content/drive/MyDrive/Graduation Project'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ***ğŸ“‚ Key Project Directory Paths***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5x_LLkb4s7TF"
      },
      "outputs": [],
      "source": [
        "PLANTVILLAGE_DIR     = \"/content/plantdisease/PlantVillage\"\n",
        "INVALID_DIR          = f'{BASE_DIR}/invalidPlants-Dataset'\n",
        "PLANTDOC_TRAIN_DIR   = \"/content/plantdoc-dataset/train\"\n",
        "PLANTDOC_TEST_DIR    = \"/content/plantdoc-dataset/test\"\n",
        "COMBINED_DIR         = f'{BASE_DIR}/Combined_Dataset'\n",
        "MODELS_DIR           = f'{BASE_DIR}/saved_models'\n",
        "TRAINING_LOGS_DIR    = f'{BASE_DIR}/training_logs'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jdcj2Dd5Vncb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ***ğŸ§°Essential Libraries and Imports***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBdqAD2ItpA4"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet152V2\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger, TerminateOnNaN\n",
        "import matplotlib.image as mpimg\n",
        "import numpy as np\n",
        "import os, shutil , glob , math\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ***ğŸ¯ Custom F1 Score Metric Implementation***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fioq2Yymt0LA"
      },
      "outputs": [],
      "source": [
        "class F1Score(tf.keras.metrics.Metric):\n",
        "    \"\"\"\n",
        "    Custom TensorFlow Metric to calculate the F1 Score during model training and evaluation.\n",
        "\n",
        "    Methods:\n",
        "    - update_state: Updates TP, FP, FN counts based on the current batch predictions and true labels.\n",
        "    - result: Computes and returns the F1 score using the accumulated TP, FP, and FN values.\n",
        "    - reset_state: Resets all counters to zero before starting a new evaluation phase.\n",
        "\n",
        "    Usage:\n",
        "    This metric can be integrated directly into the model.compile() step and monitored \n",
        "    alongside accuracy, precision, and recall for comprehensive performance evaluation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, name='f1_score', **kwargs):\n",
        "        super(F1Score, self).__init__(name=name, **kwargs)\n",
        "        self.tp = self.add_weight(name='tp', initializer='zeros')\n",
        "        self.fp = self.add_weight(name='fp', initializer='zeros')\n",
        "        self.fn = self.add_weight(name='fn', initializer='zeros')\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        y_pred = tf.argmax(y_pred, axis=1)\n",
        "        y_true = tf.argmax(y_true, axis=1)\n",
        "        self.tp.assign_add(tf.reduce_sum(tf.cast(tf.equal(y_true, y_pred), tf.float32)))\n",
        "        self.fp.assign_add(tf.reduce_sum(tf.cast(tf.not_equal(y_true, y_pred), tf.float32)))\n",
        "        self.fn.assign_add(tf.reduce_sum(tf.cast(tf.not_equal(y_true, y_pred), tf.float32)))\n",
        "\n",
        "    def result(self):\n",
        "        precision = self.tp / (self.tp + self.fp + tf.keras.backend.epsilon())\n",
        "        recall = self.tp / (self.tp + self.fn + tf.keras.backend.epsilon())\n",
        "        return 2 * ((precision * recall) / (precision + recall + tf.keras.backend.epsilon()))\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.tp.assign(0.0)\n",
        "        self.fp.assign(0.0)\n",
        "        self.fn.assign(0.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2LGr1dAt8Ee"
      },
      "outputs": [],
      "source": [
        "# Set random seed for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRvj6DMsuCw-"
      },
      "outputs": [],
      "source": [
        "# Create directories\n",
        "os.makedirs(COMBINED_DIR, exist_ok=True)\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "os.makedirs(TRAINING_LOGS_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ***ğŸ“Š Dataset Image Count Utility***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzwkA8Iutllu"
      },
      "outputs": [],
      "source": [
        "def count_images(directory):\n",
        "    \"\"\"Count the number of image files in a directory and its subdirectories.\"\"\"\n",
        "    image_extensions = {\".jpg\", \".jpeg\", \".png\"}\n",
        "    count = 0\n",
        "    for root, _, files in os.walk(directory):\n",
        "        count += sum(1 for file in files if os.path.splitext(file)[1].lower() in image_extensions)\n",
        "    return count\n",
        "\n",
        "# Count images in both datasets\n",
        "village_count = count_images(PLANTVILLAGE_DIR)\n",
        "doc_count = count_images(PLANTDOC_TRAIN_DIR)\n",
        "total_count = village_count + doc_count\n",
        "\n",
        "print(f\"Number of training images in PlantVillage dataset: {village_count}\")\n",
        "print(f\"Number of training images in PlantDoc dataset: {doc_count}\")\n",
        "print(f\"Total Number of training images : {total_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ***ğŸ“‚ Directory Folder Listing Utility***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54vcT-oCtmPx"
      },
      "outputs": [],
      "source": [
        "def list_folders(directory):\n",
        "    \"\"\"\n",
        "    Lists subfolders in the specified directory.\n",
        "    Useful for quickly checking folder structure in datasets or any directory.\n",
        "\n",
        "    Args:\n",
        "        directory (str): Path to the directory to inspect.\n",
        "\n",
        "    Prints an error message if the directory does not exist.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        folders = [folder for folder in os.listdir(directory) if os.path.isdir(os.path.join(directory, folder))]\n",
        "        if folders:\n",
        "            print(f\"Folders in '{directory}':\")\n",
        "            for folder in folders:\n",
        "                print(f\"- {folder}\")\n",
        "        else:\n",
        "            print(f\"No folders found in '{directory}'\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Directory not found: {directory}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ***ğŸ“‹ Preview of First Four Classes with Image Counts***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95F2GYX2uLRo"
      },
      "outputs": [],
      "source": [
        "def list_first_four_classes(directory):\n",
        "    \"\"\"\n",
        "    Lists the first four class folders in the given directory and prints the number of images in each.\n",
        "\n",
        "    Args:\n",
        "        directory (str): Path to the dataset directory containing class subfolders.\n",
        "\n",
        "    Prints first four class names and image counts for quick dataset overview.\n",
        "    \"\"\"\n",
        "    # Get all subdirectories (classes) in the given directory\n",
        "    classes = sorted([d for d in os.listdir(directory) if os.path.isdir(os.path.join(directory, d))])\n",
        "\n",
        "    # Limit to the first four classes\n",
        "    first_four_classes = classes[:4]\n",
        "\n",
        "    # Print the number of images in each of the first four classes\n",
        "    for class_name in first_four_classes:\n",
        "        class_path = os.path.join(directory, class_name)\n",
        "        # Count all files in the folder\n",
        "        num_images = len(glob.glob(os.path.join(class_path, '*.*')))\n",
        "        print(f\"Class: {class_name}, Number of Images: {num_images}\")\n",
        "\n",
        "# PlantVillage Dataset (Training)\n",
        "print(\"PlantVillage Dataset (Training):\")\n",
        "list_first_four_classes(PLANTVILLAGE_DIR)\n",
        "\n",
        "# PlantDoc Dataset (Training)\n",
        "print(\"\\nPlantDoc Dataset (Training):\")\n",
        "list_first_four_classes(PLANTDOC_TRAIN_DIR)\n",
        "\n",
        "# PlantDoc Dataset (Testing)\n",
        "print(\"\\nPlantDoc Dataset (Testing):\")\n",
        "list_first_four_classes(PLANTDOC_TEST_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ***ğŸ¨ Detecting Grayscale Image Folders***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ci0daJsuJsX"
      },
      "outputs": [],
      "source": [
        "def is_grayscale(image_path):\n",
        "    \"\"\"\n",
        "    Checks if an image is grayscale.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the image file.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the image is grayscale ('L' mode), False otherwise or if an error occurs.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        img = Image.open(image_path)\n",
        "        return img.mode == 'L'  # 'L' mode means grayscale\n",
        "    except Exception as e:\n",
        "        return False\n",
        "\n",
        "def list_grayscale_folders(directory):\n",
        "    \"\"\"\n",
        "    Scans subfolders in a directory and lists those containing at least one grayscale image.\n",
        "\n",
        "    Args:\n",
        "        directory (str): Root directory to scan.\n",
        "\n",
        "    Prints:\n",
        "        Names of folders with grayscale images, or a message if none found.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        grayscale_folders = []\n",
        "\n",
        "        # Walk through the directory\n",
        "        for root, dirs, files in os.walk(directory):\n",
        "            for folder in dirs:\n",
        "                folder_path = os.path.join(root, folder)\n",
        "                image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "                # Check the first image to decide if the folder is grayscale\n",
        "                for image_file in image_files:\n",
        "                    image_path = os.path.join(folder_path, image_file)\n",
        "                    if is_grayscale(image_path):\n",
        "                        grayscale_folders.append(folder)\n",
        "                        break\n",
        "\n",
        "        # Print results\n",
        "        if grayscale_folders:\n",
        "            print(f\"Grayscale folders in '{directory}':\")\n",
        "            for folder in grayscale_folders:\n",
        "                print(f\"- {folder}\")\n",
        "        else:\n",
        "            print(f\"No grayscale folders found in '{directory}'\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Directory not found: {directory}\")\n",
        "\n",
        "# Print grayscale folders\n",
        "list_grayscale_folders(PLANTVILLAGE_DIR)\n",
        "list_grayscale_folders(PLANTDOC_TEST_DIR)\n",
        "list_grayscale_folders(PLANTDOC_TRAIN_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ***ğŸ¨ Grayscale to RGB Image Conversion Utility***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2wrU7moWulC"
      },
      "outputs": [],
      "source": [
        "\n",
        "def convert_to_rgb(image_path):\n",
        "    \"\"\"\n",
        "    Converts a grayscale image to RGB format and overwrites the original file.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the image file.\n",
        "\n",
        "    Prints:\n",
        "        Converts a grayscale image to RGB or error message if conversion fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        img = Image.open(image_path).convert('RGB')\n",
        "        img.save(image_path)  # Overwrite the original image\n",
        "        print(f\"Converted: {image_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting {image_path}: {e}\")\n",
        "\n",
        "def convert_folder_to_rgb(folder_path):\n",
        "    \"\"\"Converts all grayscale images in a folder to RGB.\"\"\"\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(('.jpg', '.jpeg', '.png')):\n",
        "            image_path = os.path.join(folder_path, filename)\n",
        "            # Check if grayscale\n",
        "            if Image.open(image_path).mode == 'L':  \n",
        "                convert_to_rgb(image_path)\n",
        "\n",
        "# Path to the folder containing grayscale images\n",
        "grayscale_folder = os.path.join(PLANTDOC_TRAIN_DIR, \"Apple_rust_leaf\")\n",
        "\n",
        "# Convert images in the folder to RGB\n",
        "convert_folder_to_rgb(grayscale_folder)\n",
        "print(\"Conversion complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ***ğŸ–¼ï¸ Visualizing One Sample Image per Class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZe28NUlvzTh"
      },
      "outputs": [],
      "source": [
        "def display_one_image_per_all_classes_small(directory, title):\n",
        "    \"\"\"\n",
        "    Displays one sample image per class from a dataset directory in a compact grid.\n",
        "\n",
        "    Args:\n",
        "        directory (str): Path to the dataset root containing class subfolders.\n",
        "        title (str): Title for the entire figure.\n",
        "\n",
        "    Functionality:\n",
        "        - Lists all class subdirectories.\n",
        "        - For each class, loads and displays the first image.\n",
        "        - Arranges images in a grid with 6 columns for compact visualization.\n",
        "        - Shows the class name below each image with smaller font size.\n",
        "\n",
        "    Use Case:\n",
        "        Useful for a quick visual overview of dataset diversity and class examples.\n",
        "    \"\"\"\n",
        "    classes = sorted([d for d in os.listdir(directory) if os.path.isdir(os.path.join(directory, d))])\n",
        "\n",
        "    # Calculate the number of rows and columns for the grid\n",
        "    num_classes = len(classes)\n",
        "    # Increase the number of columns to make images smaller\n",
        "    num_cols = 6\n",
        "    # Number of rows needed\n",
        "    num_rows = math.ceil(num_classes / num_cols)\n",
        "\n",
        "    # Set up the plot with a smaller figure size\n",
        "    plt.figure(figsize=(15, 2 * num_rows))  \n",
        "    plt.suptitle(title, fontsize=16)\n",
        "\n",
        "    # Display one image from each class\n",
        "    for i, class_name in enumerate(classes):\n",
        "        # Get the path to the first image in the class folder\n",
        "        class_path = os.path.join(directory, class_name)\n",
        "        # Get the first image(slicing)\n",
        "        image_path = glob.glob(os.path.join(class_path, '*.*'))[0]\n",
        "\n",
        "        # Load and display the image\n",
        "        img = mpimg.imread(image_path)\n",
        "        plt.subplot(num_rows, num_cols, i + 1)  \n",
        "        plt.imshow(img)\n",
        "        plt.title(class_name, fontsize=8)  \n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# PlantVillage Dataset (Training)\n",
        "print(\"Displaying one image from every class in PlantVillage Dataset (Training):\")\n",
        "display_one_image_per_all_classes_small(PLANTVILLAGE_DIR, \"PlantVillage Dataset (Training)\")\n",
        "\n",
        "# PlantDoc Dataset (Training)\n",
        "print(\"\\nDisplaying one image from every class in PlantDoc Dataset (Training):\")\n",
        "display_one_image_per_all_classes_small(PLANTDOC_TRAIN_DIR, \"PlantDoc Dataset (Training)\")\n",
        "\n",
        "# PlantDoc Dataset (Testing)\n",
        "print(\"\\nDisplaying one image from every class in PlantDoc Dataset (Testing):\")\n",
        "display_one_image_per_all_classes_small(PLANTDOC_TEST_DIR, \"PlantDoc Dataset (Testing)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ***âš™ï¸ Model Configuration Parameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1wshUb8v1uS"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "MAX_EPOCHS = 50\n",
        "INIT_LR = 1e-4\n",
        "ES_PATIENCE = 10\n",
        "LR_PATIENCE = 5\n",
        "DROPOUT_RATE = 0.5\n",
        "L2_REG = 1e-4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ***ğŸ–¼ï¸ Image Preprocessing Pipeline with Data Augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYz14q4uv_4C"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.resnet_v2 import preprocess_input as resnet_preprocess\n",
        "\n",
        "def preprocess_image(image, label, is_train=True):\n",
        "    \"\"\"\n",
        "    Preprocesses input images for model training and evaluation.\n",
        "\n",
        "    Steps:\n",
        "    - Resize image to the target input size.\n",
        "    - Normalize pixel values to [0, 1].\n",
        "    - Apply data augmentation during training, including random flips and color jitter.\n",
        "    - Apply model-specific preprocessing (e.g., ResNet preprocessing).\n",
        "\n",
        "    Args:\n",
        "        image (tf.Tensor): Input image tensor.\n",
        "        label (tf.Tensor): Corresponding label.\n",
        "        is_train (bool): Whether to apply data augmentation (True for training).\n",
        "\n",
        "    Returns:\n",
        "        Tuple of preprocessed image and label.\n",
        "    \"\"\"\n",
        "    image = tf.image.resize(image, IMG_SIZE)\n",
        "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "    if is_train:\n",
        "        image = tf.image.random_flip_left_right(image)\n",
        "        image = tf.image.random_brightness(image, 0.2)\n",
        "        image = tf.image.random_contrast(image, 0.8, 1.2)\n",
        "        image = tf.image.random_saturation(image, 0.8, 1.2)\n",
        "        image = tf.image.random_hue(image, 0.05)\n",
        "    image = resnet_preprocess(image)  # <--- ADD THIS\n",
        "    return image, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ***ğŸ”„ Professional TensorFlow Data Generator with Preprocessing & Augmentation***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2GaJatIwB9e"
      },
      "outputs": [],
      "source": [
        "def create_data_generator_tf(directory, is_train=True, batch_size=BATCH_SIZE):\n",
        "    \"\"\"\n",
        "    Creates a TensorFlow data pipeline for image classification.\n",
        "    \n",
        "    Args:\n",
        "        directory (str): Path to the dataset directory structured by class subfolders.\n",
        "        is_train (bool): Whether to generate training data (True) or validation data (False).\n",
        "        batch_size (int): Number of samples per batch.\n",
        "\n",
        "    Returns:\n",
        "        dataset (tf.data.Dataset): Preprocessed, batched, and prefetched dataset ready for training or validation.\n",
        "        steps (int): Number of steps per epoch, calculated from dataset size and batch size.\n",
        "    \"\"\"\n",
        "    AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "    # Build dataset from directory\n",
        "    dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "        directory,\n",
        "        labels='inferred',\n",
        "        label_mode='binary',\n",
        "        image_size=IMG_SIZE,\n",
        "        batch_size=batch_size,  # Batch here\n",
        "        shuffle=is_train,\n",
        "        validation_split=0.2,\n",
        "        subset='training' if is_train else 'validation',\n",
        "        seed=123\n",
        "    )\n",
        "\n",
        "    # Count images\n",
        "    image_count = len(dataset.file_paths)\n",
        "    steps = math.ceil(image_count / batch_size)\n",
        "\n",
        "    # Apply preprocessing\n",
        "    dataset = dataset.map(\n",
        "        lambda x, y: preprocess_image(x, y, is_train),\n",
        "        num_parallel_calls=AUTOTUNE\n",
        "    )\n",
        "\n",
        "    # Filter out corrupted images\n",
        "    dataset = dataset.filter(lambda x, y: tf.reduce_sum(x) > 0)\n",
        "\n",
        "    # Prefetch for performance\n",
        "    dataset = dataset.prefetch(AUTOTUNE)\n",
        "\n",
        "    return dataset, steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ***ğŸ—ï¸ Optimized Model Architecture with ResNet152V2 Backbone***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOJfoUHLwD5I"
      },
      "outputs": [],
      "source": [
        "def build_optimized_model(backbone='efficientnetv2l'):\n",
        "    \"\"\"\n",
        "    Builds a CNN classification model leveraging a pre-trained backbone with custom top layers.\n",
        "\n",
        "    Args:\n",
        "        backbone (str): Backbone model architecture name. Supported: 'resnet152v2'.\n",
        "\n",
        "    Returns:\n",
        "        tf.keras.Model: A compiled Keras Sequential model ready for training.\n",
        "    \"\"\"\n",
        "    if backbone == 'resnet152v2':\n",
        "        base_model = tf.keras.applications.ResNet152V2(\n",
        "            weights='imagenet',\n",
        "            include_top=False,\n",
        "            input_shape=(*IMG_SIZE, 3)\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported backbone!\")\n",
        "\n",
        "    base_model.trainable = False  # Freeze base initially\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        base_model,\n",
        "        tf.keras.layers.GlobalAveragePooling2D(),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dropout(DROPOUT_RATE),\n",
        "\n",
        "        tf.keras.layers.Dense(256, activation='swish', kernel_regularizer=tf.keras.regularizers.l2(L2_REG)),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dropout(DROPOUT_RATE),\n",
        "\n",
        "        tf.keras.layers.Dense(64, activation='swish', kernel_regularizer=tf.keras.regularizers.l2(L2_REG)),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dropout(DROPOUT_RATE / 2),\n",
        "\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ***âš–ï¸ Compute Balanced Class Weights for Handling Class Imbalance***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yg-84Nx5USX6"
      },
      "outputs": [],
      "source": [
        "def compute_class_weights(train_data):\n",
        "    \"\"\"\n",
        "    Calculates balanced class weights based on the label distribution in the training dataset.\n",
        "\n",
        "    This helps to mitigate class imbalance by assigning higher weights to underrepresented classes,\n",
        "    improving model training fairness and performance.\n",
        "\n",
        "    Args:\n",
        "        train_data (tf.data.Dataset): A TensorFlow dataset yielding (image, label) pairs, where labels are one-hot encoded.\n",
        "\n",
        "    Returns:\n",
        "        dict: A mapping from class indices to their corresponding weights.\n",
        "    \"\"\"\n",
        "    labels = []\n",
        "    for _, y in train_data.unbatch():\n",
        "        labels.append(tf.argmax(y).numpy())\n",
        "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(labels), y=labels)\n",
        "    return dict(enumerate(class_weights))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ***ğŸ—ï¸ Model Training with ResNet152V2 Backbone***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--h1LVbswGKX"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_gen, val_gen, model_name, steps_per_epoch, val_steps=None, train_keras_gen=None):\n",
        "    \"\"\"\n",
        "    Trains a Keras model with early stopping, learning rate scheduling, checkpointing, and logging.\n",
        "    Handles class imbalance by computing class weights dynamically.\n",
        "    Uses AdamW optimizer and tracks accuracy, AUC, precision, and recall.\n",
        "    \n",
        "    Args:\n",
        "        model: Keras model to train.\n",
        "        train_gen: Training dataset generator.\n",
        "        val_gen: Validation dataset generator.\n",
        "        model_name: Name for saving logs and model files.\n",
        "        steps_per_epoch: Number of training steps per epoch.\n",
        "        val_steps: Number of validation steps (optional).\n",
        "        train_keras_gen: Optional Keras generator for class weight computation.\n",
        "\n",
        "    Returns:\n",
        "        history: Training history object with metrics per epoch.\n",
        "    \"\"\"\n",
        "    log_file = os.path.join(TRAINING_LOGS_DIR, f'{model_name}_training_log.csv')\n",
        "\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', patience=ES_PATIENCE, restore_best_weights=True, verbose=1),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=LR_PATIENCE, min_lr=1e-6, verbose=1),\n",
        "        ModelCheckpoint(os.path.join(MODELS_DIR, f'{model_name}.keras'),\n",
        "                       monitor='val_accuracy', save_best_only=True, verbose=1),\n",
        "        CSVLogger(log_file),\n",
        "        TerminateOnNaN()\n",
        "    ]\n",
        "\n",
        "    # Safe class_weight computation\n",
        "    class_weight = None\n",
        "    if train_keras_gen is not None:\n",
        "        try:\n",
        "            class_counts = np.bincount(train_keras_gen.classes)\n",
        "            total_samples = np.sum(class_counts)\n",
        "            class_weight = {i: total_samples / (2.0 * count) for i, count in enumerate(class_counts)}\n",
        "            print(f\"[INFO] Computed class_weight: {class_weight}\")\n",
        "        except Exception as e:\n",
        "            print(f\"[WARNING] Could not compute class weights: {e}\")\n",
        "            class_weight = None\n",
        "\n",
        "    optimizer = AdamW(learning_rate=INIT_LR, weight_decay=1e-4)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy', tf.keras.metrics.AUC(),\n",
        "                                tf.keras.metrics.Precision(),\n",
        "                                      tf.keras.metrics.Recall()\n",
        "                                    ]\n",
        "\n",
        "    )\n",
        "\n",
        "    weights = compute_class_weights(train_gen)\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        train_gen,\n",
        "        epochs=MAX_EPOCHS,\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        validation_data=val_gen,\n",
        "        validation_steps=val_steps,\n",
        "        callbacks=callbacks,\n",
        "        class_weight=weights,\n",
        "        verbose=2\n",
        "    )\n",
        "\n",
        "    return history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ***ğŸš€ Model Evaluation & Performance Report***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQpfq5aOwI0O"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_gen, model_name):\n",
        "    \"\"\"\n",
        "    Evaluates a trained model on a test dataset and provides detailed performance metrics.\n",
        "\n",
        "    Steps:\n",
        "    - Predicts labels for the test set.\n",
        "    - Prints a classification report including precision, recall, and F1-score per class.\n",
        "    - Generates and saves a confusion matrix heatmap for visual analysis.\n",
        "    - Returns the overall accuracy on the test set.\n",
        "\n",
        "    Args:\n",
        "        model: Trained Keras model to evaluate.\n",
        "        test_gen: Test dataset generator with .classes and .class_indices attributes.\n",
        "        model_name: Identifier used for saving output files.\n",
        "\n",
        "    Returns:\n",
        "        float: Mean accuracy on the test dataset.\n",
        "    \"\"\"\n",
        "    print(f\"\\nEvaluating {model_name}...\")\n",
        "\n",
        "    y_true = test_gen.classes\n",
        "    y_pred = np.argmax(model.predict(test_gen), axis=1)\n",
        "\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_true, y_pred, target_names=test_gen.class_indices.keys()))\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d',\n",
        "                xticklabels=test_gen.class_indices.keys(),\n",
        "                yticklabels=test_gen.class_indices.keys())\n",
        "    plt.title(f'{model_name} Confusion Matrix')\n",
        "    plt.savefig(os.path.join(MODELS_DIR, f'{model_name}_confusion_matrix.png'))\n",
        "    plt.close()\n",
        "\n",
        "    return np.mean(y_true == y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ***ğŸ“ˆ Training History Visualization***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fe1kfJmVwKu-"
      },
      "outputs": [],
      "source": [
        "def plot_training_history(history, model_name):\n",
        "    \"\"\"\n",
        "    Plots the training history of a model, showing accuracy and loss over epochs.\n",
        "\n",
        "    Parameters:\n",
        "    - history: The history object returned by model.fit()\n",
        "    - model_name: The name of the model (for the plot title)\n",
        "    \"\"\"\n",
        "    # Plot accuracy\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Training Accuracy', marker='o')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy', marker='o')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title(f'{model_name} - Accuracy Over Epochs')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Training Loss', marker='o')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss', marker='o')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title(f'{model_name} - Loss Over Epochs')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzKn3j8twNE4"
      },
      "outputs": [],
      "source": [
        "temp_dir = \"C:/Users/pc/Documents/temp_model1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ***ğŸ—‚ï¸ Dataset Preparation for Model 1: Valid & Invalid Image Segregation***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhxuwB3EwQkk"
      },
      "outputs": [],
      "source": [
        "def prepare_model1_dataset(temp_dir, plantvillage_dir, plantdoc_train_dir, invalid_dir):\n",
        "    \"\"\"\n",
        "    Prepares the training dataset for Model 1 by organizing images into two categories:\n",
        "    - Valid plant images (from PlantVillage and PlantDoc datasets).\n",
        "    - Invalid/non-plant images.\n",
        "\n",
        "    The function verifies image integrity before copying to avoid corrupted files,\n",
        "    and arranges them into separate folders within a temporary directory.\n",
        "\n",
        "    Args:\n",
        "        temp_dir (str): Temporary directory to store prepared dataset.\n",
        "        plantvillage_dir (str): Directory path for PlantVillage dataset.\n",
        "        plantdoc_train_dir (str): Directory path for PlantDoc training dataset.\n",
        "        invalid_dir (str): Directory path for invalid images.\n",
        "    \"\"\"\n",
        "    plantvillage_target = os.path.join(temp_dir, 'plantvillage')\n",
        "    invalid_target = os.path.join(temp_dir, 'invalid')\n",
        "\n",
        "    os.makedirs(plantvillage_target, exist_ok=True)\n",
        "    os.makedirs(invalid_target, exist_ok=True)\n",
        "\n",
        "    valid_count = 0\n",
        "    invalid_count = 0\n",
        "\n",
        "\n",
        "    # âœ… Copy PlantVillage images to 'plantvillage'\n",
        "    for cls in os.listdir(plantvillage_dir):\n",
        "        src_cls_path = os.path.join(plantvillage_dir, cls)\n",
        "        if os.path.isdir(src_cls_path):\n",
        "            for img in os.listdir(src_cls_path):\n",
        "                src_img_path = os.path.join(src_cls_path, img)\n",
        "                dst_img_path = os.path.join(plantvillage_target, f\"pv_{cls}_{img}\")\n",
        "                try:\n",
        "                    with Image.open(src_img_path) as im:\n",
        "                        im.verify()\n",
        "                    shutil.copy(src_img_path, dst_img_path)\n",
        "                    valid_count += 1\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "    # âœ… Copy PlantDoc train images to 'plantvillage'\n",
        "    for cls in os.listdir(plantdoc_train_dir):\n",
        "        src_cls_path = os.path.join(plantdoc_train_dir, cls)\n",
        "        if os.path.isdir(src_cls_path):\n",
        "            for img in os.listdir(src_cls_path):\n",
        "                src_img_path = os.path.join(src_cls_path, img)\n",
        "                dst_img_path = os.path.join(plantvillage_target, f\"pd_{cls}_{img}\")\n",
        "                try:\n",
        "                    with Image.open(src_img_path) as im:\n",
        "                        im.verify()\n",
        "                    shutil.copy(src_img_path, dst_img_path)\n",
        "                    valid_count += 1\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "    # âœ… Copy invalid (fake) images to 'invalid'\n",
        "    for img in os.listdir(invalid_dir):\n",
        "        src_img_path = os.path.join(invalid_dir, img)\n",
        "        dst_img_path = os.path.join(invalid_target, img)\n",
        "        try:\n",
        "            with Image.open(src_img_path) as im:\n",
        "                im.verify()\n",
        "            shutil.copy(src_img_path, dst_img_path)\n",
        "            invalid_count += 1\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    print(f\"\\nâœ… Total VALID images (PlantVillage + PlantDoc): {valid_count}\")\n",
        "    print(f\"âŒ Total INVALID images: {invalid_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0617F0oPwTNy"
      },
      "outputs": [],
      "source": [
        "# Prepare data\n",
        "prepare_model1_dataset(temp_dir, PLANTVILLAGE_DIR, PLANTDOC_TRAIN_DIR, INVALID_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ***ğŸš€ Training Pipeline for Model 1: PlantVillage Verifier (Initial + Fine-tuning Phases)***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BWjS8TzwXBd"
      },
      "outputs": [],
      "source": [
        "# Get datasets and step counts\n",
        "train_dataset, train_steps = create_data_generator_tf(temp_dir, is_train=True)\n",
        "val_dataset, val_steps = create_data_generator_tf(temp_dir, is_train=False)\n",
        "\n",
        "# Build and compile model\n",
        "model_1 = build_optimized_model()\n",
        "\n",
        "# First training phase\n",
        "print(\"\\nStarting initial training...\")\n",
        "history = train_model(\n",
        "    model=model_1,\n",
        "    train_gen=train_dataset,\n",
        "    val_gen=val_dataset,\n",
        "    model_name=\"plantvillage_verifier\",\n",
        "    steps_per_epoch=train_steps,\n",
        "    val_steps=val_steps\n",
        ")\n",
        "\n",
        "# Fine-tuning phase\n",
        "print(\"\\nContinuing training (with more layers frozen)...\")\n",
        "for layer in model_1.layers[0].layers[:150]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Recompile with lower learning rate\n",
        "optimizer = AdamW(learning_rate=INIT_LR/10, weight_decay=1e-4)\n",
        "model_1.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=[\n",
        "        'accuracy',\n",
        "        tf.keras.metrics.AUC(name='auc'),\n",
        "        tf.keras.metrics.Precision(name='precision'),\n",
        "        tf.keras.metrics.Recall(name='recall')\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Continue training\n",
        "history = train_model(\n",
        "    model=model_1,\n",
        "    train_gen=train_dataset,\n",
        "    val_gen=val_dataset,\n",
        "    model_name=\"plantvillage_verifier_finetuned\",\n",
        "    steps_per_epoch=train_steps,\n",
        "    val_steps=val_steps\n",
        ")\n",
        "\n",
        "\n",
        "# =================== Evaluate Model 1 on validation set ====================\n",
        "\n",
        "evaluation = model_1.evaluate(val_dataset, verbose=2)\n",
        "print(f\"\\nFinal Model Accuracy: {evaluation[1]:.2%}\")\n",
        "print(f\"Precision: {evaluation[3]:.2%}\")\n",
        "print(f\"Recall: {evaluation[4]:.2%}\")\n",
        "print(f\"AUC: {evaluation[2]:.2%}\")\n",
        "\n",
        "# =================== Save the final model ==================================\n",
        "model_1.save(os.path.join(MODELS_DIR, 'plantvillage_verifier_final.keras'))\n",
        "\n",
        "\n",
        "# =================== Clean up the temporary directory ======================\n",
        "shutil.rmtree(temp_dir)\n",
        "print(\"\\nTraining completed and temporary files cleaned up.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸ğŸŒ¸ âœ¿ âœ¿ âœ¿ âœ¿ âœ¿ ğŸŒ¸\n",
        "\n",
        "ğŸŒ¿ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸŒ¿ğŸŒ¿ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸŒ¿ğŸŒ¿ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸŒ¿ğŸŒ¿ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸŒ¿\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"text-align: center;\">\n",
        "  <h1><b>ğŸ Training Session Completed Successfully</b></h1>\n",
        "</div>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
